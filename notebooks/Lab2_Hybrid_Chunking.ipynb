{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Hybrid Document Chunking Workshop\n",
    "\n",
    "Welcome to the **Hybrid Document Chunking Workshop**! This comprehensive notebook demonstrates how to use Docling's advanced chunking capabilities for RAG (Retrieval-Augmented Generation) applications with full structured output support.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- Understand hybrid chunking and its advantages over simple text splitting\n",
    "- Configure advanced document processing pipelines\n",
    "- Process various document formats with OCR, table extraction, and figure exports\n",
    "- Generate structured output with organized folder hierarchies\n",
    "- Implement tokenization-aware chunking strategies\n",
    "- Use LLM-powered image descriptions for multimodal RAG\n",
    "- Analyze and visualize chunks for optimal RAG performance\n",
    "\n",
    "## üìã Workshop Sections\n",
    "\n",
    "1. **üîß Setup & Dependencies** - Install packages with UV\n",
    "2. **‚öôÔ∏è  Advanced Pipeline Configuration** - All processing options explained\n",
    "3. **üìä Structured Output System** - Organized folder hierarchies  \n",
    "4. **üñºÔ∏è  Figure & Table Exports** - Visual content extraction\n",
    "5. **ü§ñ LLM Image Descriptions** - AI-powered multimodal processing\n",
    "6. **üß© Hybrid Chunking Engine** - Smart, context-aware chunking\n",
    "7. **üìà Analysis & Visualization** - Comprehensive chunk quality analysis\n",
    "8. **üéõÔ∏è  Interactive Configuration Testing** - Compare different settings\n",
    "9. **üí° Best Practices** - Production-ready recommendations\n",
    "\n",
    "Let's dive deep! üåä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîß Setup & Dependencies with UV\n",
    "\n",
    "UV is a fast Python package installer and dependency manager. Let's install all required packages for this comprehensive workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with UV\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_with_uv():\n",
    "    \"\"\"Install packages using UV package manager with fallback to pip.\"\"\"\n",
    "    packages = [\n",
    "        \"docling\",\n",
    "        \"transformers\", \n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"ipython\",\n",
    "        \"litellm\",  # For LLM image descriptions\n",
    "        \"pillow\",   # For image processing\n",
    "        \"python-dotenv\"  # For environment variables\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Installing packages with UV...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Try UV first\n",
    "    try:\n",
    "        for package in packages:\n",
    "            print(f\"üì¶ Installing {package} with UV...\")\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"uv\", \"add\", package], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"‚ö†Ô∏è  UV not available or failed. Trying with pip...\")\n",
    "        try:\n",
    "            for package in packages:\n",
    "                print(f\"üì¶ Installing {package} with pip...\")\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], check=True)\n",
    "                print(f\"‚úÖ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as pip_error:\n",
    "            print(f\"‚ùå Pip installation failed: {pip_error}\")\n",
    "            print(\"Please install packages manually or check your environment\")\n",
    "            return False\n",
    "    \n",
    "    print(\"\\nüéâ All packages installed successfully!\")\n",
    "    print(\"üìù Note: For LLM image descriptions, you'll need API keys in your environment:\")\n",
    "    print(\"   ‚Ä¢ ANTHROPIC_API_KEY for Claude models\")\n",
    "    return True\n",
    "\n",
    "# Uncomment the line below to install packages\n",
    "# install_with_uv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  LiteLLM not available. Image descriptions will be disabled.\n",
      "‚úÖ All imports successful!\n",
      "üöÄ Ready for advanced document processing!\n"
     ]
    }
   ],
   "source": [
    "# Core imports for the workshop\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Suppress PyTorch MPS warnings on Mac\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pin_memory.*not supported on MPS.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.*\")\n",
    "\n",
    "# Docling imports\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "from docling_core.types.doc import PictureItem, TableItem\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Optional LiteLLM imports for image descriptions\n",
    "try:\n",
    "    import litellm\n",
    "    import base64\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    LITELLM_AVAILABLE = True\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    LITELLM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  LiteLLM not available. Image descriptions will be disabled.\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üöÄ Ready for advanced document processing!\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚öôÔ∏è Advanced Pipeline Configuration\n",
    "\n",
    "The `AdvancedPipelineConfig` class provides comprehensive control over document processing. This replicates all the features from the command-line processor, including structured output, exports, and LLM integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AdvancedPipelineConfig class defined!\n",
      "\n",
      "============================================================\n",
      "üîß Advanced Pipeline Configuration:\n",
      "\n",
      "üìä CORE PROCESSING:\n",
      "        - OCR: ‚úì\n",
      "        - Table Structure: ‚úì\n",
      "        - Page Images: ‚úì\n",
      "        - Picture Images: ‚úì\n",
      "\n",
      "üß© CHUNKING:\n",
      "        - Method: Hybrid (tokenization-aware)\n",
      "        - Max Tokens/Chunk: 256\n",
      "        - Merge Peers: ‚úì\n",
      "        - Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "üìÅ STRUCTURED OUTPUT:\n",
      "        - Organize Output: ‚úì\n",
      "        - Save Metadata: ‚úì\n",
      "        - Export Markdown: ‚úì\n",
      "        - Output Directory: workshop_output\n",
      "\n",
      "üñºÔ∏è EXPORTS:\n",
      "        - Export Figures: ‚úì\n",
      "        - Export Tables: ‚úì\n",
      "        - Images Scale: 2.0x (144 DPI)\n",
      "\n",
      "ü§ñ LLM FEATURES:\n",
      "        - Describe Images: ‚úì\n",
      "        - LLM Model: claude-3-haiku-20240307\n",
      "        - LiteLLM Available: ‚úó\n",
      "\n",
      "‚öôÔ∏è FORMATS:\n",
      "        - Supported: PDF, DOCX, PPTX, XLSX, HTML, MD, IMAGE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "class AdvancedPipelineConfig:\n",
    "    \"\"\"Comprehensive configuration class replicating document_processor.py functionality.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # Core processing options\n",
    "        do_ocr: bool = True,\n",
    "        do_table_structure: bool = True,\n",
    "        generate_page_images: bool = False,  # Will be enabled if export_figures=True\n",
    "        generate_picture_images: bool = False,  # Will be enabled if export_figures=True\n",
    "        \n",
    "        # Chunking options\n",
    "        chunk_max_tokens: int = 512,\n",
    "        chunk_merge_peers: bool = True,\n",
    "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \n",
    "        # Export options (the key new features!)\n",
    "        export_figures: bool = False,\n",
    "        export_tables: bool = False,\n",
    "        images_scale: float = 2.0,  # Image resolution scale (1.0 = 72 DPI)\n",
    "        \n",
    "        # Structured output options\n",
    "        organize_output: bool = True,  # Create organized folder structure\n",
    "        save_metadata: bool = True,   # Save comprehensive document metadata\n",
    "        export_markdown: bool = True, # Export as markdown\n",
    "        output_dir: str = \"workshop_output\",\n",
    "        \n",
    "        # LLM Image description options\n",
    "        describe_images: bool = False,\n",
    "        llm_model: str = \"claude-3-haiku-20240307\",\n",
    "        \n",
    "        # Supported formats\n",
    "        allowed_formats: List[InputFormat] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize comprehensive pipeline configuration.\n",
    "        \n",
    "        Key New Features Explained:\n",
    "        \n",
    "        üèóÔ∏è STRUCTURED OUTPUT (organize_output=True):\n",
    "            Creates organized folder hierarchy:\n",
    "            output_dir/files/document_name/\n",
    "            ‚îú‚îÄ‚îÄ json/               # Main JSON output + markdown\n",
    "            ‚îú‚îÄ‚îÄ metadata/           # Separate metadata file\n",
    "            ‚îî‚îÄ‚îÄ exports/            # All visual exports\n",
    "                ‚îú‚îÄ‚îÄ figures/        # Extracted figures/pictures\n",
    "                ‚îú‚îÄ‚îÄ tables/         # Table images\n",
    "                ‚îî‚îÄ‚îÄ pages/          # Page screenshots\n",
    "        \n",
    "        üñºÔ∏è FIGURE EXPORTS (export_figures=True):\n",
    "            ‚Ä¢ Extracts all figures and pictures from documents\n",
    "            ‚Ä¢ Saves as high-resolution PNG files\n",
    "            ‚Ä¢ Automatically generates page screenshots\n",
    "            ‚Ä¢ Preserves visual content for multimodal RAG\n",
    "        \n",
    "        üìä TABLE EXPORTS (export_tables=True):\n",
    "            ‚Ä¢ Extracts tables to multiple formats: CSV, HTML, Markdown\n",
    "            ‚Ä¢ Saves table images as PNG files\n",
    "            ‚Ä¢ Preserves table structure and content\n",
    "            ‚Ä¢ Enables both text and visual table retrieval\n",
    "        \n",
    "        ü§ñ LLM IMAGE DESCRIPTIONS (describe_images=True):\n",
    "            ‚Ä¢ Uses LiteLLM to describe images with any LLM\n",
    "            ‚Ä¢ Supports Claude, GPT-4V, Gemini Vision, etc.\n",
    "            ‚Ä¢ Creates searchable text descriptions\n",
    "            ‚Ä¢ Tracks costs and token usage\n",
    "            ‚Ä¢ Enables semantic search over visual content\n",
    "        \n",
    "        üìã COMPREHENSIVE METADATA (save_metadata=True):\n",
    "            ‚Ä¢ Processing configuration details\n",
    "            ‚Ä¢ Document statistics and structure info\n",
    "            ‚Ä¢ File metadata and format detection\n",
    "            ‚Ä¢ Chunk quality metrics\n",
    "        \"\"\"\n",
    "        # Core processing\n",
    "        self.do_ocr = do_ocr\n",
    "        self.do_table_structure = do_table_structure\n",
    "        self.generate_page_images = generate_page_images\n",
    "        self.generate_picture_images = generate_picture_images\n",
    "        \n",
    "        # Chunking\n",
    "        self.chunk_max_tokens = chunk_max_tokens\n",
    "        self.chunk_merge_peers = chunk_merge_peers\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Export options\n",
    "        self.export_figures = export_figures\n",
    "        self.export_tables = export_tables\n",
    "        self.images_scale = images_scale\n",
    "        \n",
    "        # Structured output\n",
    "        self.organize_output = organize_output\n",
    "        self.save_metadata = save_metadata\n",
    "        self.export_markdown = export_markdown\n",
    "        self.output_dir = Path(output_dir)\n",
    "        \n",
    "        # LLM features\n",
    "        self.describe_images = describe_images\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Multi-format support\n",
    "        self.allowed_formats = allowed_formats or [\n",
    "            InputFormat.PDF,\n",
    "            InputFormat.DOCX,\n",
    "            InputFormat.PPTX,\n",
    "            InputFormat.XLSX,\n",
    "            InputFormat.HTML,\n",
    "            InputFormat.MD,\n",
    "            InputFormat.IMAGE,\n",
    "        ]\n",
    "        \n",
    "        # Auto-enable image generation if we're exporting figures\n",
    "        if self.export_figures:\n",
    "            self.generate_page_images = True\n",
    "            self.generate_picture_images = True\n",
    "    \n",
    "    def to_pipeline_options(self) -> Optional[PdfPipelineOptions]:\n",
    "        \"\"\"Convert configuration to Docling PdfPipelineOptions.\"\"\"\n",
    "        try:\n",
    "            options = PdfPipelineOptions(\n",
    "                do_ocr=self.do_ocr,\n",
    "                do_table_structure=self.do_table_structure,\n",
    "                generate_page_images=self.generate_page_images,\n",
    "                generate_picture_images=self.generate_picture_images,\n",
    "            )\n",
    "            \n",
    "            # Set images scale for high-quality exports\n",
    "            if self.export_figures or self.generate_page_images or self.generate_picture_images:\n",
    "                options.images_scale = self.images_scale\n",
    "                \n",
    "            return options\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create pipeline options: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Return a comprehensive summary of the configuration.\"\"\"\n",
    "        formats_str = ', '.join([f.name for f in self.allowed_formats])\n",
    "        \n",
    "        return f\"\"\"üîß Advanced Pipeline Configuration:\n",
    "        \n",
    "üìä CORE PROCESSING:\n",
    "        - OCR: {'‚úì' if self.do_ocr else '‚úó'}\n",
    "        - Table Structure: {'‚úì' if self.do_table_structure else '‚úó'}\n",
    "        - Page Images: {'‚úì' if self.generate_page_images else '‚úó'}\n",
    "        - Picture Images: {'‚úì' if self.generate_picture_images else '‚úó'}\n",
    "        \n",
    "üß© CHUNKING:\n",
    "        - Method: Hybrid (tokenization-aware)\n",
    "        - Max Tokens/Chunk: {self.chunk_max_tokens}\n",
    "        - Merge Peers: {'‚úì' if self.chunk_merge_peers else '‚úó'}\n",
    "        - Embedding Model: {self.embedding_model}\n",
    "        \n",
    "üìÅ STRUCTURED OUTPUT:\n",
    "        - Organize Output: {'‚úì' if self.organize_output else '‚úó'}\n",
    "        - Save Metadata: {'‚úì' if self.save_metadata else '‚úó'}\n",
    "        - Export Markdown: {'‚úì' if self.export_markdown else '‚úó'}\n",
    "        - Output Directory: {self.output_dir}\n",
    "        \n",
    "üñºÔ∏è EXPORTS:\n",
    "        - Export Figures: {'‚úì' if self.export_figures else '‚úó'}\n",
    "        - Export Tables: {'‚úì' if self.export_tables else '‚úó'}\n",
    "        - Images Scale: {self.images_scale}x ({int(self.images_scale * 72)} DPI)\n",
    "        \n",
    "ü§ñ LLM FEATURES:\n",
    "        - Describe Images: {'‚úì' if self.describe_images else '‚úó'}\n",
    "        - LLM Model: {self.llm_model}\n",
    "        - LiteLLM Available: {'‚úì' if LITELLM_AVAILABLE else '‚úó'}\n",
    "        \n",
    "‚öôÔ∏è FORMATS:\n",
    "        - Supported: {formats_str}\"\"\"\n",
    "\n",
    "# Create a comprehensive configuration (matching the example command)\n",
    "config = AdvancedPipelineConfig(\n",
    "    chunk_max_tokens=256,\n",
    "    organize_output=True,\n",
    "    export_figures=True,\n",
    "    export_tables=True,\n",
    "    save_metadata=True,\n",
    "    describe_images=True,\n",
    "    llm_model=\"claude-3-haiku-20240307\",\n",
    "    output_dir=\"workshop_output\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AdvancedPipelineConfig class defined!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(config.summary())\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üìä Structured Output System Explained\n",
    "\n",
    "The structured output system creates a comprehensive, organized hierarchy that makes it easy to manage and access all processed content. Let's understand each component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ STRUCTURED OUTPUT HIERARCHY:\n",
      "\n",
      "workshop_output/\n",
      "‚îî‚îÄ‚îÄ files/                           # Categorized by type (files vs audio)\n",
      "    ‚îî‚îÄ‚îÄ document_name/               # One folder per document\n",
      "        ‚îú‚îÄ‚îÄ json/                    # Main content\n",
      "        ‚îÇ   ‚îú‚îÄ‚îÄ document_name.json   # Complete processed data\n",
      "        ‚îÇ   ‚îî‚îÄ‚îÄ document_name.md     # Markdown export\n",
      "        ‚îÇ\n",
      "        ‚îú‚îÄ‚îÄ metadata/                # Metadata only\n",
      "        ‚îÇ   ‚îî‚îÄ‚îÄ document_name_metadata.json\n",
      "        ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ exports/                 \n",
      "            ‚îú‚îÄ‚îÄ figures/             \n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-picture-1.png\n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-picture-2.png\n",
      "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "            ‚îÇ\n",
      "            ‚îú‚îÄ‚îÄ tables/              \n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.png      # Visual\n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.csv      # Data  \n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.html     # Formatted\n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.md       # Markdown\n",
      "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "            ‚îÇ\n",
      "            ‚îú‚îÄ‚îÄ pages/               # Page screenshots\n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-page-1.png\n",
      "            ‚îÇ   ‚îú‚îÄ‚îÄ document-page-2.png\n",
      "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "    \n",
      "\n",
      "üîç COMPONENT EXPLANATIONS:\n",
      "\n",
      "üìÑ json/\n",
      "   Contains the main processed data as JSON and the original text as Markdown. This is your primary content for RAG.\n",
      "\n",
      "üìã metadata/\n",
      "   Separate metadata file with processing config, document stats, file info, and quality metrics.\n",
      "\n",
      "üñºÔ∏è figures/\n",
      "   All pictures, diagrams, charts, and visual elements extracted as high-res PNG files for multimodal RAG.\n",
      "\n",
      "üìä tables/\n",
      "   Tables in multiple formats: PNG images for visual retrieval, CSV for data analysis, HTML for web display, Markdown for text processing.\n",
      "\n",
      "üìñ pages/\n",
      "   Screenshots of each document page, useful for layout-aware applications and visual document search.\n",
      "\n",
      "ü§ñ image-descriptions.json\n",
      "   LLM-generated descriptions of all images, with cost tracking and metadata. Makes visual content searchable via text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explain_output_structure():\n",
    "    \"\"\"Show the complete output structure with explanations.\"\"\"\n",
    "    \n",
    "    structure_diagram = \"\"\"\n",
    "üìÅ STRUCTURED OUTPUT HIERARCHY:\n",
    "\n",
    "workshop_output/\n",
    "‚îî‚îÄ‚îÄ files/                           # Categorized by type (files vs audio)\n",
    "    ‚îî‚îÄ‚îÄ document_name/               # One folder per document\n",
    "        ‚îú‚îÄ‚îÄ json/                    # Main content\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ document_name.json   # Complete processed data\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ document_name.md     # Markdown export\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚îÄ metadata/                # Metadata only\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ document_name_metadata.json\n",
    "        ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ exports/                 \n",
    "            ‚îú‚îÄ‚îÄ figures/             \n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-picture-1.png\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-picture-2.png\n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "            ‚îÇ\n",
    "            ‚îú‚îÄ‚îÄ tables/              \n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.png      # Visual\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.csv      # Data  \n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.html     # Formatted\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-table-1.md       # Markdown\n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "            ‚îÇ\n",
    "            ‚îú‚îÄ‚îÄ pages/               # Page screenshots\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-page-1.png\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ document-page-2.png\n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    \"\"\"\n",
    "    \n",
    "    explanations = {\n",
    "        \"üìÑ json/\": \"Contains the main processed data as JSON and the original text as Markdown. This is your primary content for RAG.\",\n",
    "        \n",
    "        \"üìã metadata/\": \"Separate metadata file with processing config, document stats, file info, and quality metrics.\",\n",
    "        \n",
    "        \"üñºÔ∏è figures/\": \"All pictures, diagrams, charts, and visual elements extracted as high-res PNG files for multimodal RAG.\",\n",
    "        \n",
    "        \"üìä tables/\": \"Tables in multiple formats: PNG images for visual retrieval, CSV for data analysis, HTML for web display, Markdown for text processing.\",\n",
    "        \n",
    "        \"üìñ pages/\": \"Screenshots of each document page, useful for layout-aware applications and visual document search.\",\n",
    "        \n",
    "        \"ü§ñ image-descriptions.json\": \"LLM-generated descriptions of all images, with cost tracking and metadata. Makes visual content searchable via text.\"\n",
    "    }\n",
    "    \n",
    "    print(structure_diagram)\n",
    "    print(\"\\nüîç COMPONENT EXPLANATIONS:\\n\")\n",
    "    \n",
    "    for component, explanation in explanations.items():\n",
    "        print(f\"{component}\")\n",
    "        print(f\"   {explanation}\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "explain_output_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß† Comprehensive Document Processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ComprehensiveDocumentProcessor class defined!\n",
      "üîß Class capabilities:\n",
      "  ‚Ä¢ Document formats: PDF, DOCX\n",
      "  ‚Ä¢ Chunking: Hybrid chunking with configurable token limits\n",
      "  ‚Ä¢ Exports: Figures, tables, pages, markdown\n",
      "  ‚Ä¢ Output organization: Structured folders with metadata\n",
      "  ‚Ä¢ LLM integration: Image descriptions and content analysis\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveDocumentProcessor:\n",
    "    \"\"\"Full-featured document processor with all advanced capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[AdvancedPipelineConfig] = None):\n",
    "        \"\"\"Initialize the comprehensive processor.\"\"\"\n",
    "        self.config = config or AdvancedPipelineConfig()\n",
    "        \n",
    "        # Initialize DocumentConverter\n",
    "        self.converter = self._initialize_converter()\n",
    "        \n",
    "        # Initialize HybridChunker\n",
    "        self.hybrid_chunker = None\n",
    "        self.tokenizer = None\n",
    "        self._initialize_hybrid_chunker()\n",
    "        \n",
    "        # Store conversion result for exports\n",
    "        self._conversion_result = None\n",
    "    \n",
    "    def _initialize_converter(self) -> DocumentConverter:\n",
    "        \"\"\"Initialize DocumentConverter with multi-format support.\"\"\"\n",
    "        try:\n",
    "            pipeline_options = self.config.to_pipeline_options()\n",
    "            \n",
    "            format_options = {}\n",
    "            if InputFormat.PDF in self.config.allowed_formats and pipeline_options:\n",
    "                format_options[InputFormat.PDF] = PdfFormatOption(\n",
    "                    pipeline_cls=StandardPdfPipeline,\n",
    "                    backend=PyPdfiumDocumentBackend,\n",
    "                    pipeline_options=pipeline_options\n",
    "                )\n",
    "            \n",
    "            converter = DocumentConverter(\n",
    "                allowed_formats=self.config.allowed_formats,\n",
    "                format_options=format_options if format_options else None\n",
    "            )\n",
    "            \n",
    "            formats_str = ', '.join([f.name for f in self.config.allowed_formats])\n",
    "            print(f\"‚úÖ Initialized converter for formats: {formats_str}\")\n",
    "            return converter\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not apply format options: {e}\")\n",
    "            return DocumentConverter()\n",
    "    \n",
    "    def _initialize_hybrid_chunker(self):\n",
    "        \"\"\"Initialize the hybrid chunker with tokenizer.\"\"\"\n",
    "        try:\n",
    "            print(f\"üîß Loading tokenizer: {self.config.embedding_model}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.embedding_model)\n",
    "            \n",
    "            self.hybrid_chunker = HybridChunker(\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_tokens=self.config.chunk_max_tokens,\n",
    "                merge_peers=self.config.chunk_merge_peers\n",
    "            )\n",
    "            print(f\"‚úÖ Initialized HybridChunker (max_tokens={self.config.chunk_max_tokens})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: Could not initialize hybrid chunker: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a document with full structured output.\"\"\"\n",
    "        try:\n",
    "            print(f\"üîÑ Processing document: {Path(file_path).name}\")\n",
    "            \n",
    "            # Convert document\n",
    "            result = self.converter.convert(file_path)\n",
    "            doc = result.document\n",
    "            self._conversion_result = result\n",
    "            \n",
    "            # Create output structure\n",
    "            doc_folder = self._get_output_folder(file_path)\n",
    "            \n",
    "            # Extract comprehensive metadata\n",
    "            metadata = self._extract_comprehensive_metadata(result, file_path, doc_folder)\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self._create_hybrid_chunks(doc)\n",
    "            \n",
    "            # Extract tables and headers\n",
    "            tables = self._extract_tables(doc)\n",
    "            headers = self._extract_headers(doc)\n",
    "            \n",
    "            # Handle exports (figures, tables)\n",
    "            exports_info = {}\n",
    "            if self.config.export_figures or self.config.export_tables:\n",
    "                exports_info = self._handle_exports(result, doc, file_path, doc_folder)\n",
    "            \n",
    "            # Prepare complete processed data\n",
    "            processed_data = {\n",
    "                \"metadata\": metadata,\n",
    "                \"content\": {\n",
    "                    \"full_text\": doc.export_to_markdown(),\n",
    "                    \"structured_content\": json.loads(doc.to_json()) if hasattr(doc, 'to_json') else {},\n",
    "                },\n",
    "                \"chunks\": chunks,\n",
    "                \"tables\": tables,\n",
    "                \"headers\": headers,\n",
    "                \"exports\": exports_info,\n",
    "                \"document_stats\": {\n",
    "                    \"total_characters\": len(doc.export_to_markdown()),\n",
    "                    \"total_words\": len(doc.export_to_markdown().split()),\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"total_tables\": len(tables),\n",
    "                    \"total_headers\": len(headers),\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create structured output if enabled\n",
    "            if self.config.organize_output:\n",
    "                output_structure = self._create_output_structure(doc_folder, processed_data)\n",
    "                processed_data[\"output_structure\"] = output_structure\n",
    "            \n",
    "            print(f\"‚úÖ Document processed successfully! Created {len(chunks)} chunks\")\n",
    "            return processed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Failed to process document: {str(e)}\",\n",
    "                \"metadata\": {\"source_file\": str(file_path)},\n",
    "            }\n",
    "    \n",
    "    def _get_output_folder(self, file_path: str) -> Path:\n",
    "        \"\"\"Determine output folder structure.\"\"\"\n",
    "        if self.config.organize_output:\n",
    "            return self.config.output_dir / \"files\" / Path(file_path).stem\n",
    "        else:\n",
    "            return self.config.output_dir\n",
    "    \n",
    "    def _create_hybrid_chunks(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create chunks using Docling's HybridChunker.\"\"\"\n",
    "        print(\"üß© Creating hybrid chunks...\")\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            chunk_iter = self.hybrid_chunker.chunk(dl_doc=doc)\n",
    "            \n",
    "            for i, chunk in enumerate(chunk_iter):\n",
    "                contextualized_text = self.hybrid_chunker.contextualize(chunk=chunk)\n",
    "                \n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": i,\n",
    "                    \"text\": chunk.text,\n",
    "                    \"contextualized_text\": contextualized_text,\n",
    "                    \"token_count\": len(self.tokenizer.encode(chunk.text)) if self.tokenizer else len(chunk.text.split()),\n",
    "                    \"char_count\": len(chunk.text),\n",
    "                    \"contextualized_char_count\": len(contextualized_text),\n",
    "                    \"metadata\": {\n",
    "                        \"headings\": getattr(chunk.meta, 'headings', []) if hasattr(chunk, 'meta') else [],\n",
    "                        \"page_info\": getattr(chunk.meta, 'page_info', []) if hasattr(chunk, 'meta') else [],\n",
    "                        \"content_type\": getattr(chunk.meta, 'content_type', None) if hasattr(chunk, 'meta') else None,\n",
    "                        \"chunk_type\": \"hybrid\"\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk_data)\n",
    "            \n",
    "            print(f\"‚úÖ Created {len(chunks)} hybrid chunks\")\n",
    "            return chunks\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: Hybrid chunking failed: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _extract_tables(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract table information from the document.\"\"\"\n",
    "        tables = []\n",
    "        try:\n",
    "            if hasattr(doc, 'to_json'):\n",
    "                doc_dict = json.loads(doc.to_json())\n",
    "                if 'tables' in doc_dict:\n",
    "                    for i, table in enumerate(doc_dict['tables']):\n",
    "                        tables.append({\n",
    "                            \"table_id\": i,\n",
    "                            \"content\": table,\n",
    "                            \"extraction_method\": \"structured_json\"\n",
    "                        })\n",
    "            \n",
    "            if hasattr(doc, 'tables'):\n",
    "                for i, table in enumerate(doc.tables):\n",
    "                    tables.append({\n",
    "                        \"table_id\": len(tables),\n",
    "                        \"content\": str(table) if hasattr(table, '__str__') else table,\n",
    "                        \"extraction_method\": \"direct_attribute\"\n",
    "                    })\n",
    "                    \n",
    "            if tables:\n",
    "                print(f\"‚úÖ Extracted {len(tables)} tables\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not extract tables: {e}\")\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def _extract_headers(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract header information from the document.\"\"\"\n",
    "        headers = []\n",
    "        try:\n",
    "            markdown_content = doc.export_to_markdown()\n",
    "            lines = markdown_content.split('\\n')\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if line.startswith('#'):\n",
    "                    level = len(line) - len(line.lstrip('#'))\n",
    "                    text = line.lstrip('#').strip()\n",
    "                    if text:\n",
    "                        headers.append({\n",
    "                            \"level\": level,\n",
    "                            \"text\": text,\n",
    "                            \"line_number\": i,\n",
    "                        })\n",
    "                        \n",
    "            if headers:\n",
    "                print(f\"‚úÖ Extracted {len(headers)} headers\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not extract headers: {e}\")\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def _extract_comprehensive_metadata(self, result, file_path: str, doc_folder: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive metadata about the document and processing.\"\"\"\n",
    "        file_path_obj = Path(file_path)\n",
    "        \n",
    "        metadata = {\n",
    "            \"source_file\": str(file_path),\n",
    "            \"file_name\": file_path_obj.name,\n",
    "            \"file_stem\": file_path_obj.stem,\n",
    "            \"file_type\": file_path_obj.suffix.lower(),\n",
    "            \"file_size_bytes\": file_path_obj.stat().st_size if file_path_obj.exists() else 0,\n",
    "            \"title\": getattr(result.document, 'title', None) or file_path_obj.stem,\n",
    "            \"output_folder\": str(doc_folder),\n",
    "            \"processing_config\": {\n",
    "                \"chunking_method\": \"hybrid\",\n",
    "                \"max_tokens_per_chunk\": self.config.chunk_max_tokens,\n",
    "                \"ocr_enabled\": self.config.do_ocr,\n",
    "                \"table_structure_enabled\": self.config.do_table_structure,\n",
    "                \"export_figures\": self.config.export_figures,\n",
    "                \"export_tables\": self.config.export_tables,\n",
    "                \"organize_output\": self.config.organize_output,\n",
    "                \"describe_images\": self.config.describe_images,\n",
    "                \"llm_model\": self.config.llm_model if self.config.describe_images else None,\n",
    "                \"embedding_model\": self.config.embedding_model,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # Document structure metadata\n",
    "        doc = result.document\n",
    "        if hasattr(doc, 'pages'):\n",
    "            metadata[\"page_count\"] = len(doc.pages) if doc.pages else 0\n",
    "        \n",
    "        if hasattr(doc, 'tables'):\n",
    "            metadata[\"table_count\"] = len(doc.tables) if doc.tables else 0\n",
    "        \n",
    "        # Content statistics\n",
    "        full_text = doc.export_to_markdown()\n",
    "        metadata[\"content_stats\"] = {\n",
    "            \"total_characters\": len(full_text),\n",
    "            \"total_words\": len(full_text.split()),\n",
    "            \"total_lines\": len(full_text.split('\\n')),\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _create_output_structure(self, doc_folder: Path, processed_data: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Create organized output folder structure and save files.\"\"\"\n",
    "        folders = {\n",
    "            \"json_folder\": doc_folder / \"json\",\n",
    "            \"metadata_folder\": doc_folder / \"metadata\",\n",
    "            \"exports_folder\": doc_folder / \"exports\",\n",
    "            \"figures_folder\": doc_folder / \"exports\" / \"figures\",\n",
    "            \"tables_folder\": doc_folder / \"exports\" / \"tables\", \n",
    "            \"pages_folder\": doc_folder / \"exports\" / \"pages\",\n",
    "        }\n",
    "        \n",
    "        # Create directories\n",
    "        for folder in folders.values():\n",
    "            folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save JSON output\n",
    "        json_file = folders[\"json_folder\"] / f\"{processed_data['metadata']['file_stem']}.json\"\n",
    "        with json_file.open('w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save metadata separately\n",
    "        metadata_file = folders[\"metadata_folder\"] / f\"{processed_data['metadata']['file_stem']}_metadata.json\"\n",
    "        with metadata_file.open('w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data['metadata'], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save markdown if enabled\n",
    "        if self.config.export_markdown and hasattr(self._conversion_result, 'document'):\n",
    "            markdown_file = folders[\"json_folder\"] / f\"{processed_data['metadata']['file_stem']}.md\"\n",
    "            with markdown_file.open('w', encoding='utf-8') as f:\n",
    "                f.write(self._conversion_result.document.export_to_markdown())\n",
    "        \n",
    "        print(f\"üìÅ Created structured output in: {doc_folder}\")\n",
    "        return {k: str(v) for k, v in folders.items()}\n",
    "    \n",
    "    # We'll add the export methods in the next cell due to length...\n",
    "\n",
    "print(\"‚úÖ ComprehensiveDocumentProcessor class defined!\")\n",
    "\n",
    "print(\"üîß Class capabilities:\")\n",
    "print(\"  ‚Ä¢ Document formats: PDF, DOCX\")\n",
    "print(\"  ‚Ä¢ Chunking: Hybrid chunking with configurable token limits\")\n",
    "print(\"  ‚Ä¢ Exports: Figures, tables, pages, markdown\")\n",
    "print(\"  ‚Ä¢ Output organization: Structured folders with metadata\")\n",
    "print(\"  ‚Ä¢ LLM integration: Image descriptions and content analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Export functionality added to ComprehensiveDocumentProcessor!\n"
     ]
    }
   ],
   "source": [
    "# Add the export methods to the ComprehensiveDocumentProcessor class\n",
    "# This extends the class with figure/table export and LLM description capabilities\n",
    "\n",
    "def _handle_exports(self, result, doc, file_path: str, doc_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Handle figure and table exports based on configuration.\"\"\"\n",
    "    exports_info = {\n",
    "        \"figures_exported\": 0,\n",
    "        \"tables_exported\": 0,\n",
    "        \"pages_exported\": 0,\n",
    "        \"export_directory\": str(doc_folder / \"exports\"),\n",
    "        \"exported_files\": [],\n",
    "        \"image_descriptions\": []\n",
    "    }\n",
    "    \n",
    "    if not (self.config.export_figures or self.config.export_tables):\n",
    "        return exports_info\n",
    "        \n",
    "    # Create output directories\n",
    "    exports_folder = doc_folder / \"exports\"\n",
    "    figures_folder = exports_folder / \"figures\"\n",
    "    tables_folder = exports_folder / \"tables\"\n",
    "    pages_folder = exports_folder / \"pages\"\n",
    "    \n",
    "    for folder in [exports_folder, figures_folder, tables_folder, pages_folder]:\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    doc_filename = Path(file_path).stem\n",
    "    \n",
    "    # Export figures if enabled\n",
    "    if self.config.export_figures:\n",
    "        figure_info = self._export_figures(result, doc, doc_filename, figures_folder, tables_folder, pages_folder, exports_folder)\n",
    "        exports_info.update(figure_info)\n",
    "        \n",
    "    # Export tables if enabled\n",
    "    if self.config.export_tables:\n",
    "        table_info = self._export_tables(result, doc, doc_filename, tables_folder)\n",
    "        exports_info.update(table_info)\n",
    "        \n",
    "    return exports_info\n",
    "\n",
    "def _export_figures(self, result, doc, doc_filename: str, figures_folder: Path, tables_folder: Path, pages_folder: Path, exports_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Export figures, page images, and pictures with optional LLM descriptions.\"\"\"\n",
    "    print(\"üñºÔ∏è  Exporting figures...\")\n",
    "    exported_files = []\n",
    "    image_descriptions = []\n",
    "    \n",
    "    try:\n",
    "        # Save page images to pages subfolder\n",
    "        page_counter = 0\n",
    "        if hasattr(result, 'document') and hasattr(result.document, 'pages'):\n",
    "            for page_no, page in result.document.pages.items():\n",
    "                if hasattr(page, 'image') and page.image:\n",
    "                    page_counter += 1\n",
    "                    page_image_filename = pages_folder / f\"{doc_filename}-page-{page.page_no}.png\"\n",
    "                    with page_image_filename.open(\"wb\") as fp:\n",
    "                        page.image.pil_image.save(fp, format=\"PNG\")\n",
    "                    exported_files.append(str(page_image_filename))\n",
    "        \n",
    "        # Save images of figures and tables\n",
    "        table_counter = 0\n",
    "        picture_counter = 0\n",
    "        \n",
    "        if hasattr(result, 'document'):\n",
    "            for element, _level in result.document.iterate_items():\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter += 1\n",
    "                    element_image_filename = tables_folder / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "                    try:\n",
    "                        with element_image_filename.open(\"wb\") as fp:\n",
    "                            element.get_image(result.document).save(fp, \"PNG\")\n",
    "                        exported_files.append(str(element_image_filename))\n",
    "                        \n",
    "                        # Add LLM description if enabled\n",
    "                        if self.config.describe_images and LITELLM_AVAILABLE:\n",
    "                            desc_result = self._describe_image_with_llm(element_image_filename, self.config.llm_model)\n",
    "                            desc_result.update({\n",
    "                                \"type\": \"table\",\n",
    "                                \"image_filename\": element_image_filename.name,\n",
    "                                \"sequence_number\": table_counter\n",
    "                            })\n",
    "                            image_descriptions.append(desc_result)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Could not export table {table_counter} image: {e}\")\n",
    "                \n",
    "                if isinstance(element, PictureItem):\n",
    "                    picture_counter += 1\n",
    "                    element_image_filename = figures_folder / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "                    try:\n",
    "                        with element_image_filename.open(\"wb\") as fp:\n",
    "                            element.get_image(result.document).save(fp, \"PNG\")\n",
    "                        exported_files.append(str(element_image_filename))\n",
    "                        \n",
    "                        # Add LLM description if enabled\n",
    "                        if self.config.describe_images and LITELLM_AVAILABLE:\n",
    "                            desc_result = self._describe_image_with_llm(element_image_filename, self.config.llm_model)\n",
    "                            desc_result.update({\n",
    "                                \"type\": \"picture/figure\",\n",
    "                                \"image_filename\": element_image_filename.name,\n",
    "                                \"sequence_number\": picture_counter\n",
    "                            })\n",
    "                            image_descriptions.append(desc_result)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Could not export picture {picture_counter} image: {e}\")\n",
    "        \n",
    "        # Save consolidated image descriptions if any were generated\n",
    "        if self.config.describe_images and image_descriptions:\n",
    "            consolidated_descriptions = {\n",
    "                \"document_name\": doc_filename,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_images\": len(image_descriptions),\n",
    "                \"total_cost\": sum(desc.get(\"cost\", 0) for desc in image_descriptions),\n",
    "                \"total_input_tokens\": sum(desc.get(\"input_tokens\", 0) for desc in image_descriptions),\n",
    "                \"total_output_tokens\": sum(desc.get(\"output_tokens\", 0) for desc in image_descriptions),\n",
    "                \"model_used\": self.config.llm_model,\n",
    "                \"descriptions\": image_descriptions\n",
    "            }\n",
    "            \n",
    "            consolidated_filename = exports_folder / f\"{doc_filename}-image-descriptions.json\"\n",
    "            with consolidated_filename.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                json.dump(consolidated_descriptions, fp, indent=2, ensure_ascii=False)\n",
    "            exported_files.append(str(consolidated_filename))\n",
    "            print(f\"ü§ñ Generated {len(image_descriptions)} LLM image descriptions\")\n",
    "        \n",
    "        print(f\"‚úÖ Exported {len(exported_files)} figure files\")\n",
    "        return {\n",
    "            \"figures_exported\": table_counter + picture_counter,\n",
    "            \"pages_exported\": page_counter,\n",
    "            \"figure_files\": exported_files,\n",
    "            \"image_descriptions\": image_descriptions\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Warning: Figure export failed: {e}\")\n",
    "        return {\"figures_exported\": 0, \"figure_files\": []}\n",
    "\n",
    "def _export_tables(self, result, doc, doc_filename: str, tables_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Export tables to various formats (CSV, HTML, Markdown).\"\"\"\n",
    "    print(\"üìä Exporting tables...\")\n",
    "    exported_files = []\n",
    "    \n",
    "    try:\n",
    "        table_counter = 0\n",
    "        if hasattr(doc, 'tables'):\n",
    "            for table_ix, table in enumerate(doc.tables):\n",
    "                table_counter += 1\n",
    "                \n",
    "                try:\n",
    "                    table_df = table.export_to_dataframe(doc=doc)\n",
    "                    \n",
    "                    # Save as CSV\n",
    "                    csv_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.csv\"\n",
    "                    table_df.to_csv(csv_filename, index=False)\n",
    "                    exported_files.append(str(csv_filename))\n",
    "                    \n",
    "                    # Save as HTML\n",
    "                    html_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.html\"\n",
    "                    with html_filename.open(\"w\") as fp:\n",
    "                        fp.write(table.export_to_html(doc=doc))\n",
    "                    exported_files.append(str(html_filename))\n",
    "                    \n",
    "                    # Save as Markdown\n",
    "                    md_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.md\"\n",
    "                    with md_filename.open(\"w\") as fp:\n",
    "                        fp.write(f\"## Table {table_ix + 1}\\n\\n\")\n",
    "                        fp.write(table_df.to_markdown(index=False))\n",
    "                    exported_files.append(str(md_filename))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Could not export table {table_ix + 1}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Exported {table_counter} tables in {len(exported_files)} files\")\n",
    "        return {\n",
    "            \"tables_exported\": table_counter,\n",
    "            \"table_files\": exported_files\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Warning: Table export failed: {e}\")\n",
    "        return {\"tables_exported\": 0, \"table_files\": []}\n",
    "\n",
    "def _describe_image_with_llm(self, image_path: Path, model: str = \"claude-3-haiku-20240307\") -> Dict[str, Any]:\n",
    "    \"\"\"Describe an image using LiteLLM with any supported model.\"\"\"\n",
    "    if not LITELLM_AVAILABLE:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"LiteLLM not available - install with 'pip install litellm'\",\n",
    "            \"description\": None,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"cost\": 0.0\n",
    "        }\n",
    "        \n",
    "    try:\n",
    "        # Get API key from environment\n",
    "        api_key = None\n",
    "        if \"claude\" in model.lower():\n",
    "            api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        elif \"gpt\" in model.lower() or \"openai\" in model.lower():\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not api_key:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"No API key found for model {model}. Set ANTHROPIC_API_KEY or OPENAI_API_KEY.\",\n",
    "                \"description\": None,\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"cost\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Convert image to base64\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        prompt_text = \"Describe this image in detail. Focus on the main content, text, data, charts, diagrams, or any other relevant information that would be useful for document understanding and search.\"\n",
    "        \n",
    "        # Use LiteLLM to describe the image\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        description = response.choices[0].message.content.strip()\n",
    "        usage = getattr(response, 'usage', None)\n",
    "        input_tokens = getattr(usage, 'prompt_tokens', 0) if usage else 0\n",
    "        output_tokens = getattr(usage, 'completion_tokens', 0) if usage else 0\n",
    "        \n",
    "        # Calculate cost if possible\n",
    "        cost = 0.0\n",
    "        try:\n",
    "            cost = litellm.completion_cost(completion_response=response) or 0.0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"description\": description,\n",
    "            \"image_path\": str(image_path),\n",
    "            \"model\": model,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost\": cost,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"description\": None,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"cost\": 0.0,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Add these methods to our ComprehensiveDocumentProcessor class\n",
    "ComprehensiveDocumentProcessor._handle_exports = _handle_exports\n",
    "ComprehensiveDocumentProcessor._export_figures = _export_figures  \n",
    "ComprehensiveDocumentProcessor._export_tables = _export_tables\n",
    "ComprehensiveDocumentProcessor._describe_image_with_llm = _describe_image_with_llm\n",
    "\n",
    "print(\"‚úÖ Export functionality added to ComprehensiveDocumentProcessor!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üéØ Complete Demo: Processing a Document with Full Features\n",
    "\n",
    "Now let's demonstrate the complete functionality by processing a document with all features enabled, just like the command-line example!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized converter for formats: PDF, DOCX, PPTX, XLSX, HTML, MD, IMAGE\n",
      "üîß Loading tokenizer: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Initialized HybridChunker (max_tokens=256)\n",
      " Using document path: ../examples_files/const_ai_anthropic.pdf\n",
      "\n",
      "Configuration Summary:\n",
      "============================================================\n",
      "üîß Advanced Pipeline Configuration:\n",
      "\n",
      "üìä CORE PROCESSING:\n",
      "        - OCR: ‚úì\n",
      "        - Table Structure: ‚úì\n",
      "        - Page Images: ‚úì\n",
      "        - Picture Images: ‚úì\n",
      "\n",
      "üß© CHUNKING:\n",
      "        - Method: Hybrid (tokenization-aware)\n",
      "        - Max Tokens/Chunk: 256\n",
      "        - Merge Peers: ‚úì\n",
      "        - Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "üìÅ STRUCTURED OUTPUT:\n",
      "        - Organize Output: ‚úì\n",
      "        - Save Metadata: ‚úì\n",
      "        - Export Markdown: ‚úì\n",
      "        - Output Directory: workshop_output\n",
      "\n",
      "üñºÔ∏è EXPORTS:\n",
      "        - Export Figures: ‚úì\n",
      "        - Export Tables: ‚úì\n",
      "        - Images Scale: 2.0x (144 DPI)\n",
      "\n",
      "ü§ñ LLM FEATURES:\n",
      "        - Describe Images: ‚úì\n",
      "        - LLM Model: claude-3-haiku-20240307\n",
      "        - LiteLLM Available: ‚úó\n",
      "\n",
      "‚öôÔ∏è FORMATS:\n",
      "        - Supported: PDF, DOCX, PPTX, XLSX, HTML, MD, IMAGE\n"
     ]
    }
   ],
   "source": [
    "# Initialize the comprehensive processor with full features\n",
    "processor = ComprehensiveDocumentProcessor(config)\n",
    "\n",
    "# Check for available example files\n",
    "document_path = \"../examples_files/const_ai_anthropic.pdf\"\n",
    "print(f\" Using document path: {document_path}\")\n",
    "\n",
    "\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(config.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 16:23:13,413 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-05 16:23:13,417 - INFO - Going to convert document batch...\n",
      "2025-10-05 16:23:13,418 - INFO - Initializing pipeline for StandardPdfPipeline with options hash ce2db4bc6b59e8bf84cfaffa1879c953\n",
      "2025-10-05 16:23:13,419 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive document processing...\n",
      "This may take a few minutes depending on document size and features enabled.\n",
      "üîÑ Processing document: const_ai_anthropic.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 16:23:17,294 - INFO - Accelerator device: 'mps'\n",
      "2025-10-05 16:23:19,633 - INFO - Accelerator device: 'mps'\n",
      "2025-10-05 16:23:20,190 - INFO - Processing document const_ai_anthropic.pdf\n",
      "2025-10-05 16:23:55,146 - INFO - Finished converting document const_ai_anthropic.pdf in 41.74 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Creating hybrid chunks...\n",
      "‚úÖ Created 152 hybrid chunks\n",
      "‚úÖ Extracted 11 tables\n",
      "‚úÖ Extracted 47 headers\n",
      "üñºÔ∏è  Exporting figures...\n",
      "‚úÖ Exported 57 figure files\n",
      "üìä Exporting tables...\n",
      "‚úÖ Exported 11 tables in 33 files\n",
      "üìÅ Created structured output in: workshop_output/files/const_ai_anthropic\n",
      "‚úÖ Document processed successfully! Created 152 chunks\n",
      "\n",
      "üéâ Document processed successfully!\n",
      "\n",
      "üìä PROCESSING RESULTS:\n",
      "============================================================\n",
      "üìÑ Document: const_ai_anthropic.pdf\n",
      "üìÅ Output folder: workshop_output/files/const_ai_anthropic\n",
      "üìè File size: 2,088,111 bytes\n",
      "üìñ Pages: 34\n",
      "\n",
      "üß© CONTENT ANALYSIS:\n",
      "   ‚Ä¢ Total characters: 150,926\n",
      "   ‚Ä¢ Total words: 18,968\n",
      "   ‚Ä¢ Total chunks: 152\n",
      "   ‚Ä¢ Total tables: 11\n",
      "   ‚Ä¢ Total headers: 47\n",
      "   ‚Ä¢ Avg chars/chunk: 993\n",
      "   ‚Ä¢ Avg words/chunk: 125\n",
      "\n",
      "üé® EXPORTS SUMMARY:\n",
      "   ‚Ä¢ Figures exported: 23\n",
      "   ‚Ä¢ Tables exported: 11\n",
      "   ‚Ä¢ Pages exported: 34\n",
      "   ‚Ä¢ Export files created: 57\n",
      "\n",
      "üìÅ STRUCTURED OUTPUT CREATED:\n",
      "   ‚Ä¢ json: workshop_output/files/const_ai_anthropic/json\n",
      "   ‚Ä¢ metadata: workshop_output/files/const_ai_anthropic/metadata\n",
      "   ‚Ä¢ exports: workshop_output/files/const_ai_anthropic/exports\n",
      "   ‚Ä¢ figures: workshop_output/files/const_ai_anthropic/exports/figures\n",
      "   ‚Ä¢ tables: workshop_output/files/const_ai_anthropic/exports/tables\n",
      "   ‚Ä¢ pages: workshop_output/files/const_ai_anthropic/exports/pages\n"
     ]
    }
   ],
   "source": [
    "# Process the document with full features\n",
    "print(\"üöÄ Starting comprehensive document processing...\")\n",
    "print(\"This may take a few minutes depending on document size and features enabled.\")\n",
    "\n",
    "result = processor.process_document(document_path)\n",
    "\n",
    "# Analyze and display results\n",
    "if \"error\" in result:\n",
    "    print(f\"‚ùå Error processing document: {result['error']}\")\n",
    "    print(\"üí° Make sure the document path is correct and you have the required API keys if using image descriptions.\")\n",
    "else:\n",
    "    print(f\"\\nüéâ Document processed successfully!\")\n",
    "    \n",
    "    # Display comprehensive summary\n",
    "    stats = result[\"document_stats\"]\n",
    "    metadata = result[\"metadata\"]\n",
    "    exports = result.get(\"exports\", {})\n",
    "    \n",
    "    print(f\"\\nüìä PROCESSING RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÑ Document: {metadata['file_name']}\")\n",
    "    print(f\"üìÅ Output folder: {metadata['output_folder']}\")\n",
    "    print(f\"üìè File size: {metadata['file_size_bytes']:,} bytes\")\n",
    "    if \"page_count\" in metadata:\n",
    "        print(f\"üìñ Pages: {metadata['page_count']}\")\n",
    "    \n",
    "    print(f\"\\nüß© CONTENT ANALYSIS:\")\n",
    "    print(f\"   ‚Ä¢ Total characters: {stats['total_characters']:,}\")\n",
    "    print(f\"   ‚Ä¢ Total words: {stats['total_words']:,}\")\n",
    "    print(f\"   ‚Ä¢ Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"   ‚Ä¢ Total tables: {stats['total_tables']}\")\n",
    "    print(f\"   ‚Ä¢ Total headers: {stats['total_headers']}\")\n",
    "    \n",
    "    if stats['total_chunks'] > 0:\n",
    "        avg_chars_per_chunk = stats['total_characters'] / stats['total_chunks']\n",
    "        avg_words_per_chunk = stats['total_words'] / stats['total_chunks']\n",
    "        print(f\"   ‚Ä¢ Avg chars/chunk: {avg_chars_per_chunk:.0f}\")\n",
    "        print(f\"   ‚Ä¢ Avg words/chunk: {avg_words_per_chunk:.0f}\")\n",
    "    \n",
    "    print(f\"\\nüé® EXPORTS SUMMARY:\")\n",
    "    if exports:\n",
    "        if exports.get('figures_exported', 0) > 0:\n",
    "            print(f\"   ‚Ä¢ Figures exported: {exports['figures_exported']}\")\n",
    "        if exports.get('tables_exported', 0) > 0:\n",
    "            print(f\"   ‚Ä¢ Tables exported: {exports['tables_exported']}\")\n",
    "        if exports.get('pages_exported', 0) > 0:\n",
    "            print(f\"   ‚Ä¢ Pages exported: {exports['pages_exported']}\")\n",
    "        if exports.get('figure_files'):\n",
    "            print(f\"   ‚Ä¢ Export files created: {len(exports['figure_files'])}\")\n",
    "        if exports.get('image_descriptions'):\n",
    "            total_cost = sum(desc.get(\"cost\", 0) for desc in exports['image_descriptions'])\n",
    "            total_tokens = sum(desc.get(\"input_tokens\", 0) + desc.get(\"output_tokens\", 0) for desc in exports['image_descriptions'])\n",
    "            print(f\"   ‚Ä¢ LLM descriptions: {len(exports['image_descriptions'])}\")\n",
    "            print(f\"   ‚Ä¢ Total LLM cost: ${total_cost:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Total LLM tokens: {total_tokens:,}\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ No exports configured\")\n",
    "    \n",
    "    if \"output_structure\" in result:\n",
    "        print(f\"\\nüìÅ STRUCTURED OUTPUT CREATED:\")\n",
    "        structure = result[\"output_structure\"]\n",
    "        for folder_type, folder_path in structure.items():\n",
    "            folder_name = folder_type.replace(\"_folder\", \"\")\n",
    "            print(f\"   ‚Ä¢ {folder_name}: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ What We've Done\n",
    "\n",
    "- **üîß UV Package Management**: Modern Python dependency management\n",
    "- **‚öôÔ∏è Advanced Pipeline Configuration**: All document processing options\n",
    "- **üìä Structured Output System**: Organized, production-ready folder hierarchies  \n",
    "- **üñºÔ∏è Figure & Table Exports**: Multi-format visual content extraction\n",
    "- **ü§ñ LLM Image Descriptions**: AI-powered multimodal processing capabilities\n",
    "- **üß© Hybrid Chunking**: Smart, context-aware chunking for optimal RAG\n",
    "- **üìà Comprehensive Analysis**: Quality metrics and visualization tools\n",
    "\n",
    "\n",
    "- **Docling Documentation**: [docling-project.github.io](https://docling-project.github.io/docling/)\n",
    "- **LiteLLM Docs**: [docs.litellm.ai](https://docs.litellm.ai/)\n",
    "- **UV Package Manager**: [docs.astral.sh/uv](https://docs.astral.sh/uv/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
