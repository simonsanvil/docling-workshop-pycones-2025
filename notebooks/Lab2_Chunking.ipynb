{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Hybrid Document Chunking Workshop\n",
    "\n",
    "Welcome to the **Hybrid Document Chunking Workshop**! This comprehensive notebook demonstrates how to use Docling's advanced chunking capabilities for RAG (Retrieval-Augmented Generation) applications with full structured output support.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- Understand hybrid chunking and its advantages over simple text splitting\n",
    "- Configure advanced document processing pipelines\n",
    "- Process various document formats with OCR, table extraction, and figure exports\n",
    "- Generate structured output with organized folder hierarchies\n",
    "- Implement tokenization-aware chunking strategies\n",
    "- Use LLM-powered image descriptions for multimodal RAG\n",
    "- Analyze and visualize chunks for optimal RAG performance\n",
    "\n",
    "## ðŸ“‹ Workshop Sections\n",
    "\n",
    "1. **ðŸ”§ Setup & Dependencies** - Install packages with UV\n",
    "2. **âš™ï¸  Advanced Pipeline Configuration** - All processing options explained\n",
    "3. **ðŸ“Š Structured Output System** - Organized folder hierarchies  \n",
    "4. **ðŸ–¼ï¸  Figure & Table Exports** - Visual content extraction\n",
    "5. **ðŸ¤– LLM Image Descriptions** - AI-powered multimodal processing\n",
    "6. **ðŸ§© Hybrid Chunking Engine** - Smart, context-aware chunking\n",
    "7. **ðŸ“ˆ Analysis & Visualization** - Comprehensive chunk quality analysis\n",
    "8. **ðŸŽ›ï¸  Interactive Configuration Testing** - Compare different settings\n",
    "9. **ðŸ’¡ Best Practices** - Production-ready recommendations\n",
    "\n",
    "Let's dive deep! ðŸŒŠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸ”§ Setup & Dependencies with UV\n",
    "\n",
    "UV is a fast Python package installer and dependency manager. Let's install all required packages for this comprehensive workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b738b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    'langchain_huggingface[full]' \\\n",
    "    docling \\\n",
    "    replicate \\\n",
    "    matplotlib\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01317cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for the workshop\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Suppress PyTorch MPS warnings on Mac\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pin_memory.*not supported on MPS.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.*\")\n",
    "\n",
    "# Docling imports\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "from docling_core.types.doc import PictureItem, TableItem\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.llms import Replicate\n",
    "\n",
    "\n",
    "# Optional replicate imports for image descriptions\n",
    "try:\n",
    "    import base64\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    REPLICATE_AVAILABLE = True\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    REPLICATE_AVAILABLE = False\n",
    "    print(\"âš ï¸  Replicate not available. Image descriptions will be disabled.\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(\"ðŸš€ Ready for advanced document processing!\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeee639",
   "metadata": {},
   "source": [
    "## 2. âš™ï¸ Advanced Pipeline Configuration\n",
    "\n",
    "The `AdvancedPipelineConfig` class provides comprehensive control over document processing. This replicates all the features from the command-line processor, including structured output, exports, and LLM integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPipelineConfig:\n",
    "    \"\"\"Comprehensive configuration class replicating document_processor.py functionality.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # Core processing options\n",
    "        do_ocr: bool = True,\n",
    "        do_table_structure: bool = True,\n",
    "        generate_page_images: bool = False,  # Will be enabled if export_figures=True\n",
    "        generate_picture_images: bool = False,  # Will be enabled if export_figures=True\n",
    "        \n",
    "        # Chunking options\n",
    "        chunk_max_tokens: int = 512,\n",
    "        chunk_merge_peers: bool = True,\n",
    "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \n",
    "        # Export options (the key new features!)\n",
    "        export_figures: bool = False,\n",
    "        export_tables: bool = False,\n",
    "        images_scale: float = 2.0,  # Image resolution scale (1.0 = 72 DPI)\n",
    "        \n",
    "        # Structured output options\n",
    "        organize_output: bool = True,  # Create organized folder structure\n",
    "        save_metadata: bool = True,   # Save comprehensive document metadata\n",
    "        export_markdown: bool = True, # Export as markdown\n",
    "        output_dir: str = \"workshop_output\",\n",
    "        \n",
    "        # LLM Image description options\n",
    "        describe_images: bool = False,\n",
    "        llm_model: str = \"yorickvp/llava-13b\",\n",
    "        \n",
    "        # Supported formats\n",
    "        allowed_formats: List[InputFormat] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize comprehensive pipeline configuration.\n",
    "        \n",
    "        Key New Features Explained:\n",
    "        \n",
    "        ðŸ—ï¸ STRUCTURED OUTPUT (organize_output=True):\n",
    "            Creates organized folder hierarchy:\n",
    "            output_dir/files/document_name/\n",
    "            â”œâ”€â”€ json/               # Main JSON output + markdown\n",
    "            â”œâ”€â”€ metadata/           # Separate metadata file\n",
    "            â””â”€â”€ exports/            # All visual exports\n",
    "                â”œâ”€â”€ figures/        # Extracted figures/pictures\n",
    "                â”œâ”€â”€ tables/         # Table images\n",
    "                â””â”€â”€ pages/          # Page screenshots\n",
    "        \n",
    "        ðŸ–¼ï¸ FIGURE EXPORTS (export_figures=True):\n",
    "            â€¢ Extracts all figures and pictures from documents\n",
    "            â€¢ Saves as high-resolution PNG files\n",
    "            â€¢ Automatically generates page screenshots\n",
    "            â€¢ Preserves visual content for multimodal RAG\n",
    "        \n",
    "        ðŸ“Š TABLE EXPORTS (export_tables=True):\n",
    "            â€¢ Extracts tables to multiple formats: CSV, HTML, Markdown\n",
    "            â€¢ Saves table images as PNG files\n",
    "            â€¢ Preserves table structure and content\n",
    "            â€¢ Enables both text and visual table retrieval\n",
    "        \n",
    "        ðŸ¤– LLM IMAGE DESCRIPTIONS (describe_images=True):\n",
    "            â€¢ Supports Claude, GPT-4V, Gemini Vision, etc.\n",
    "            â€¢ Creates searchable text descriptions\n",
    "            â€¢ Tracks costs and token usage\n",
    "            â€¢ Enables semantic search over visual content\n",
    "        \n",
    "        ðŸ“‹ COMPREHENSIVE METADATA (save_metadata=True):\n",
    "            â€¢ Processing configuration details\n",
    "            â€¢ Document statistics and structure info\n",
    "            â€¢ File metadata and format detection\n",
    "            â€¢ Chunk quality metrics\n",
    "        \"\"\"\n",
    "        # Core processing\n",
    "        self.do_ocr = do_ocr\n",
    "        self.do_table_structure = do_table_structure\n",
    "        self.generate_page_images = generate_page_images\n",
    "        self.generate_picture_images = generate_picture_images\n",
    "        \n",
    "        # Chunking\n",
    "        self.chunk_max_tokens = chunk_max_tokens\n",
    "        self.chunk_merge_peers = chunk_merge_peers\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Export options\n",
    "        self.export_figures = export_figures\n",
    "        self.export_tables = export_tables\n",
    "        self.images_scale = images_scale\n",
    "        \n",
    "        # Structured output\n",
    "        self.organize_output = organize_output\n",
    "        self.save_metadata = save_metadata\n",
    "        self.export_markdown = export_markdown\n",
    "        self.output_dir = Path(output_dir)\n",
    "        \n",
    "        # LLM features\n",
    "        self.describe_images = describe_images\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Multi-format support\n",
    "        self.allowed_formats = allowed_formats or [\n",
    "            InputFormat.PDF,\n",
    "            InputFormat.DOCX,\n",
    "            InputFormat.PPTX,\n",
    "            InputFormat.XLSX,\n",
    "            InputFormat.HTML,\n",
    "            InputFormat.MD,\n",
    "            InputFormat.IMAGE,\n",
    "        ]\n",
    "        \n",
    "        # Auto-enable image generation if we're exporting figures\n",
    "        if self.export_figures:\n",
    "            self.generate_page_images = True\n",
    "            self.generate_picture_images = True\n",
    "    \n",
    "    def to_pipeline_options(self) -> Optional[PdfPipelineOptions]:\n",
    "        \"\"\"Convert configuration to Docling PdfPipelineOptions.\"\"\"\n",
    "        try:\n",
    "            options = PdfPipelineOptions(\n",
    "                do_ocr=self.do_ocr,\n",
    "                do_table_structure=self.do_table_structure,\n",
    "                generate_page_images=self.generate_page_images,\n",
    "                generate_picture_images=self.generate_picture_images,\n",
    "            )\n",
    "            \n",
    "            # Set images scale for high-quality exports\n",
    "            if self.export_figures or self.generate_page_images or self.generate_picture_images:\n",
    "                options.images_scale = self.images_scale\n",
    "                \n",
    "            return options\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create pipeline options: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Return a comprehensive summary of the configuration.\"\"\"\n",
    "        formats_str = ', '.join([f.name for f in self.allowed_formats])\n",
    "        \n",
    "        return f\"\"\"ðŸ”§ Advanced Pipeline Configuration:\n",
    "        \n",
    "ðŸ“Š CORE PROCESSING:\n",
    "        - OCR: {'âœ“' if self.do_ocr else 'âœ—'}\n",
    "        - Table Structure: {'âœ“' if self.do_table_structure else 'âœ—'}\n",
    "        - Page Images: {'âœ“' if self.generate_page_images else 'âœ—'}\n",
    "        - Picture Images: {'âœ“' if self.generate_picture_images else 'âœ—'}\n",
    "        \n",
    "ðŸ§© CHUNKING:\n",
    "        - Method: Hybrid (tokenization-aware)\n",
    "        - Max Tokens/Chunk: {self.chunk_max_tokens}\n",
    "        - Merge Peers: {'âœ“' if self.chunk_merge_peers else 'âœ—'}\n",
    "        - Embedding Model: {self.embedding_model}\n",
    "        \n",
    "ðŸ“ STRUCTURED OUTPUT:\n",
    "        - Organize Output: {'âœ“' if self.organize_output else 'âœ—'}\n",
    "        - Save Metadata: {'âœ“' if self.save_metadata else 'âœ—'}\n",
    "        - Export Markdown: {'âœ“' if self.export_markdown else 'âœ—'}\n",
    "        - Output Directory: {self.output_dir}\n",
    "        \n",
    "ðŸ–¼ï¸ EXPORTS:\n",
    "        - Export Figures: {'âœ“' if self.export_figures else 'âœ—'}\n",
    "        - Export Tables: {'âœ“' if self.export_tables else 'âœ—'}\n",
    "        - Images Scale: {self.images_scale}x ({int(self.images_scale * 72)} DPI)\n",
    "        \n",
    "ðŸ¤– LLM FEATURES:\n",
    "        - Describe Images: {'âœ“' if self.describe_images else 'âœ—'}\n",
    "        - LLM Model: {self.llm_model}\n",
    "        \n",
    "âš™ï¸ FORMATS:\n",
    "        - Supported: {formats_str}\"\"\"\n",
    "\n",
    "# Create a comprehensive configuration (matching the example command)\n",
    "config = AdvancedPipelineConfig(\n",
    "    chunk_max_tokens=256,\n",
    "    organize_output=True,\n",
    "    export_figures=True,\n",
    "    export_tables=True,\n",
    "    save_metadata=True,\n",
    "    describe_images=True,\n",
    "    llm_model=\"yorickvp/llava-13b\",\n",
    "    output_dir=\"workshop_output\"\n",
    ")\n",
    "\n",
    "print(\"âœ… AdvancedPipelineConfig class defined!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(config.summary())\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cdaea",
   "metadata": {},
   "source": [
    "## 3. ðŸ“Š Structured Output System Explained\n",
    "\n",
    "The structured output system creates a comprehensive, organized hierarchy that makes it easy to manage and access all processed content. Let's understand each component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_output_structure():\n",
    "    \"\"\"Show the complete output structure with explanations.\"\"\"\n",
    "    \n",
    "    structure_diagram = \"\"\"\n",
    "ðŸ“ STRUCTURED OUTPUT HIERARCHY:\n",
    "\n",
    "workshop_output/\n",
    "â””â”€â”€ files/                           # Categorized by type (files vs audio)\n",
    "    â””â”€â”€ document_name/               # One folder per document\n",
    "        â”œâ”€â”€ json/                    # Main content\n",
    "        â”‚   â”œâ”€â”€ document_name.json   # Complete processed data\n",
    "        â”‚   â””â”€â”€ document_name.md     # Markdown export\n",
    "        â”‚\n",
    "        â”œâ”€â”€ metadata/                # Metadata only\n",
    "        â”‚   â””â”€â”€ document_name_metadata.json\n",
    "        â”‚\n",
    "        â””â”€â”€ exports/                 \n",
    "            â”œâ”€â”€ figures/             \n",
    "            â”‚   â”œâ”€â”€ document-picture-1.png\n",
    "            â”‚   â”œâ”€â”€ document-picture-2.png\n",
    "            â”‚   â””â”€â”€ ...\n",
    "            â”‚\n",
    "            â”œâ”€â”€ tables/              \n",
    "            â”‚   â”œâ”€â”€ document-table-1.png      # Visual\n",
    "            â”‚   â”œâ”€â”€ document-table-1.csv      # Data  \n",
    "            â”‚   â”œâ”€â”€ document-table-1.html     # Formatted\n",
    "            â”‚   â”œâ”€â”€ document-table-1.md       # Markdown\n",
    "            â”‚   â””â”€â”€ ...\n",
    "            â”‚\n",
    "            â”œâ”€â”€ pages/               # Page screenshots\n",
    "            â”‚   â”œâ”€â”€ document-page-1.png\n",
    "            â”‚   â”œâ”€â”€ document-page-2.png\n",
    "            â”‚   â””â”€â”€ ...\n",
    "    \"\"\"\n",
    "    \n",
    "    explanations = {\n",
    "        \"ðŸ“„ json/\": \"Contains the main processed data as JSON and the original text as Markdown. This is your primary content for RAG.\",\n",
    "        \n",
    "        \"ðŸ“‹ metadata/\": \"Separate metadata file with processing config, document stats, file info, and quality metrics.\",\n",
    "        \n",
    "        \"ðŸ–¼ï¸ figures/\": \"All pictures, diagrams, charts, and visual elements extracted as high-res PNG files for multimodal RAG.\",\n",
    "        \n",
    "        \"ðŸ“Š tables/\": \"Tables in multiple formats: PNG images for visual retrieval, CSV for data analysis, HTML for web display, Markdown for text processing.\",\n",
    "        \n",
    "        \"ðŸ“– pages/\": \"Screenshots of each document page, useful for layout-aware applications and visual document search.\",\n",
    "        \n",
    "        \"ðŸ¤– image-descriptions.json\": \"LLM-generated descriptions of all images, with cost tracking and metadata. Makes visual content searchable via text.\"\n",
    "    }\n",
    "    \n",
    "    print(structure_diagram)\n",
    "    print(\"\\nðŸ” COMPONENT EXPLANATIONS:\\n\")\n",
    "    \n",
    "    for component, explanation in explanations.items():\n",
    "        print(f\"{component}\")\n",
    "        print(f\"   {explanation}\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "explain_output_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0186c",
   "metadata": {},
   "source": [
    "## 4. ðŸ§  Comprehensive Document Processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveDocumentProcessor:\n",
    "    \"\"\"Full-featured document processor with all advanced capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[AdvancedPipelineConfig] = None):\n",
    "        \"\"\"Initialize the comprehensive processor.\"\"\"\n",
    "        self.config = config or AdvancedPipelineConfig()\n",
    "        \n",
    "        # Initialize DocumentConverter\n",
    "        self.converter = self._initialize_converter()\n",
    "        \n",
    "        # Initialize HybridChunker\n",
    "        self.hybrid_chunker = None\n",
    "        self.tokenizer = None\n",
    "        self._initialize_hybrid_chunker()\n",
    "        \n",
    "        # Store conversion result for exports\n",
    "        self._conversion_result = None\n",
    "    \n",
    "    def _initialize_converter(self) -> DocumentConverter:\n",
    "        \"\"\"Initialize DocumentConverter with multi-format support.\"\"\"\n",
    "        try:\n",
    "            pipeline_options = self.config.to_pipeline_options()\n",
    "            \n",
    "            format_options = {}\n",
    "            if InputFormat.PDF in self.config.allowed_formats and pipeline_options:\n",
    "                format_options[InputFormat.PDF] = PdfFormatOption(\n",
    "                    pipeline_cls=StandardPdfPipeline,\n",
    "                    backend=PyPdfiumDocumentBackend,\n",
    "                    pipeline_options=pipeline_options\n",
    "                )\n",
    "            \n",
    "            converter = DocumentConverter(\n",
    "                allowed_formats=self.config.allowed_formats,\n",
    "                format_options=format_options if format_options else None\n",
    "            )\n",
    "            \n",
    "            formats_str = ', '.join([f.name for f in self.config.allowed_formats])\n",
    "            print(f\"âœ… Initialized converter for formats: {formats_str}\")\n",
    "            return converter\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Warning: Could not apply format options: {e}\")\n",
    "            return DocumentConverter()\n",
    "    \n",
    "    def _initialize_hybrid_chunker(self):\n",
    "        \"\"\"Initialize the hybrid chunker with tokenizer.\"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ”§ Loading tokenizer: {self.config.embedding_model}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.embedding_model)\n",
    "            \n",
    "            self.hybrid_chunker = HybridChunker(\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_tokens=self.config.chunk_max_tokens,\n",
    "                merge_peers=self.config.chunk_merge_peers\n",
    "            )\n",
    "            print(f\"âœ… Initialized HybridChunker (max_tokens={self.config.chunk_max_tokens})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: Could not initialize hybrid chunker: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a document with full structured output.\"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ”„ Processing document: {Path(file_path).name}\")\n",
    "            \n",
    "            # Convert document\n",
    "            result = self.converter.convert(file_path)\n",
    "            doc = result.document\n",
    "            self._conversion_result = result\n",
    "            \n",
    "            # Create output structure\n",
    "            doc_folder = self._get_output_folder(file_path)\n",
    "            \n",
    "            # Extract comprehensive metadata\n",
    "            metadata = self._extract_comprehensive_metadata(result, file_path, doc_folder)\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self._create_hybrid_chunks(doc)\n",
    "            \n",
    "            # Extract tables and headers\n",
    "            tables = self._extract_tables(doc)\n",
    "            headers = self._extract_headers(doc)\n",
    "            \n",
    "            # Handle exports (figures, tables)\n",
    "            exports_info = {}\n",
    "            if self.config.export_figures or self.config.export_tables:\n",
    "                exports_info = self._handle_exports(result, doc, file_path, doc_folder)\n",
    "            \n",
    "            # Prepare complete processed data\n",
    "            processed_data = {\n",
    "                \"metadata\": metadata,\n",
    "                \"content\": {\n",
    "                    \"full_text\": doc.export_to_markdown(),\n",
    "                    \"structured_content\": json.loads(doc.to_json()) if hasattr(doc, 'to_json') else {},\n",
    "                },\n",
    "                \"chunks\": chunks,\n",
    "                \"tables\": tables,\n",
    "                \"headers\": headers,\n",
    "                \"exports\": exports_info,\n",
    "                \"document_stats\": {\n",
    "                    \"total_characters\": len(doc.export_to_markdown()),\n",
    "                    \"total_words\": len(doc.export_to_markdown().split()),\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"total_tables\": len(tables),\n",
    "                    \"total_headers\": len(headers),\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create structured output if enabled\n",
    "            if self.config.organize_output:\n",
    "                output_structure = self._create_output_structure(doc_folder, processed_data)\n",
    "                processed_data[\"output_structure\"] = output_structure\n",
    "            \n",
    "            print(f\"âœ… Document processed successfully! Created {len(chunks)} chunks\")\n",
    "            return processed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Failed to process document: {str(e)}\",\n",
    "                \"metadata\": {\"source_file\": str(file_path)},\n",
    "            }\n",
    "    \n",
    "    def _get_output_folder(self, file_path: str) -> Path:\n",
    "        \"\"\"Determine output folder structure.\"\"\"\n",
    "        if self.config.organize_output:\n",
    "            return self.config.output_dir / \"files\" / Path(file_path).stem\n",
    "        else:\n",
    "            return self.config.output_dir\n",
    "    \n",
    "    def _create_hybrid_chunks(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create chunks using Docling's HybridChunker.\"\"\"\n",
    "        print(\"ðŸ§© Creating hybrid chunks...\")\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            chunk_iter = self.hybrid_chunker.chunk(dl_doc=doc)\n",
    "            \n",
    "            for i, chunk in enumerate(chunk_iter):\n",
    "                contextualized_text = self.hybrid_chunker.contextualize(chunk=chunk)\n",
    "                \n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": i,\n",
    "                    \"text\": chunk.text,\n",
    "                    \"contextualized_text\": contextualized_text,\n",
    "                    \"token_count\": len(self.tokenizer.encode(chunk.text)) if self.tokenizer else len(chunk.text.split()),\n",
    "                    \"char_count\": len(chunk.text),\n",
    "                    \"contextualized_char_count\": len(contextualized_text),\n",
    "                    \"metadata\": {\n",
    "                        \"headings\": getattr(chunk.meta, 'headings', []) if hasattr(chunk, 'meta') else [],\n",
    "                        \"page_info\": getattr(chunk.meta, 'page_info', []) if hasattr(chunk, 'meta') else [],\n",
    "                        \"content_type\": getattr(chunk.meta, 'content_type', None) if hasattr(chunk, 'meta') else None,\n",
    "                        \"chunk_type\": \"hybrid\"\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk_data)\n",
    "            \n",
    "            print(f\"âœ… Created {len(chunks)} hybrid chunks\")\n",
    "            return chunks\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: Hybrid chunking failed: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def _extract_tables(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract table information from the document.\"\"\"\n",
    "        tables = []\n",
    "        try:\n",
    "            if hasattr(doc, 'to_json'):\n",
    "                doc_dict = json.loads(doc.to_json())\n",
    "                if 'tables' in doc_dict:\n",
    "                    for i, table in enumerate(doc_dict['tables']):\n",
    "                        tables.append({\n",
    "                            \"table_id\": i,\n",
    "                            \"content\": table,\n",
    "                            \"extraction_method\": \"structured_json\"\n",
    "                        })\n",
    "            \n",
    "            if hasattr(doc, 'tables'):\n",
    "                for i, table in enumerate(doc.tables):\n",
    "                    tables.append({\n",
    "                        \"table_id\": len(tables),\n",
    "                        \"content\": str(table) if hasattr(table, '__str__') else table,\n",
    "                        \"extraction_method\": \"direct_attribute\"\n",
    "                    })\n",
    "                    \n",
    "            if tables:\n",
    "                print(f\"âœ… Extracted {len(tables)} tables\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Warning: Could not extract tables: {e}\")\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def _extract_headers(self, doc) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract header information from the document.\"\"\"\n",
    "        headers = []\n",
    "        try:\n",
    "            markdown_content = doc.export_to_markdown()\n",
    "            lines = markdown_content.split('\\n')\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if line.startswith('#'):\n",
    "                    level = len(line) - len(line.lstrip('#'))\n",
    "                    text = line.lstrip('#').strip()\n",
    "                    if text:\n",
    "                        headers.append({\n",
    "                            \"level\": level,\n",
    "                            \"text\": text,\n",
    "                            \"line_number\": i,\n",
    "                        })\n",
    "                        \n",
    "            if headers:\n",
    "                print(f\"âœ… Extracted {len(headers)} headers\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Warning: Could not extract headers: {e}\")\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def _extract_comprehensive_metadata(self, result, file_path: str, doc_folder: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive metadata about the document and processing.\"\"\"\n",
    "        file_path_obj = Path(file_path)\n",
    "        \n",
    "        metadata = {\n",
    "            \"source_file\": str(file_path),\n",
    "            \"file_name\": file_path_obj.name,\n",
    "            \"file_stem\": file_path_obj.stem,\n",
    "            \"file_type\": file_path_obj.suffix.lower(),\n",
    "            \"file_size_bytes\": file_path_obj.stat().st_size if file_path_obj.exists() else 0,\n",
    "            \"title\": getattr(result.document, 'title', None) or file_path_obj.stem,\n",
    "            \"output_folder\": str(doc_folder),\n",
    "            \"processing_config\": {\n",
    "                \"chunking_method\": \"hybrid\",\n",
    "                \"max_tokens_per_chunk\": self.config.chunk_max_tokens,\n",
    "                \"ocr_enabled\": self.config.do_ocr,\n",
    "                \"table_structure_enabled\": self.config.do_table_structure,\n",
    "                \"export_figures\": self.config.export_figures,\n",
    "                \"export_tables\": self.config.export_tables,\n",
    "                \"organize_output\": self.config.organize_output,\n",
    "                \"describe_images\": self.config.describe_images,\n",
    "                \"llm_model\": self.config.llm_model if self.config.describe_images else None,\n",
    "                \"embedding_model\": self.config.embedding_model,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # Document structure metadata\n",
    "        doc = result.document\n",
    "        if hasattr(doc, 'pages'):\n",
    "            metadata[\"page_count\"] = len(doc.pages) if doc.pages else 0\n",
    "        \n",
    "        if hasattr(doc, 'tables'):\n",
    "            metadata[\"table_count\"] = len(doc.tables) if doc.tables else 0\n",
    "        \n",
    "        # Content statistics\n",
    "        full_text = doc.export_to_markdown()\n",
    "        metadata[\"content_stats\"] = {\n",
    "            \"total_characters\": len(full_text),\n",
    "            \"total_words\": len(full_text.split()),\n",
    "            \"total_lines\": len(full_text.split('\\n')),\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _create_output_structure(self, doc_folder: Path, processed_data: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Create organized output folder structure and save files.\"\"\"\n",
    "        folders = {\n",
    "            \"json_folder\": doc_folder / \"json\",\n",
    "            \"metadata_folder\": doc_folder / \"metadata\",\n",
    "            \"exports_folder\": doc_folder / \"exports\",\n",
    "            \"figures_folder\": doc_folder / \"exports\" / \"figures\",\n",
    "            \"tables_folder\": doc_folder / \"exports\" / \"tables\", \n",
    "            \"pages_folder\": doc_folder / \"exports\" / \"pages\",\n",
    "        }\n",
    "        \n",
    "        # Create directories\n",
    "        for folder in folders.values():\n",
    "            folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save JSON output\n",
    "        json_file = folders[\"json_folder\"] / f\"{processed_data['metadata']['file_stem']}.json\"\n",
    "        with json_file.open('w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save metadata separately\n",
    "        metadata_file = folders[\"metadata_folder\"] / f\"{processed_data['metadata']['file_stem']}_metadata.json\"\n",
    "        with metadata_file.open('w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data['metadata'], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save markdown if enabled\n",
    "        if self.config.export_markdown and hasattr(self._conversion_result, 'document'):\n",
    "            markdown_file = folders[\"json_folder\"] / f\"{processed_data['metadata']['file_stem']}.md\"\n",
    "            with markdown_file.open('w', encoding='utf-8') as f:\n",
    "                f.write(self._conversion_result.document.export_to_markdown())\n",
    "        \n",
    "        print(f\"ðŸ“ Created structured output in: {doc_folder}\")\n",
    "        return {k: str(v) for k, v in folders.items()}\n",
    "    \n",
    "    # We'll add the export methods in the next cell due to length...\n",
    "\n",
    "print(\"âœ… ComprehensiveDocumentProcessor class defined!\")\n",
    "\n",
    "print(\"ðŸ”§ Class capabilities:\")\n",
    "print(\"  â€¢ Document formats: PDF, DOCX\")\n",
    "print(\"  â€¢ Chunking: Hybrid chunking with configurable token limits\")\n",
    "print(\"  â€¢ Exports: Figures, tables, pages, markdown\")\n",
    "print(\"  â€¢ Output organization: Structured folders with metadata\")\n",
    "print(\"  â€¢ LLM integration: Image descriptions and content analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_exports(self, result, doc, file_path: str, doc_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Handle figure and table exports based on configuration.\"\"\"\n",
    "    exports_info = {\n",
    "        \"figures_exported\": 0,\n",
    "        \"tables_exported\": 0,\n",
    "        \"pages_exported\": 0,\n",
    "        \"export_directory\": str(doc_folder / \"exports\"),\n",
    "        \"exported_files\": [],\n",
    "        \"image_descriptions\": []\n",
    "    }\n",
    "    \n",
    "    if not (self.config.export_figures or self.config.export_tables):\n",
    "        return exports_info\n",
    "        \n",
    "    # Create output directories\n",
    "    exports_folder = doc_folder / \"exports\"\n",
    "    figures_folder = exports_folder / \"figures\"\n",
    "    tables_folder = exports_folder / \"tables\"\n",
    "    pages_folder = exports_folder / \"pages\"\n",
    "    \n",
    "    for folder in [exports_folder, figures_folder, tables_folder, pages_folder]:\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    doc_filename = Path(file_path).stem\n",
    "    \n",
    "    # Export figures if enabled\n",
    "    if self.config.export_figures:\n",
    "        figure_info = self._export_figures(result, doc, doc_filename, figures_folder, tables_folder, pages_folder, exports_folder)\n",
    "        exports_info.update(figure_info)\n",
    "        \n",
    "    # Export tables if enabled\n",
    "    if self.config.export_tables:\n",
    "        table_info = self._export_tables(result, doc, doc_filename, tables_folder)\n",
    "        exports_info.update(table_info)\n",
    "        \n",
    "    return exports_info\n",
    "\n",
    "def _export_figures(self, result, doc, doc_filename: str, figures_folder: Path, tables_folder: Path, pages_folder: Path, exports_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Export figures, page images, and pictures with optional LLM descriptions.\"\"\"\n",
    "    print(\"ðŸ–¼ï¸  Exporting figures...\")\n",
    "    exported_files = []\n",
    "    image_descriptions = []\n",
    "    \n",
    "    try:\n",
    "        # Save page images to pages subfolder\n",
    "        page_counter = 0\n",
    "        if hasattr(result, 'document') and hasattr(result.document, 'pages'):\n",
    "            for page_no, page in result.document.pages.items():\n",
    "                if hasattr(page, 'image') and page.image:\n",
    "                    page_counter += 1\n",
    "                    page_image_filename = pages_folder / f\"{doc_filename}-page-{page.page_no}.png\"\n",
    "                    with page_image_filename.open(\"wb\") as fp:\n",
    "                        page.image.pil_image.save(fp, format=\"PNG\")\n",
    "                    exported_files.append(str(page_image_filename))\n",
    "        \n",
    "        # Save images of figures and tables\n",
    "        table_counter = 0\n",
    "        picture_counter = 0\n",
    "        \n",
    "        if hasattr(result, 'document'):\n",
    "            for element, _level in result.document.iterate_items():\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter += 1\n",
    "                    element_image_filename = tables_folder / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "                    try:\n",
    "                        with element_image_filename.open(\"wb\") as fp:\n",
    "                            element.get_image(result.document).save(fp, \"PNG\")\n",
    "                        exported_files.append(str(element_image_filename))\n",
    "                        \n",
    "                        # Add LLM description if enabled\n",
    "                        if self.config.describe_images and REPLICATE_AVAILABLE:\n",
    "                            desc_result = self._describe_image_with_llm(element_image_filename, self.config.llm_model)\n",
    "                            desc_result.update({\n",
    "                                \"type\": \"table\",\n",
    "                                \"image_filename\": element_image_filename.name,\n",
    "                                \"sequence_number\": table_counter\n",
    "                            })\n",
    "                            image_descriptions.append(desc_result)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Warning: Could not export table {table_counter} image: {e}\")\n",
    "                \n",
    "                if isinstance(element, PictureItem):\n",
    "                    picture_counter += 1\n",
    "                    element_image_filename = figures_folder / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "                    try:\n",
    "                        with element_image_filename.open(\"wb\") as fp:\n",
    "                            element.get_image(result.document).save(fp, \"PNG\")\n",
    "                        exported_files.append(str(element_image_filename))\n",
    "                        \n",
    "                        # Add LLM description if enabled\n",
    "                        if self.config.describe_images and REPLICATE_AVAILABLE:\n",
    "                            desc_result = self._describe_image_with_llm(element_image_filename, self.config.llm_model)\n",
    "                            desc_result.update({\n",
    "                                \"type\": \"picture/figure\",\n",
    "                                \"image_filename\": element_image_filename.name,\n",
    "                                \"sequence_number\": picture_counter\n",
    "                            })\n",
    "                            image_descriptions.append(desc_result)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Warning: Could not export picture {picture_counter} image: {e}\")\n",
    "        \n",
    "        # Save consolidated image descriptions if any were generated\n",
    "        if self.config.describe_images and image_descriptions:\n",
    "            consolidated_descriptions = {\n",
    "                \"document_name\": doc_filename,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_images\": len(image_descriptions),\n",
    "                \"total_cost\": sum(desc.get(\"cost\", 0) for desc in image_descriptions),\n",
    "                \"total_input_tokens\": sum(desc.get(\"input_tokens\", 0) for desc in image_descriptions),\n",
    "                \"total_output_tokens\": sum(desc.get(\"output_tokens\", 0) for desc in image_descriptions),\n",
    "                \"model_used\": self.config.llm_model,\n",
    "                \"descriptions\": image_descriptions\n",
    "            }\n",
    "            \n",
    "            consolidated_filename = exports_folder / f\"{doc_filename}-image-descriptions.json\"\n",
    "            with consolidated_filename.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                json.dump(consolidated_descriptions, fp, indent=2, ensure_ascii=False)\n",
    "            exported_files.append(str(consolidated_filename))\n",
    "            print(f\"ðŸ¤– Generated {len(image_descriptions)} LLM image descriptions\")\n",
    "        \n",
    "        print(f\"âœ… Exported {len(exported_files)} figure files\")\n",
    "        return {\n",
    "            \"figures_exported\": table_counter + picture_counter,\n",
    "            \"pages_exported\": page_counter,\n",
    "            \"figure_files\": exported_files,\n",
    "            \"image_descriptions\": image_descriptions\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Warning: Figure export failed: {e}\")\n",
    "        return {\"figures_exported\": 0, \"figure_files\": []}\n",
    "\n",
    "def _export_tables(self, result, doc, doc_filename: str, tables_folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Export tables to various formats (CSV, HTML, Markdown).\"\"\"\n",
    "    print(\"ðŸ“Š Exporting tables...\")\n",
    "    exported_files = []\n",
    "    \n",
    "    try:\n",
    "        table_counter = 0\n",
    "        if hasattr(doc, 'tables'):\n",
    "            for table_ix, table in enumerate(doc.tables):\n",
    "                table_counter += 1\n",
    "                \n",
    "                try:\n",
    "                    table_df = table.export_to_dataframe(doc=doc)\n",
    "                    \n",
    "                    # Save as CSV\n",
    "                    csv_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.csv\"\n",
    "                    table_df.to_csv(csv_filename, index=False)\n",
    "                    exported_files.append(str(csv_filename))\n",
    "                    \n",
    "                    # Save as HTML\n",
    "                    html_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.html\"\n",
    "                    with html_filename.open(\"w\") as fp:\n",
    "                        fp.write(table.export_to_html(doc=doc))\n",
    "                    exported_files.append(str(html_filename))\n",
    "                    \n",
    "                    # Save as Markdown\n",
    "                    md_filename = tables_folder / f\"{doc_filename}-table-{table_ix + 1}.md\"\n",
    "                    with md_filename.open(\"w\") as fp:\n",
    "                        fp.write(f\"## Table {table_ix + 1}\\n\\n\")\n",
    "                        fp.write(table_df.to_markdown(index=False))\n",
    "                    exported_files.append(str(md_filename))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Warning: Could not export table {table_ix + 1}: {e}\")\n",
    "        \n",
    "        print(f\"âœ… Exported {table_counter} tables in {len(exported_files)} files\")\n",
    "        return {\n",
    "            \"tables_exported\": table_counter,\n",
    "            \"table_files\": exported_files\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Warning: Table export failed: {e}\")\n",
    "        return {\"tables_exported\": 0, \"table_files\": []}\n",
    "\n",
    "def _describe_image_with_llm(self, image_path: Path, model: str = \"yorickvp/llava-13b:80537f9eead1a5bfa72d5ac6ea6414379be41d4d4f6679fd776e9535d1eb58bb\") -> Dict[str, Any]:\n",
    "        \"\"\"Describe an image using Replicate LLaVA model.\"\"\"\n",
    "        try:\n",
    "            import replicate\n",
    "            \n",
    "            # Convert image to base64 data URI\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "                image_data_uri = f\"data:image/png;base64,{image_data}\"\n",
    "            \n",
    "            prompt_text = \"Describe this image in detail. Focus on the main content, text, data, charts, diagrams, or any other relevant information that would be useful for document understanding and search.\"\n",
    "            \n",
    "            # Use Replicate LLaVA model to describe the image\n",
    "            output = replicate.run(\n",
    "                \"yorickvp/llava-13b:80537f9eead1a5bfa72d5ac6ea6414379be41d4d4f6679fd776e9535d1eb58bb\",\n",
    "                input={\n",
    "                    \"image\": image_data_uri,\n",
    "                    \"top_p\": 1,\n",
    "                    \"prompt\": prompt_text,\n",
    "                    \"max_tokens\": 1024,\n",
    "                    \"temperature\": 0.2\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # LLaVA returns an iterator, collect all output\n",
    "            description_parts = []\n",
    "            for item in output:\n",
    "                description_parts.append(str(item))\n",
    "            \n",
    "            description = \"\".join(description_parts).strip()\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"description\": description,\n",
    "                \"prompt\": prompt_text,\n",
    "                \"image_path\": str(image_path),\n",
    "                \"model\": \"yorickvp/llava-13b\",\n",
    "                \"input_tokens\": 0,  # Replicate doesn't provide token info\n",
    "                \"output_tokens\": len(description.split()) if description else 0,\n",
    "                \"cost\": 0.0,  # Replicate doesn't provide cost info in response\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"description\": None,\n",
    "                \"prompt\": None,\n",
    "                \"image_path\": str(image_path),\n",
    "                \"model\": \"yorickvp/llava-13b\",\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"cost\": 0.0,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "\n",
    "\n",
    "# Add these methods to our ComprehensiveDocumentProcessor class\n",
    "ComprehensiveDocumentProcessor._handle_exports = _handle_exports\n",
    "ComprehensiveDocumentProcessor._export_figures = _export_figures  \n",
    "ComprehensiveDocumentProcessor._export_tables = _export_tables\n",
    "ComprehensiveDocumentProcessor._describe_image_with_llm = _describe_image_with_llm\n",
    "\n",
    "print(\"âœ… Export functionality added to ComprehensiveDocumentProcessor!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6cb84",
   "metadata": {},
   "source": [
    "## 5. ðŸŽ¯ Complete Demo: Processing a Document with Full Features\n",
    "\n",
    "Now let's demonstrate the complete functionality by processing a document with all features enabled, just like the command-line example!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first download the PDF locally for processing\n",
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2501.17887\"\n",
    "document_path = \"docling_paper.pdf\"\n",
    "response = requests.get(url)\n",
    "with open(document_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(f\"âœ… Downloaded sample PDF: {document_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the comprehensive processor with full features\n",
    "processor = ComprehensiveDocumentProcessor(config)\n",
    "\n",
    "# Check for available example files\n",
    "print(f\" Using document path: {document_path}\")\n",
    "\n",
    "\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(config.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ff64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the document with full features\n",
    "print(\"ðŸš€ Starting comprehensive document processing...\")\n",
    "print(\"This may take a few minutes depending on document size and features enabled.\")\n",
    "\n",
    "result = processor.process_document(document_path)\n",
    "\n",
    "# Analyze and display results\n",
    "if \"error\" in result:\n",
    "    print(f\"âŒ Error processing document: {result['error']}\")\n",
    "    print(\"ðŸ’¡ Make sure the document path is correct and you have the required API keys if using image descriptions.\")\n",
    "else:\n",
    "    print(f\"\\nðŸŽ‰ Document processed successfully!\")\n",
    "    \n",
    "    # Display comprehensive summary\n",
    "    stats = result[\"document_stats\"]\n",
    "    metadata = result[\"metadata\"]\n",
    "    exports = result.get(\"exports\", {})\n",
    "    \n",
    "    print(f\"\\nðŸ“Š PROCESSING RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“„ Document: {metadata['file_name']}\")\n",
    "    print(f\"ðŸ“ Output folder: {metadata['output_folder']}\")\n",
    "    print(f\"ðŸ“ File size: {metadata['file_size_bytes']:,} bytes\")\n",
    "    if \"page_count\" in metadata:\n",
    "        print(f\"ðŸ“– Pages: {metadata['page_count']}\")\n",
    "    \n",
    "    print(f\"\\nðŸ§© CONTENT ANALYSIS:\")\n",
    "    print(f\"   â€¢ Total characters: {stats['total_characters']:,}\")\n",
    "    print(f\"   â€¢ Total words: {stats['total_words']:,}\")\n",
    "    print(f\"   â€¢ Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"   â€¢ Total tables: {stats['total_tables']}\")\n",
    "    print(f\"   â€¢ Total headers: {stats['total_headers']}\")\n",
    "    \n",
    "    if stats['total_chunks'] > 0:\n",
    "        avg_chars_per_chunk = stats['total_characters'] / stats['total_chunks']\n",
    "        avg_words_per_chunk = stats['total_words'] / stats['total_chunks']\n",
    "        print(f\"   â€¢ Avg chars/chunk: {avg_chars_per_chunk:.0f}\")\n",
    "        print(f\"   â€¢ Avg words/chunk: {avg_words_per_chunk:.0f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¨ EXPORTS SUMMARY:\")\n",
    "    if exports:\n",
    "        if exports.get('figures_exported', 0) > 0:\n",
    "            print(f\"   â€¢ Figures exported: {exports['figures_exported']}\")\n",
    "        if exports.get('tables_exported', 0) > 0:\n",
    "            print(f\"   â€¢ Tables exported: {exports['tables_exported']}\")\n",
    "        if exports.get('pages_exported', 0) > 0:\n",
    "            print(f\"   â€¢ Pages exported: {exports['pages_exported']}\")\n",
    "        if exports.get('figure_files'):\n",
    "            print(f\"   â€¢ Export files created: {len(exports['figure_files'])}\")\n",
    "        if exports.get('image_descriptions'):\n",
    "            total_cost = sum(desc.get(\"cost\", 0) for desc in exports['image_descriptions'])\n",
    "            total_tokens = sum(desc.get(\"input_tokens\", 0) + desc.get(\"output_tokens\", 0) for desc in exports['image_descriptions'])\n",
    "            print(f\"   â€¢ LLM descriptions: {len(exports['image_descriptions'])}\")\n",
    "            print(f\"   â€¢ Total LLM cost: ${total_cost:.4f}\")\n",
    "            print(f\"   â€¢ Total LLM tokens: {total_tokens:,}\")\n",
    "    else:\n",
    "        print(\"   â€¢ No exports configured\")\n",
    "    \n",
    "    if \"output_structure\" in result:\n",
    "        print(f\"\\nðŸ“ STRUCTURED OUTPUT CREATED:\")\n",
    "        structure = result[\"output_structure\"]\n",
    "        for folder_type, folder_path in structure.items():\n",
    "            folder_name = folder_type.replace(\"_folder\", \"\")\n",
    "            print(f\"   â€¢ {folder_name}: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d838f82",
   "metadata": {},
   "source": [
    "### âœ… What We've Done\n",
    "\n",
    "- **ðŸ”§ UV Package Management**: Modern Python dependency management\n",
    "- **âš™ï¸ Advanced Pipeline Configuration**: All document processing options\n",
    "- **ðŸ“Š Structured Output System**: Organized, production-ready folder hierarchies  \n",
    "- **ðŸ–¼ï¸ Figure & Table Exports**: Multi-format visual content extraction\n",
    "- **ðŸ¤– LLM Image Descriptions**: AI-powered multimodal processing capabilities\n",
    "- **ðŸ§© Hybrid Chunking**: Smart, context-aware chunking for optimal RAG\n",
    "- **ðŸ“ˆ Comprehensive Analysis**: Quality metrics and visualization tools\n",
    "\n",
    "\n",
    "- **Docling Documentation**: [docling-project.github.io](https://docling-project.github.io/docling/)\n",
    "- **UV Package Manager**: [docs.astral.sh/uv](https://docs.astral.sh/uv/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
