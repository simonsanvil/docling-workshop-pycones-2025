{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U48hO1_V_JRG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Construyendo un sistema RAG multimodal con Docling\n",
    "\n",
    "*Usando IBM Granite vision, embeddings de texto y modelos de IA generativa*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYjGpqcgXR9W"
   },
   "source": [
    "## Lab 3: Del Papel al Conocimiento para IA Transparente - El Viaje Completo\n",
    "Bienvenido al laboratorio final de nuestra workshop de Docling! Has recorrido un largo camino:\n",
    "\n",
    "- **Lab 1**: Aprendiste a convertir documentos preservando la estructura\n",
    "- **Lab 2**: Dominaste estrategias inteligentes de chunking\n",
    "- **Lab 3**: Ahora, construiremos un sistema RAG completo y listo para producción con una característica revolucionaria: el *visual grounding*\n",
    "\n",
    "\n",
    "Este laboratorio representa la culminación de todo lo que has aprendido, mostrando cómo Docling permite no solo el procesamiento de documentos, sino sistemas de IA verdaderamente transparentes.\n",
    "\n",
    "## ¿Por qué este laboratorio es importante?\n",
    "\n",
    "Los sistemas RAG tradicionales tienen un problema de confianza. Cuando una IA proporciona información, los usuarios a menudo se preguntan:\n",
    "\n",
    "- \"¿De dónde proviene esta información?\" 🔍\n",
    "- \"¿Puedo verificar que esto es preciso?\" ✅\n",
    "- \"¿Está la IA alucinando o utilizando datos reales?\" 🤔\n",
    "\n",
    "**Visual Grounding** resuelve este problema mostrando a los usuarios exactamente de dónde se recuperó la información en los documentos originales. Esto no es solo una característica *agradable* de un sistema de IA, es esencial para casos de uso donde la precisión y la verificabilidad son cruciales:\n",
    "\n",
    "- **Salud**: 🏥 Verificar fuentes de información médica\n",
    "- **Legal**: ⚖️ Rastrear citas a ubicaciones exactas en documentos\n",
    "- **Financiero**: 💰 Auditar ideas financieras generadas por IA\n",
    "- **Investigación**: 🔬 Validar afirmaciones científicas\n",
    "- **Empresarial**: 🏢 Construir sistemas de IA internos confiables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HIV_JkOXw9q"
   },
   "source": [
    "## ¿Qué hace especial a este lab?\n",
    "\n",
    "Estamos construyendo un sistema RAG multimodal con *visual grounding* que:\n",
    "\n",
    "1. **Procesa múltiples tipos de datos**: Texto, tablas e imágenes de tus documentos\n",
    "2. **Muestra fuentes exactas**: Resalta la ubicación precisa de la información recuperada\n",
    "3. **Comprende imágenes**: Utiliza modelos de visión por IA para comprender el contenido visual\n",
    "4. **Mantiene la transparencia**: Cada respuesta puede ser verificada visualmente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAmR3BxvZ2Y0"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJl_fmofYBmH"
   },
   "source": [
    "## Entendiendo RAG Multimodal con Visual Grounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghh3uCNMCFwU"
   },
   "source": [
    "### ¿Qué es RAG Multimodal?\n",
    "\n",
    "[Retrieval Augmented Generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) es una técnica utilizada con LLMs para conectar el modelo con una base de conocimiento externa sin necesidad de realizar [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning).\n",
    "\n",
    "Los sistemas RAG tradicionales están limitados a casos de uso basados en texto. Sin embargo, los documentos reales contienen:\n",
    "- **Texto**: Párrafos, listas, encabezados\n",
    "- **Tablas**: Datos estructurados, información financiera\n",
    "- **Imágenes**: Gráficos, diagramas, fotos, ilustraciones\n",
    "\n",
    "El RAG Multimodal puede utilizar [LLMs multimodales](https://www.ibm.com/think/topics/multimodal-ai) (MLLM) para procesar información de múltiples tipos de datos que se incluyen como parte de la base de conocimiento externa utilizada en RAG. Los datos multimodales pueden incluir texto, imágenes, audio, video u otras formas.\n",
    "\n",
    "\n",
    "Puedes leer más sobre RAG Multimodal en este [artículo de IBM](https://www.ibm.com/think/topics/multimodal-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpxgNPDCZEDW"
   },
   "source": [
    "### Visual Grounding: La Capa de Transparencia\n",
    "\n",
    "El grounding agrega una capa crucial de transparencia a los sistemas RAG. Cuando el sistema recupera información para responder a una consulta, no solo devuelve texto, sino que también muestra exactamente de dónde proviene esa información en el documento original mediante:\n",
    "\n",
    "- Dibujar cuadros delimitadores en las páginas del documento\n",
    "- Resaltar regiones específicas\n",
    "- Etiquetar tipos de contenido (TEXTO, TABLA, IMAGEN)\n",
    "- Usar diferentes colores para múltiples fuentes\n",
    "\n",
    "En este notebook, utilizarás los modelos IBM Granite, capaces de procesar diferentes modalidades, mejorados con las capacidades de grounding visual de Docling para crear un sistema de IA transparente y verificable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_S93MsAZz1S"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eDALN1A9LF8"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "En este laboratorio, aprenderás a:\n",
    "\n",
    "1. **Configurar Docling para el Grounding Visual**: Configurar el procesamiento de documentos para mantener referencias visuales\n",
    "2. **Procesar Contenido Multimodal**: Manejar texto, tablas e imágenes con los metadatos adecuados\n",
    "3. **Aprovechar los Modelos de Visión AI**: Utilizar los modelos de visión IBM Granite para entendimiento de imágenes\n",
    "4. **Construir una Base de Datos Vectorial**: Almacenar embeddings con metadatos para visual grounding\n",
    "5. **Implementar Atribución Visual**: Mostrar a los usuarios exactamente de dónde proviene la información\n",
    "6. **Crear un Pipeline RAG Completo**: Combinar todos los componentes en un sistema listo para producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSIhob4jZcAN"
   },
   "source": [
    "### Tecnologías que usaremos\n",
    "\n",
    "Componentes clave:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** Un kit de herramientas de código abierto utilizado para analizar y convertir documentos.\n",
    "2. **[LangChain](https://langchain.com)**: Para orquestar el pipeline RAG\n",
    "3. **[IBM Granite Vision Models](https://www.ibm.com/granite/)**: Para entendimiento de contenido de imágenes\n",
    "4. **Visual Grounding**: La capacidad única de Docling para atribución de fuentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQlsFgvGZ9hH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vooxv7ltEZBf"
   },
   "source": [
    "## Prerequisitos\n",
    "\n",
    "Antes de comenzar, asegúrate de tener:\n",
    "- Completados los Laboratorios 1 y 2 (o conocimiento equivalente de Docling)\n",
    "- Python 3.10, 3.11 o 3.12 instalado\n",
    "- Comprensión básica de embeddings y bases de datos vectoriales\n",
    "- Familiaridad con los conceptos de los laboratorios anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoM938B_JRH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BfMWUUSs_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ae0cf79a-d586-4836-dbdc-97d18b217a16",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    'langchain_huggingface[full]' \\\n",
    "    langchain_milvus 'pymilvus[milvus_lite]'\\\n",
    "    docling \\\n",
    "    replicate \\\n",
    "    matplotlib\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gu-Oeay_JRJ"
   },
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G-ke7lF4CFwW"
   },
   "outputs": [],
   "source": [
    "# To see detailed information about the document processing and visual grounding operations, we'll configure INFO log level.\n",
    "# NOTE: It is okay to skip running this cell if you prefer less verbose output.\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rJwoqaBPHySg"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocling_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlabels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocItemLabel\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# LangChain imports for RAG pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Replicate\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_huggingface'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "import io\n",
    "import itertools\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "from PIL import ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "# Docling imports for document processing and visual grounding\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "from docling.chunking import DocMeta\n",
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "from docling_core.types.doc.document import TableItem, RefItem\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "\n",
    "# LangChain imports for RAG pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Model imports\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from ibm_granite_community.notebook_utils import get_env_var, escape_f_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yodQeEWVa3Xa"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyixsnLzCFwW"
   },
   "source": [
    "### Selección de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB_Hry0ebAtE"
   },
   "source": [
    "### Los Tres Pilares de RAG Multimodal\n",
    "\n",
    "Para un sistema completo de RAG multimodal con visual grounding, necesitamos tres tipos de modelos, cada uno cumpliendo un propósito crucial:\n",
    "\n",
    "1. **Modelo de Embeddings**: Convierte texto en representaciones vectoriales\n",
    "   - Permite la búsqueda semántica (\"encontrar contenido similar en significado\")\n",
    "   - Debe manejar texto de fragmentos, tablas y descripciones de imágenes\n",
    "\n",
    "2. **Modelo de Visión**: Entiende y describe contenido visual\n",
    "   - Procesa imágenes encontradas en documentos\n",
    "   - Genera descripciones textuales para su recuperación\n",
    "\n",
    "3. **Modelo de Lenguaje**: Genera respuestas finales\n",
    "   - Sintetiza la información recuperada\n",
    "   - Produce respuestas coherentes y precisas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEIpVe6yIAN6"
   },
   "source": [
    "### Modelo de Embeddings\n",
    "\n",
    "Usaremos un [modelo de embeddings Granite](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb) para generar vectores de embeddings de texto.\n",
    "\n",
    "\n",
    "- Optimizado para texto en múltiples idiomas\n",
    "- Compacto (107 millones de parámetros) para un procesamiento rápido\n",
    "- Excelente comprensión semántica\n",
    "- Ventana de contexto de 512 tokens\n",
    "\n",
    "Si deseas usar un modelo diferente, puedes consultar [esta receta de modelos de embeddings de la comunidad de IBM Granite](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "30aed391581c47598facbe74763a71f3",
      "bb9f7ac4cfce4dc1aa2cd4f9456c4a05",
      "a8214a9c57ca467faa6b32127332d1b7",
      "7ebbf8143a604b409e558131b83706a9",
      "bd7b3990a2e5414a8bfdd5d58aa4c6c4",
      "1d17691f611147058a744cf1b6d92ec5",
      "005e089874c14d16a72d263559c6f11e",
      "f519c63bf62b471eb4138a54774b25b2",
      "46444911d17e43e28758b79d6fd1698c",
      "105cf6ef3ba8456c8666a158d0f3075a",
      "ded96aceadd4480b89374a99d67f56e2",
      "ffa777f2454743a6ab66c35bbd1ac9dd",
      "146abad9d66d4c2ca8f2f31ffe4bee64",
      "1b9419b296104289a2f68d3bc2615ea2",
      "f87a0f86f8ed41579edb87cb2936b980",
      "ff1e65972bee45eeafb4070ca8064517",
      "b9491b7c2a83410097591f434df30df1",
      "268a66e6bdd94deaad362f7e6e860f87",
      "0511ef3a989a4cd49fa4909918e53cf9",
      "7ad3eeeb3980431a91f55b3d7bd97529",
      "c768a35348e14b2cad0d9bc6ed047f41",
      "3109510d96ca4385aca0a3d6b0b99e65",
      "8d94a499aa02419da19ff30e9a4ccc8b",
      "ff2b9ac983e54316858ea7f870d041a9",
      "809e4f104bd04244a5bc2c1c65790497",
      "de429ecb3f7c44978cf6bd9085259933",
      "ea4efa50585b49e5a3242c64fb83a241",
      "dc81b5fd026544ba975f4baa890c9ed2",
      "1078c105925a437b93f26a8f04662b88",
      "c82003732c1146d0b5b59c757f1859b8",
      "78e5987e84d14efab9619dd6de9da651",
      "45bdc919748f41ec946a78453533b9f1",
      "63db65a7e1ba44a49953e564475774e1",
      "0d13347b9e4848d5998b2419ed5765b4",
      "83cb2acd36634bd8ab2e323b54cc8f43",
      "5530b12b7d7146b7be524bf542509765",
      "bef293aaada14720b396c2b2b464e5fe",
      "be3cd11187f04f2ba79a29d6a93ad80b",
      "701317fcc9e047678eca988cda1f0b19",
      "67e6bc45803c4b069c1a054fe1ee28f0",
      "f28fc775dcd34c66bf48c41f829dde38",
      "c1bfcd0e354c4805a13d0b1fc111ee45",
      "6399b41f5b9a4be8813893cd3224b1ce",
      "69570c2887f04f3f8d67bbd975eaa79e",
      "73c46fdee29e46068167061884060eb5",
      "8e523d385eb743899c1086245f25f72a",
      "c699c8d54bf845ddb0c4519a135dd862",
      "fb534df5f7b34766b8a8e783c2008466",
      "09563f36f1c8401c98acdb6176deb521",
      "18bdb8d9c93a4251842c161ce7c15926",
      "e22cbf4f9bcb4370abfebeb52272d15c",
      "42057f222a1e462fb146991dae15b5fd",
      "7e8a2b4d2d914a118ccfd66c460ce89c",
      "bb496aa21cb841e5a368f12a7ef51209",
      "f0b3502132724ec8a6c6345855b6a74d",
      "c62bedacd48d4c70a1db6896bd3ead1b",
      "6108fd6deb8c448fa37918088c9ac44a",
      "d388ea4db4d14dbaad7fb7f2af893fdf",
      "e5c720ed8ff44bf995dd22cc346364ca",
      "49689529e77541fea13c6ae5b9bfb63b",
      "ad8827ca671d4732a55a62bedd6533de",
      "312ad4ea71454b55baab0c6552a4dac8",
      "77ae0c3def7348d987d91a3aef9f8d12",
      "2d856987a7994377b4482d4229681a5c",
      "35f8a006f3c84b168282a927f065afc8",
      "c66a1538bb0b44c6a617ba57189ef7b9",
      "99f8b9c321f940d885df7cfa57d104ae",
      "65278a69c9a24cddbbd1b20ab08abaeb",
      "fe3acc6c2c19435891201245f1bf078c",
      "65595dd5223c457e863e9ff049f6f90b",
      "f34b67eba2ca41e4a3a8c08e6649256c",
      "17361220c0594b9ca30f6500543db6e8",
      "36a09ad6dd31454d8b8185a7a67e36fc",
      "c5e095ef30f7444dbf98a3356c676234",
      "a9116fd44acf4d3290d43f7643d372d1",
      "72814321ea6440e1a6fe2a5f83236376",
      "e3e72eb246594d3497e175ad017e6267",
      "17b72d7de14b48c296350ed182da1abd",
      "aabfc86e48924653b7b816f3653d2ca6",
      "60ce3f06c5144ba3b8f13cb9813ad8c6",
      "91e10ba5e5dd45c1a6e869829f657264",
      "ea0ca547ee1b4d7895a16f525eae20d5",
      "c029c57f8d2a4f89bbe759281c3ad16f",
      "2d870e4a1d3146e49c71c07306b6a10d",
      "0256b42e2d9e43658a52d9bb767fec3c",
      "e05013c54d594ce28aa98b2ac2d72dd2",
      "1ba73dfd141f43aa9ba337192ad39667",
      "7049612009674999be46373e34af2245",
      "927b891f7b7f4061b0ba2ba71de64dc7",
      "0f963da83c9b4469afe820915871fc62",
      "71e6963c6e6d4753bef3e6f036061558",
      "50bd57c1bac040cb860f2aea40e2f64e",
      "e0b9bfb59c8044f19087cef6b08f3d15",
      "3381fa31190349e6bb996e1e333704f0",
      "5ff928c8b9694b2997eb492519cb73ef",
      "488135d46a0c4ac2af0feba79f1e2b12",
      "9d5ca3199c2940e486c8e691abcfe370",
      "50da0c70814f4623a599891fb022caa5",
      "95c84535d1fc4223ae24694e2d8f932d",
      "873b3db7c970448b887deab25c48d37f",
      "08e964cf2358451e8b07678709e98ef2",
      "35fca115556c425aa1f205294cd302b4",
      "16c4dbd4c35a4c78b5254a245028c710",
      "0cda12f4cb304c68ab52be19030ae4bd",
      "81619090425b434bbee76003feec3baa",
      "bd21dad115cf4e77937a7f2701cbcae5",
      "72b986829a23454f9d0381e23e731e6d",
      "9d36c4e7a66447c1bd93028168494b21",
      "197ce4c0d60f43d786064011d0655d63",
      "01eb40035b6544ddb790da91c4c49f5b",
      "a4654dce525f428ea1826f551f6ecf96",
      "1e544112cf7d44db8c3c0538be77bf1b",
      "38764c10d2af43198d15d50241405bf6",
      "20f39553605149899141309285c4d3ea",
      "0116ae1b4a724e4eb3a2d890f6e2d916",
      "dd1ec695d2be45ce957691f086b90c10",
      "fe183c499c1e414ca6c1c41347a74594",
      "19ca17543ff443e19f043478cd82657a",
      "d8723555c1ce41a28a2dcfc24b8ed26d",
      "02d6cc2e89d54f699475cb9f070900a7",
      "12caabb1fa3349afa1b5ba246484534c"
     ]
    },
    "id": "mvztNZly_JRJ",
    "outputId": "b65d35cf-4549-4e77-87f9-46ba5c3f6138"
   },
   "outputs": [],
   "source": [
    "embeddings_model_path = \"ibm-granite/granite-embedding-107m-multilingual\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)\n",
    "print(f\"Embeddings model loaded: {embeddings_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9nTm3qICFwW"
   },
   "source": [
    "### Cargar el Modelo de Visión Granite\n",
    "\n",
    "El modelo de visión nos ayudará a comprender las imágenes dentro de los documentos. Esto es crucial para un RAG verdaderamente multimodal, ya que muchos documentos contienen información visual importante.\n",
    "\n",
    "**¿Por qué usar un modelo alojado en la nube en lugar de uno local?**\n",
    "- Procesamiento más rápido sin necesidad de GPU\n",
    "- Rendimiento consistente\n",
    "- Fácil de escalar\n",
    "\n",
    "**Nota**: Para configurar Replicate, consulta [Introducción a Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "Para conectarte a un modelo en un proveedor diferente a Replicate, consulta la [receta de LLMs de Langchain de la comunidad de IBM Granite](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377,
     "referenced_widgets": [
      "a534e43df09f4bd184b5ec03735ba106",
      "35df11e94a084fca84a79ff238dd41dc",
      "fcc03f7726b0475b8c692f8102f300db",
      "e38eb7bd68c443b49597a42bf99fddb6",
      "9f0f75412d184759a4c866ce0c57ebca",
      "8fe1a9a63b1f48878b3c46ee8aadcc79",
      "fc671127aeab4397b217363b696a8d32",
      "1ac06c72a57d450f959e4f676f3eb953",
      "eda16fcae5774b6ab97db390f2cf6212",
      "0358db24b603458c94bfbaa265bcd699",
      "8ea2420779ae49938724db859bdfc218",
      "f15ae056261348698338ef17784b267c",
      "c89e1c71be9a4f9d9c520a9e655de69d",
      "021d9eca558d4c52a7d5719fa595019a",
      "b44231c04c5a42808cc49bfef5cb5761",
      "afc6940a831a4712a1c6ec3bdcdeed7c",
      "6a2ef5dbc966469c9e68bde49fa7b912",
      "03d324aaa6734a058bd2ae8ce3fcc198",
      "85874d0df7ba4223b2fc49a150ccf235",
      "018db3282d104bfdbad8e79d568e6452",
      "1656c7ac85ab48949681a70f34706ec4",
      "18e60b6f5e1c415dbf1726d5678ff69a",
      "d7776f5095b1447a87af1a5ff743ffea",
      "f58d57126aca42d7ac30eb5f0d6c5862",
      "c6f57eb2d7144b9382cbe71e65bd78c9",
      "f2c5e47cd6194c7bb48228ea6abdece2",
      "033c76f5356f4e7290d0f7c9f5c7e976",
      "ba4b9c4a029c445a9c2d7bc2f4b94c2f",
      "279c4bf80ed34918a39c9eed77e20448",
      "e848ffc816fd4ac0ae220a575d816257",
      "5535857e2f964b80947edb78e2af619c",
      "7784639f83124f9e8891390744b5f7e8",
      "ccc8df11a0a748d6ae1098f718bf909a",
      "936fbdcc7f8f4d7e9f4c60a91f1bce82",
      "c0863ca49d4343dda2f1e522305ae51c",
      "22793dec401e416ab7e4a1fff8731353",
      "14005f1853094e009bc0792807b56132",
      "4618c66a4e964ea2b6ebbb0b2f620abf",
      "91e6cb46a47f429fa124ee4830905f20",
      "eba20cb1f3094c56a743398a488117a9",
      "3d97db037b7e445d854bcceac1478fea",
      "df5f7eadbaa544a9a90a8325440559ba",
      "5378f121bb09445d83fdaf58d7260b28",
      "7affa0c2c9694c5f8e117ce1820ff07d",
      "8b051518b298442ea0e50ae4de00b607",
      "99776d2b3526490da7dd7f91aace918d",
      "3f97c371ec024e308e4e4bf9deac89a1",
      "c11039a4c7644d51bf815a8c6a147b62",
      "011f08213068401dabee1bef4f21cd30",
      "23589f5c9174456e90a91ad04ab3190b",
      "9980cf7d59434126b5aa29c724b924cc",
      "616ddd6f314d47c3941762d2423e53a9",
      "d6c46e36441a4b8c8cedae9379528e49",
      "033085b9dc7b4a229f10560ec13ed5e4",
      "197584bb2ee748ffae5a12d773c74c59",
      "4158830094b849bda48c0f3383762906",
      "5ce4d6659b09413c9d1c023c6c02d0a0",
      "c5e2a132b2c54a1aaef37d05b6c02d24",
      "a512326a7ce64f51b7acccde449cf211",
      "e5d42ce5909f4bad8988074b3f7e095b",
      "df483317ae1a491d8a6823fd0c542371",
      "d86b3c886e094fecbebac6d06a60c83e",
      "689f0d3ec2fe4a2cb4a906c4a8727bb3",
      "cb9eb27155bc45b3a7581d41a8fbc924",
      "06610e0f196146a9bf3f6d5bd542b3c4",
      "c704fb332dcd4f558684ff8dbbcaab5d",
      "9f299a085bdc49cf9928a8b727bf745f",
      "bfe7ee9615354df2b55844ac87e53347",
      "d7b4dfe4d3f34302986549791633a5a4",
      "9fc1888a5f054a7ea1773f343519234b",
      "328a38ed6f774e229cf94220b5e87d26",
      "581df90d93a24b3c85f73147e84a04db",
      "7c0dc1f65d8f477ebc9984e9014bbc7f",
      "00068a3f007e48bcabe426b077055012",
      "b6bb194f067d4e8bb3eb74ee34c617e5",
      "f36158e5ffe8438a98207c333476e885",
      "c89195399f53453caa59964dfc8fb311",
      "20723c36ae014a79aae19124387ce7f4",
      "208333b9ad43414db26be168e88a9764",
      "067dfbebbd6b493ebe8e56cc68efe016",
      "db953222e9514659bc18e4e9b15d756b",
      "7fb731dd508d47f5a7abb5eb16e790d7",
      "58ee2cc2bd084374a294e8d3edc0833e",
      "df2261a91c354e4aa3133b1eb19e48d9",
      "80b2b32f78314bc89d549242a1f24a26",
      "aa8b3d7be6604fc58fbb55e2016a4f24",
      "896a033ae3894e8990cb626883ad7459",
      "3033414c0baf4d03857520b79de534fc",
      "221c2936753f4954b81fa29d167303c9",
      "ea3a35bc32524806957cc6ef391f035e",
      "b6cde6b309bf4c79afa1e7250e737ed7",
      "79705b356a834632aa5324b66daeef1d",
      "e50b987dc41c4231aa8a5c9219fc49d5",
      "6cacca41b9024c329d2bcba9e821d012",
      "b14dd3351e7f4465a057e2038ff1108d",
      "ea809b34f1d545fea87b12d16a991991",
      "c21796d6756c4247bac05db4453d80eb",
      "6a477ba04063486d87f937d97dac615e",
      "80ec01e074b74fc5829a4be11cc64887"
     ]
    },
    "id": "-5272bCOCFwW",
    "outputId": "d6ad7a1f-2988-4acc-8622-710a694d5702"
   },
   "outputs": [],
   "source": [
    "vision_model_path = \"ibm-granite/granite-vision-3.3-2b\"\n",
    "vision_model = Replicate(\n",
    "    model=vision_model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence,\n",
    "        \"min_tokens\": 100,\n",
    "        \"temperature\": 0.01 # low temperature for reproduceability\n",
    "    },\n",
    ")\n",
    "vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n",
    "print(f\"Vision model loaded: {vision_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma8eWR10_JRJ"
   },
   "source": [
    "### Cargar el Modelo de Lenguaje Granite\n",
    "\n",
    "Finalmente, nuestro modelo de lenguaje generará respuestas basadas en el contexto recuperado.\n",
    "\n",
    "**¿Por qué usar este modelo?**:\n",
    "- 8B parámetros: Buen equilibrio entre calidad y velocidad\n",
    "- Ajustado a instrucciones: Sigue nuestros prompts con precisión\n",
    "- Familia Granite: Código abierto y con licencia Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "44c6e3c381944af9ba6850e66e066f1f",
      "10f070266fff46509ca258b61dae89e5",
      "f0e36bc66e794361aee399c9c4f2d683",
      "d03e6698b9d74f00af51217c993fa765",
      "a5bbf421325644fdbc6edf12f8a034d9",
      "64f672136d8c4d4abc184e6fa973b61e",
      "0c838d83f14b4f37bbcf83e7f5401049",
      "8f32c79df5d942129f3bdbc3342d8a05",
      "1eaa73c964af482b8a6852ddcdbc4830",
      "f67521ed26cd46e982675813d0fa5167",
      "b12c8c3c552e451693c1063a298e7a6d",
      "99b1488e5d094dba9e72c7bc2ca21427",
      "cbb843c832fd4527a107089ca5f2fad3",
      "07ead5501f5b4c358a2b0baa34b08490",
      "2c5b43a5f60544fc9b6f5b5e7114b1b8",
      "40a81bb7bd1e475885afbd2e09ef1fc6",
      "85a41be27c604244b35c582e32be1c30",
      "0749c65dab7e4a46b2f3dce6ccde0987",
      "06eca4f2a8cc4bc6a22f190a68ca53e5",
      "d8b0ed5af0a24cb1ae9d530de10b91f5",
      "e31a3cf13e1b46d9a304cfff8ba8cc4d",
      "9b6f8669ac5a459f84ec622b831e6b94",
      "3fe5bb2331e34a62a54d07dc6ccb181b",
      "7ee6bf328f9c41779e0ec04a11128d23",
      "f52d124f4eaf47209d9736876b079301",
      "f8a832d2353844efb5605d9c833a38b5",
      "bf4655af6bc54811b9c190a796088651",
      "6560e20b62374bddac5c2ba17c463b34",
      "7fa8f0281e57466691155c9372286419",
      "6d1220067e184e0f9637f10962ea8aff",
      "46fa9c2dbdc44d8ba27e551ff6d84b65",
      "df8b0f131e9a44e2a925d34ca2ccecb6",
      "e7c88f3fb527430caaf311309de069a5",
      "76c3c4d8293d4953876edff0f1a75898",
      "657e53b228354a5695f73b5a7f6e2335",
      "e00786348c644dcdaf5026296145e703",
      "2d2f6e60a1c8457593943784a91c20fa",
      "dc6d9d6bb5d14358b3c1d15f93d6d398",
      "7fc30294363d4e9d8f1d38297996f17d",
      "ba50a4b2fefd4b069a8f02f169d4b753",
      "515b69ef7ef640c1b2d0c078c4075c0d",
      "543b1fa684d24d57b5756816a1948dd6",
      "6c3f9316cc424841a7bf2d8a9e70df70",
      "40062877163d4ec38610bc4cbf0f6fcb",
      "8efb1ca61b53466abedc92f95ae16633",
      "d87cb3db4f1749bc8f749d2bf7fc7d45",
      "2a233805f4f2437595b06c70a4d702b1",
      "c3f51829de3b448291e0cc66cff2312b",
      "218a3779ad18493eb1d23a06c6774388",
      "b69c6b35e51d496891dc2af739ee6526",
      "198a035bd5e94ab7a7b8768ce5c85e0a",
      "5bccd67539f94be19746bc2777eac659",
      "82796002bad64fada9314d34bcf4fecd",
      "c0c081a48abe4410b16e60173eaa4326",
      "51cb65a268a74e2e9e8b5e357c3b022f",
      "65d629651352431384420d3606289847",
      "cce0890083934592b0f60e04fb9622ac",
      "70690a4626714ab397e249ad2cfe286e",
      "3351e52693674feaac570cf92c83bf55",
      "a7c612ec254d4de59ea7a85022610b9a",
      "3e1b1638c3f64d76a2b6990a7474385c",
      "ddb41d7e7adf4d97906ec238bd63edc6",
      "69257ae2351c4565a97320b554fc3c16",
      "4939785c8a514734ba57f363860fd587",
      "b2be447bf8844e4eaba87d100f1af07b",
      "ac210c1417ac4f16bf4ec22a111bd6cc"
     ]
    },
    "id": "Ckyj7Zrh_JRK",
    "outputId": "41c24db5-5e33-4afb-e358-fdb2efe3ff38"
   },
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"min_tokens\": 100,\n",
    "        \"temperature\": 0.01 # low temperature for reproduceability\n",
    "\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(f\"Language model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVfNx7tCcB8l"
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviHG3n7_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Procesamiento de documentos con soporte para visual grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0TSvvRFJTDJ"
   },
   "source": [
    "El visual grounding requiere una configuración especial durante la conversión de documentos. A diferencia del procesamiento estándar, necesitamos:\n",
    "\n",
    "1. **Generar imágenes de página de alta calidad**: Para resaltar elementos de la página visualmente.\n",
    "2. **Preservar la información de coordenadas**: Para saber dónde se encuentra el contenido.\n",
    "3. **Mantener la estructura del documento**: Para una atribución de fuentes precisa.\n",
    "4. **Almacenar los documentos adecuadamente**: Para su posterior recuperación y visualización.\n",
    "\n",
    "Vamos a configurar Docling con estos requisitos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3jGNMkKJZ-R",
    "outputId": "b85c1c47-5206-4701-fd54-20212154f568"
   },
   "outputs": [],
   "source": [
    "# Configure the document converter to support visual grounding\n",
    "pdf_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=False,  # Set to True if your PDFs contain scanned images\n",
    "    generate_picture_images=True,  # Extract images from documents\n",
    "    generate_page_images=True,  # CRITICAL: Generate page images for visual grounding\n",
    ")\n",
    "\n",
    "format_options = {\n",
    "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
    "}\n",
    "\n",
    "converter = DocumentConverter(format_options=format_options)\n",
    "print(\"Document converter configured with visual grounding support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ7Guu7A_JRK"
   },
   "source": [
    "### Creamos un almacén local de documentos para el visual grounding\n",
    "\n",
    "Almacenaremos documentos que mantendrán la estructura completa del documento necesaria para el visual grounding. Esto es esencial para resaltar las coordenadas de origen más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNGz_0gZ_JRK",
    "outputId": "f3585ca6-1668-49ae-c7cf-252301e98d94"
   },
   "outputs": [],
   "source": [
    "# Create document store for visual grounding\n",
    "doc_store = {}\n",
    "doc_store_root = Path(mkdtemp()) # Temporary directory for document store\n",
    "print(f\"Document store created at: {doc_store_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSLYgc7JCFwX"
   },
   "source": [
    "### Convertir documentos con seguimiento visual\n",
    "\n",
    "Ahora procesaremos documentos mientras preservamos toda la información necesaria para el visual grounding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLDMCxFbCFwX",
    "outputId": "d2a325a8-04ef-4cf2-94ac-c8c6a9f372e0"
   },
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"https://arxiv.org/pdf/2501.17887\" # Docling paper\n",
    "    # \"https://arxiv.org/pdf/2206.01062\", # DocLayNet paper\n",
    "    # \"https://arxiv.org/pdf/2311.18481\", # DocQA\n",
    "    # Añade más documentos según sea necesario\n",
    "]\n",
    "\n",
    "conversions = {}\n",
    "\n",
    "print(\"Iniciando la conversión de documentos con visual grounding...\")\n",
    "for source in sources:\n",
    "    # Por cada fuente, convertimos el documento preservando las imágenes y guardándolas en nuestro almacén local de documentos\n",
    "    print(f\"\\n Procesando: {source}\")\n",
    "\n",
    "    # Convert document\n",
    "    result = converter.convert(source=source)\n",
    "    docling_document = result.document\n",
    "    conversions[source] = docling_document\n",
    "\n",
    "    # Save document to store for visual grounding\n",
    "    # The binary hash ensures unique identification\n",
    "    file_path = doc_store_root / f\"{docling_document.origin.binary_hash}.json\"\n",
    "    docling_document.save_as_json(file_path)\n",
    "    doc_store[docling_document.origin.binary_hash] = file_path\n",
    "\n",
    "    print(\"Documento convertido y guardado en el almacén local.\")\n",
    "    print(f\"  - Document ID: {docling_document.origin.binary_hash}\")\n",
    "    print(f\"  - Paginas: {len(docling_document.pages)}\")\n",
    "    print(f\"  - Tablas: {len(docling_document.tables)}\")\n",
    "    print(f\"  - Imágenes: {len(docling_document.pictures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vX3XqSOKBos"
   },
   "source": [
    "## Procesado del Contenido del Documento con Metadatos de Atribución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dvccMBeKC7c"
   },
   "source": [
    "### La Importancia de los Metadatos para el Visual Grounding\n",
    "\n",
    "Para que el visual grounding funcione, cada fragmento de contenido debe mantener metadatos sobre su ubicación de origen. Esto incluye:\n",
    "- **Números de página**: Qué página(s) contienen este contenido\n",
    "- **Cajas delimitadoras**: Coordenadas exactas en la página\n",
    "- **Referencias de documentos**: Enlaces de regreso al documento fuente\n",
    "- **Tipo de contenido**: Si es texto, tabla o imagen\n",
    "\n",
    "Este metadato es lo que nos permite resaltar regiones de interés específicas en las páginas del documento más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdT7gzIeKFyd"
   },
   "source": [
    "## Procesamos los Chunks de Texto con metadatos de ubicación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zgnHW4UCFwX"
   },
   "source": [
    "Ahora procesamos cualquier tabla en los documentos. Convertimos los datos de la tabla al formato markdown para pasarlos al modelo de lenguaje. Se crea una lista de documentos LangChain a partir de las representaciones en markdown de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDa6yik4CFwX",
    "outputId": "a8ff50a6-20d6-4a0f-c198-a7a6e4dc9f46"
   },
   "outputs": [],
   "source": [
    "from docling.chunking import DocMeta\n",
    "\n",
    "doc_id = 0\n",
    "texts: list[Document] = []\n",
    "\n",
    "print(\"\\nProcessing text chunks with visual grounding metadata...\")\n",
    "for source, docling_document in conversions.items():\n",
    "    chunker = HybridChunker(tokenizer=embeddings_tokenizer)\n",
    "\n",
    "    for chunk in chunker.chunk(docling_document):\n",
    "        items = chunk.meta.doc_items\n",
    "\n",
    "        # Skip single-item chunks that are tables (we'll process them separately)\n",
    "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
    "            continue\n",
    "\n",
    "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
    "        text = chunk.text\n",
    "\n",
    "        # Create document with enhanced metadata for visual grounding\n",
    "        document = Document( # langchain_core.documents.Document\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"doc_id\": (doc_id:=doc_id+1),\n",
    "                \"source\": source,\n",
    "                \"ref\": refs,  # References for tracking specific document items\n",
    "                \"dl_meta\": chunk.meta.model_dump(),  # CRITICAL: Store chunk metadata for visual grounding\n",
    "                \"origin_hash\": docling_document.origin.binary_hash  # Link to stored document\n",
    "            },\n",
    "        )\n",
    "        texts.append(document)\n",
    "\n",
    "print(f\"Created {len(texts)} text chunks with visual grounding metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzoFPRBjCFwX"
   },
   "source": [
    "### Procesamos las tablas con información espacial\n",
    "\n",
    "Las tablas requieren un manejo especial para preservar su estructura y ubicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaiUN8nVCFwX",
    "outputId": "bb25ef96-c755-4884-89bb-093e47524dd1"
   },
   "outputs": [],
   "source": [
    "doc_id = len(texts)\n",
    "tables: list[Document] = []\n",
    "\n",
    "print(\"\\nProcessing tables...\")\n",
    "for source, docling_document in conversions.items():\n",
    "    for table in docling_document.tables:\n",
    "        if table.label in [DocItemLabel.TABLE]:\n",
    "            ref = table.get_ref().cref\n",
    "            text = table.export_to_markdown(docling_document)\n",
    "\n",
    "            # Extract provenance information for visual grounding\n",
    "            prov_data = []\n",
    "            if hasattr(table, 'prov') and table.prov:\n",
    "                for prov in table.prov:\n",
    "                    # Get the page to access its height for coordinate conversion\n",
    "                    if prov.page_no < len(docling_document.pages):\n",
    "                        page = docling_document.pages[prov.page_no]\n",
    "                        # Convert to top-left origin and normalize\n",
    "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                        bbox_norm = bbox.normalized(page.size)\n",
    "\n",
    "                        prov_data.append({\n",
    "                           \"page_no\": prov.page_no,\n",
    "                           \"bbox\": {\n",
    "                              \"l\": bbox_norm.l,  # Use normalized coordinates\n",
    "                              \"t\": bbox_norm.t,\n",
    "                              \"r\": bbox_norm.r,\n",
    "                              \"b\": bbox_norm.b\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
    "                    \"item_type\": \"table\",  # Mark as table\n",
    "                    \"prov_data\": prov_data  # Store provenance as simple data\n",
    "                },\n",
    "            )\n",
    "            tables.append(document)\n",
    "\n",
    "print(f\"Created {len(tables)} table documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWHbpeEuCFwX"
   },
   "source": [
    "### Procesamos las imágenes con entendimiento visual\n",
    "\n",
    "Para un verdadero RAG multimodal, necesitamos entender el contenido de las imágenes. Usaremos el modelo de visión Granite para generar descripciones.\n",
    "\n",
    "**NOTA**: El procesamiento de imágenes puede llevar tiempo dependiendo de la cantidad de imágenes y del servicio del modelo de visión. Cada imagen será analizada individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htIYVVjHPKSX",
    "outputId": "251f0d3e-132c-43b6-eeae-e87c6bd87a03"
   },
   "outputs": [],
   "source": [
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    \"\"\"Encode image to base64 for vision model processing\"\"\"\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri\n",
    "\n",
    "# Configuración del prompt de visión - siéntete libre de experimentar con esto!\n",
    "image_prompt = \"Give a detailed description of what is depicted in the image\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": image_prompt},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "vision_prompt = vision_processor.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "pictures: list[Document] = []\n",
    "doc_id = len(texts) + len(tables)\n",
    "\n",
    "for source, docling_document in conversions.items():\n",
    "    num_pictures = len(docling_document.pictures)\n",
    "    for i, picture in enumerate(docling_document.pictures):\n",
    "        ref = picture.get_ref().cref\n",
    "        print(f\"  Processing image: {ref} ({i+1}/{num_pictures})\")\n",
    "\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            # Generate image description using vision model\n",
    "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
    "\n",
    "            # Extract provenance information for visual grounding\n",
    "            prov_data = []\n",
    "            if hasattr(picture, 'prov') and picture.prov:\n",
    "                for prov in picture.prov:\n",
    "                    # Get the page to access its height for coordinate conversion\n",
    "                    if prov.page_no < len(docling_document.pages):\n",
    "                        page = docling_document.pages[prov.page_no]\n",
    "                        # Convert to top-left origin and normalize\n",
    "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                        bbox_norm = bbox.normalized(page.size)\n",
    "\n",
    "                        prov_data.append({\n",
    "                            \"page_no\": prov.page_no,\n",
    "                            \"bbox\": {\n",
    "                                \"l\": bbox_norm.l,\n",
    "                                \"t\": bbox_norm.t,\n",
    "                                \"r\": bbox_norm.r,\n",
    "                                \"b\": bbox_norm.b\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
    "                    \"item_type\": \"picture\",  # Mark as picture for special handling\n",
    "                    \"prov_data\": prov_data  # Store normalized provenance data\n",
    "                },\n",
    "            )\n",
    "            pictures.append(document)\n",
    "\n",
    "print(f\"Created {len(pictures)} image descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    \"\"\"Encode image to base64 for vision model processing\"\"\"\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV3N4DIVL1a_"
   },
   "source": [
    "### Mostramos una muestra de los documentos procesados\n",
    "\n",
    "Examinemos lo que hemos creado para entender la naturaleza multimodal de nuestro sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "C_F_3_ZUL1yS",
    "outputId": "5a54c832-6f9d-432f-8df1-1205791b0b8d"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"\\nSample processed documents:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample text chunks\n",
    "print(\"\\nTEXT CHUNK EXAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "for i, text_doc in enumerate(texts[:3]):  # Show first 3 text chunks\n",
    "    print(f\"\\nText Chunk {i+1}:\")\n",
    "    print(f\"  Document ID: {text_doc.metadata['doc_id']}\")\n",
    "    print(f\"  Source: {text_doc.metadata['source'].split('/')[-1]}\")  # Just filename\n",
    "    print(f\"  Reference: {text_doc.metadata['ref']}\")\n",
    "    print(f\"  Has visual grounding: {'dl_meta' in text_doc.metadata}\")\n",
    "    print(\"  Content preview:\")\n",
    "    print(f\"    {text_doc.page_content[:250]}...\")\n",
    "    if i < 2:  # Add separator between examples except after the last one\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Show sample tables\n",
    "print(\"\\n\\nTABLE EXAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "if tables:\n",
    "    for i, table_doc in enumerate(tables[:3]):  # Show first 3 tables\n",
    "        print(f\"\\nTable {i+1}:\")\n",
    "        print(f\"  Document ID: {table_doc.metadata['doc_id']}\")\n",
    "        print(f\"  Reference: {table_doc.metadata['ref']}\")\n",
    "        print(\"  Content preview (Markdown format):\")\n",
    "        # Show first few lines of the table\n",
    "        table_lines = table_doc.page_content.split('\\n')[:8]\n",
    "        for line in table_lines:\n",
    "            print(f\"    {line}\")\n",
    "else:\n",
    "    print(\"  No tables found in the document.\")\n",
    "\n",
    "# Show sample images with descriptions\n",
    "print(\"\\n\\nIMAGE EXAMPLES WITH AI-GENERATED DESCRIPTIONS:\")\n",
    "print(\"-\" * 80)\n",
    "if pictures:\n",
    "    for i, pic_doc in enumerate(pictures[:3]):  # Show first 3 images\n",
    "        print(f\"\\nImage {i+1}:\")\n",
    "        print(f\"  Document ID: {pic_doc.metadata['doc_id']}\")\n",
    "        print(f\"  Reference: {pic_doc.metadata['ref']}\")\n",
    "        print(\"  AI-Generated Description:\")\n",
    "        # Wrap the description for better readability\n",
    "        wrapped_text = textwrap.fill(pic_doc.page_content, width=70, initial_indent=\"    \", subsequent_indent=\"    \")\n",
    "        print(wrapped_text)\n",
    "\n",
    "        # Display the actual image\n",
    "        source = pic_doc.metadata['source']\n",
    "        ref = pic_doc.metadata['ref']\n",
    "        docling_document = conversions[source]\n",
    "        picture = RefItem(cref=ref).resolve(docling_document)\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            print(\"\\n  Original Image:\")\n",
    "            # Resize image for display if too large\n",
    "            display_image = image.copy()\n",
    "            max_width = 600\n",
    "            if display_image.width > max_width:\n",
    "                ratio = max_width / display_image.width\n",
    "                new_height = int(display_image.height * ratio)\n",
    "                display_image = display_image.resize((max_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
    "            display(display_image)\n",
    "\n",
    "        if i < min(2, len(pictures)-1):\n",
    "            print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"  No images found in the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W292HEjOOkC3"
   },
   "source": [
    "### Comprendiendo la Atribución Visual de Fuentes\n",
    "\n",
    "El grounding visual es lo que diferencia a este sistema. Las funciones que definiremos en las siguientes celdas nos permiten:\n",
    "1. **Localizar**: Encontrar la fuente exacta de cualquier información recuperada\n",
    "2. **Resaltar**: Dibujar indicadores visuales en las páginas del documento\n",
    "3. **Diferenciar**: Usar estilos distintos para texto, tablas e imágenes\n",
    "4. **Verificar**: Permitir a los usuarios confirmar las respuestas de la IA contra los documentos fuente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0SFvP-tOtkE"
   },
   "outputs": [],
   "source": [
    "# Esta función visualiza de dónde proviene un fragmento de texto en el documento original.\n",
    "def visualize_chunk_grounding(chunk, doc_store, highlight_color=\"blue\"):\n",
    "    \"\"\"\n",
    "    Visualize where a text chunk comes from in the original document.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the original document from the store\n",
    "    2. Finds the pages containing the chunk content\n",
    "    3. Draws bounding boxes around the source regions\n",
    "    4. Displays the highlighted pages\n",
    "\n",
    "    Args:\n",
    "        chunk: LangChain Document with visual grounding metadata\n",
    "        doc_store: Dictionary mapping document hashes to file paths\n",
    "        highlight_color: Color for highlighting (blue, green, red, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of page images with highlights\n",
    "    \"\"\"\n",
    "    # Get the origin hash\n",
    "    origin_hash = chunk.metadata.get(\"origin_hash\")\n",
    "    if not origin_hash:\n",
    "        print(\"No origin hash found in metadata\")\n",
    "        return None\n",
    "\n",
    "    # Load the full document from store\n",
    "    dl_doc = DoclingDocument.load_from_json(doc_store.get(origin_hash))\n",
    "\n",
    "    print(f\"Visualizing source location for chunk {chunk.metadata.get('doc_id', 'Unknown')}\")\n",
    "\n",
    "    # Handle different types of content\n",
    "    page_images = {}\n",
    "    item_type = chunk.metadata.get(\"item_type\", \"text\")\n",
    "\n",
    "    if item_type in [\"picture\", \"table\"] and \"prov_data\" in chunk.metadata:\n",
    "        # Handle tables and pictures with simple provenance data\n",
    "        prov_data = chunk.metadata[\"prov_data\"]\n",
    "\n",
    "        if not prov_data:\n",
    "            print(f\"No provenance data available for this {item_type}\")\n",
    "            return None\n",
    "\n",
    "        for prov in prov_data:\n",
    "            page_no = prov[\"page_no\"]\n",
    "\n",
    "            # Get page image\n",
    "            if page_no < len(dl_doc.pages):\n",
    "                page = dl_doc.pages[page_no]\n",
    "                if hasattr(page, 'image') and page.image:\n",
    "                    if page_no not in page_images:\n",
    "                        img = page.image.pil_image.copy()\n",
    "                        page_images[page_no] = {\n",
    "                            'image': img,\n",
    "                            'page': page,\n",
    "                            'draw': ImageDraw.Draw(img)\n",
    "                        }\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    draw = page_images[page_no]['draw']\n",
    "                    bbox = prov[\"bbox\"]\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    draw = page_images[page_no]['draw']\n",
    "                    bbox = prov[\"bbox\"]\n",
    "\n",
    "                    # The coordinates are already normalized and in top-left origin\n",
    "                    # Just scale to image dimensions\n",
    "                    img_width = page_images[page_no]['image'].width\n",
    "                    img_height = page_images[page_no]['image'].height\n",
    "\n",
    "                    l = int(bbox[\"l\"] * img_width)\n",
    "                    r = int(bbox[\"r\"] * img_width)\n",
    "                    t = int(bbox[\"t\"] * img_height)\n",
    "                    b = int(bbox[\"b\"] * img_height)\n",
    "\n",
    "                    # Ensure coordinates are valid (min/max) just in case\n",
    "                    l, r = min(l, r), max(l, r)\n",
    "                    t, b = min(t, b), max(t, b)\n",
    "\n",
    "                    # Clamp to image bounds\n",
    "                    l = max(0, min(l, img_width - 1))\n",
    "                    r = max(0, min(r, img_width - 1))\n",
    "                    t = max(0, min(t, img_height - 1))\n",
    "                    b = max(0, min(b, img_height - 1))\n",
    "\n",
    "                    # Draw highlight with different styles for different types\n",
    "                    if item_type == \"picture\":\n",
    "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=4)\n",
    "                        draw.text((l, t-20), \"IMAGE\", fill=highlight_color)\n",
    "                    elif item_type == \"table\":\n",
    "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=3)\n",
    "                        draw.text((l, t-20), \"TABLE\", fill=highlight_color)\n",
    "\n",
    "    elif \"dl_meta\" in chunk.metadata:\n",
    "        # Handle text chunks with DocMeta\n",
    "        try:\n",
    "            meta = DocMeta.model_validate(chunk.metadata[\"dl_meta\"])\n",
    "\n",
    "            # Process each item in the chunk to find source locations\n",
    "            for doc_item in meta.doc_items:\n",
    "                if hasattr(doc_item, 'prov') and doc_item.prov:\n",
    "                    for prov in doc_item.prov:\n",
    "                        page_no = prov.page_no\n",
    "\n",
    "                        # Get or create page image\n",
    "                        if page_no not in page_images:\n",
    "                            if page_no < len(dl_doc.pages):\n",
    "                                page = dl_doc.pages[page_no]\n",
    "                                if hasattr(page, 'image') and page.image:\n",
    "                                    img = page.image.pil_image.copy()\n",
    "                                    page_images[page_no] = {\n",
    "                                        'image': img,\n",
    "                                        'page': page,\n",
    "                                        'draw': ImageDraw.Draw(img)\n",
    "                                    }\n",
    "\n",
    "                        # Draw bounding box on the page\n",
    "                        if page_no in page_images:\n",
    "                            page_data = page_images[page_no]\n",
    "                            page = page_data['page']\n",
    "                            draw = page_data['draw']\n",
    "\n",
    "                            # Convert coordinates to image space\n",
    "                            bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                            bbox = bbox.normalized(page.size)\n",
    "\n",
    "                            # Scale to actual image dimensions\n",
    "                            l = int(bbox.l * page_data['image'].width)\n",
    "                            r = int(bbox.r * page_data['image'].width)\n",
    "                            t = int(bbox.t * page_data['image'].height)\n",
    "                            b = int(bbox.b * page_data['image'].height)\n",
    "\n",
    "                            # Draw highlight rectangle\n",
    "                            draw.rectangle([l, t, r, b], outline=highlight_color, width=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text chunk metadata: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No visual grounding metadata available for this chunk\")\n",
    "        return None\n",
    "\n",
    "    # Display highlighted pages\n",
    "    for page_no, page_data in sorted(page_images.items()):\n",
    "        plt.figure(figsize=(12, 16))\n",
    "        plt.imshow(page_data['image'])\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Add title indicating content type\n",
    "        if item_type == \"picture\":\n",
    "            title = \"Image Location\"\n",
    "        elif item_type == \"table\":\n",
    "            title = \"Table Location\"\n",
    "        else:\n",
    "            title = \"Text Location\"\n",
    "        plt.title(f'{title} - Page {page_no + 1}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return page_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuVVgC_YQaln"
   },
   "source": [
    "## Popular la base de datos vectorial con embeddings y metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wqnAw0Te1zi"
   },
   "source": [
    "### Comprendiendo las Bases de Datos Vectoriales en RAG Multimodal\n",
    "\n",
    "Las bases de datos vectoriales son el motor de búsqueda de nuestro sistema RAG. Estas:\n",
    "- Almacenan representaciones numéricas (embeddings) de nuestro contenido\n",
    "- Permiten búsquedas por similitud semántica\n",
    "- Mantienen todos los metadatos necesarios para el grounding visual\n",
    "- Soportan una recuperación rápida a escala\n",
    "\n",
    "Para contenido multimodal, esto significa:\n",
    "- Los fragmentos de texto se incrustan directamente\n",
    "- El markdown de las tablas se incrusta para búsquedas estructurales\n",
    "- Las descripciones de imágenes generadas por IA se incrustan para búsquedas visuales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_0oC5arQbHi"
   },
   "source": [
    "### Configuración de la Base de Datos Vectorial\n",
    "\n",
    "Usaremos Milvus, una base de datos vectorial de alto rendimiento. Milvus es una base de datos de código abierto diseñada para manejar grandes volúmenes de datos vectoriales, lo que la hace ideal para aplicaciones de IA y aprendizaje automático. Para más opciones de bases de datos vectoriales, consulta [esta receta de Almacenamiento Vectorial con Langchain](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQIOysr3Qgg5",
    "outputId": "0feefa9e-27df-446f-ae90-3e9f156eebd5"
   },
   "outputs": [],
   "source": [
    "# Create a temporary database file\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
    "print(f\"Vector database will be saved to: {db_file}\")\n",
    "\n",
    "# Initialize Milvus vector store\n",
    "vector_db: VectorStore = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,  # Allows flexible metadata storage\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},  # Automatic index optimization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy-12B1eQq_P"
   },
   "source": [
    "### Añadimos Documentos a la Base de Datos Vectorial\n",
    "\n",
    "Ahora añadiremos todos nuestros documentos procesados (fragmentos de texto, tablas y descripciones de imágenes) a la base de datos vectorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlQMqXOEQt2o",
    "outputId": "0302a03a-c95a-4e46-f1e5-9c9320a7abda"
   },
   "outputs": [],
   "source": [
    "print(\"\\nAñadiendo documentos a la base de datos vectorial...\")\n",
    "documents = list(itertools.chain(texts, tables, pictures))\n",
    "ids = vector_db.add_documents(documents)\n",
    "print(f\"Añadidos {len(ids)} documentos a la base de datos vectorial.\")\n",
    "print(f\"  - Fragmentos de texto: {len(texts)}\")\n",
    "print(f\"  - Tablas: {len(tables)}\")\n",
    "print(f\"  - Descripciones de imágenes: {len(pictures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xavz2IieQyqI"
   },
   "source": [
    "## Test de Recuperación con Atribución Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjuJtoCKfPAo"
   },
   "source": [
    "### Probando la Recuperación con Atribución Visual\n",
    "\n",
    "Antes de construir todo el pipeline de RAG, probemos que nuestra recuperación y atribución visual funcionen correctamente. Esto ayuda a verificar:\n",
    "- Se encuentra contenido basado en similitud semántica\n",
    "- Se preserva la metadata de atribución visual\n",
    "- Se manejan correctamente diferentes tipos de contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gLXwOObfSCR"
   },
   "source": [
    "### Basic Retrieval Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuc09LOtQ5IP",
    "outputId": "c8d08282-c57a-442c-c154-1469271947d3"
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"How much was spent on food distribution relative to the amount of food distributed?\"\n",
    "\n",
    "print(f\"\\nTesting retrieval for query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs = vector_db.as_retriever().invoke(test_query)\n",
    "\n",
    "# Display retrieved documents\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nRetrieved Document {i+1}:\")\n",
    "\n",
    "    # Determine content type\n",
    "    item_type = doc.metadata.get('item_type', 'text')\n",
    "    if item_type == 'picture':\n",
    "        content_type = \"AI-Generated Image Description\"\n",
    "    elif item_type == 'table':\n",
    "        content_type = \"Table (Markdown)\"\n",
    "    else:\n",
    "        content_type = \"Text Chunk\"\n",
    "\n",
    "    print(f\"Type: {content_type}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source'].split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iBcEjhVRBcG"
   },
   "source": [
    "## Construcción del Pipeline RAG Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma2N5kKTRHWO"
   },
   "source": [
    "\n",
    "Ahora implementaremos el sistema RAG multimodal completo que:\n",
    "1. Recupera contenido multimodal relevante\n",
    "2. Muestra exactamente de dónde proviene cada pieza\n",
    "3. Genera respuestas precisas y fundamentadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_60Mv9wgfmkm"
   },
   "source": [
    "### Diferenciando Tipos de Contenido\n",
    "\n",
    "Nuestro sistema maneja tres tipos de contenido de manera distinta:\n",
    "\n",
    "1. **Fragmentos de Texto**: El resaltado estándar muestra pasajes de texto\n",
    "2. **Tablas**: Bordes gruesos con etiquetas \"TABLE\" marcan datos estructurados\n",
    "3. **Imágenes**: Bordes distintivos con etiquetas \"IMAGE\" muestran ubicaciones de imágenes\n",
    "\n",
    "Esta diferenciación visual ayuda a los usuarios a comprender rápidamente qué tipo de contenido contribuyó a la respuesta de la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PRpHSBcJehQ"
   },
   "source": [
    "### Probando el Sistema RAG Multimodal con Visual Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmQdDvcXRD39"
   },
   "outputs": [],
   "source": [
    "# Bringing It All Together\n",
    "def rag_with_visual_grounding(question, vector_db, doc_store, model, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform RAG with visual grounding of results.\n",
    "\n",
    "    This function:\n",
    "    1. Retrieves relevant chunks from the vector database\n",
    "    2. Visualizes where each chunk comes from in the original document\n",
    "    3. Generates a response using the retrieved context\n",
    "\n",
    "    Args:\n",
    "        question: User's query\n",
    "        vector_db: Vector database with embedded documents\n",
    "        doc_store: Document store for visual grounding\n",
    "        model: Language model for response generation\n",
    "        tokenizer: Tokenizer for the language model\n",
    "        top_k: Number of chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (outputs, relevant_chunks)\n",
    "    \"\"\"\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(f\"\\nRecuperando los {top_k} fragmentos relevantes...\")\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    relevant_chunks = retriever.invoke(question)\n",
    "\n",
    "    print(f\"Se encontraron {len(relevant_chunks)} fragmentos relevantes\")\n",
    "\n",
    "    # Step 2: Visualize each chunk's source location\n",
    "    print(\"\\nVisualizando la ubicación de los fragmentos recuperados...\")\n",
    "\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"\\n--- Resultado {i+1} ---\")\n",
    "\n",
    "        # Determine content type\n",
    "        item_type = chunk.metadata.get('item_type', 'text')\n",
    "        if item_type == 'picture':\n",
    "            content_type = \"Descripción de Imagen Generada por IA\"\n",
    "            color = 'red'\n",
    "        elif item_type == 'table':\n",
    "            content_type = \"Tabla (Markdown)\"\n",
    "            color = 'green'\n",
    "        else:\n",
    "            content_type = \"Fragmento de Texto\"\n",
    "            color = 'blue'\n",
    "\n",
    "        print(f\"Content type: {content_type}\")\n",
    "        print(f\"Text preview: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Source: {chunk.metadata.get('source', 'Unknown').split('/')[-1]}\")\n",
    "\n",
    "        # Show visual grounding if available\n",
    "        if \"dl_meta\" in chunk.metadata or \"prov_data\" in chunk.metadata:\n",
    "            visualize_chunk_grounding(\n",
    "                chunk,\n",
    "                doc_store,\n",
    "                highlight_color=color\n",
    "            )\n",
    "        else:\n",
    "            print(\"  (No visual grounding available for this chunk)\")\n",
    "\n",
    "    # Step 3: Create RAG pipeline for response generation\n",
    "    print(\"\\nGenerando respuesta con LLM...\")\n",
    "\n",
    "    # Create Granite prompt template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer the question based on the provided context in a concise manner. Always answer in the same language as the question.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"{input}\",\n",
    "        }],\n",
    "        documents=[{\n",
    "            \"doc_id\": \"0\",\n",
    "            \"text\": \"{context}\",\n",
    "        }],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=escape_f_string(prompt, \"input\", \"context\")\n",
    "    )\n",
    "\n",
    "    # Document prompt template\n",
    "    document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
    "{page_content}\"\"\")\n",
    "\n",
    "    # Create chains\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt_template,\n",
    "        document_prompt=document_prompt_template,\n",
    "        document_separator=\"\",\n",
    "    )\n",
    "\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=combine_docs_chain,\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    outputs = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "    print(\"\\nRespuesta generada:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(outputs['answer'])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return outputs, relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQitCyY0RVPn"
   },
   "source": [
    "## Demostración Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d__u-q6RVnO"
   },
   "source": [
    "\n",
    "Vamos a ejecutar una consulta y ver el sistema completo en acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ud0Oy_K4Ra7u",
    "outputId": "f7f459a6-e44b-4878-c119-bc4f28d69d08"
   },
   "outputs": [],
   "source": [
    "main_query = \"Como funciona la pipeline de conversión de PDFs de Docling?\"\n",
    "outputs, chunks = rag_with_visual_grounding(\n",
    "    main_query,\n",
    "    vector_db,\n",
    "    doc_store,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dABGUdQ5RaX4"
   },
   "source": [
    "# Resumen y Próximos Pasos\n",
    "\n",
    "### Lo que Has Logrado\n",
    "\n",
    "¡Felicidades! Has construido con éxito un sistema avanzado de RAG multimodal. Esto es lo que has aprendido:\n",
    "\n",
    "1. **Implementación de un RAG con *Visual Grounding***\n",
    "   - Configuraste Docling para preservar referencias visuales\n",
    "   - Mantuviste metadatos de coordenadas a lo largo del pipeline de procesamiento\n",
    "   - Creaste atribución visual para todos los tipos de contenido\n",
    "\n",
    "2. **Procesamiento de Documentos Multimodal**\n",
    "   - Manejo fluido de texto, tablas e imágenes\n",
    "   - Uso de chunking inteligente para optimizar la recuperación\n",
    "   - Uso de modelos de visión IA para comprensión de imágenes\n",
    "\n",
    "3. **Arquitectura Transparente de RAG**\n",
    "   - Como establecer confianza en tu sistema RAG mediante verificación visual\n",
    "   - Habilitación de atribución de fuentes para cada respuesta\n",
    "   - Creación de un resaltado diferenciado por cada tipo de contenido\n",
    "\n",
    "4. **Integración Lista para Producción**\n",
    "   - Combinación efectiva de múltiples modelos de IA\n",
    "   - Creación de un pipeline escalable de procesamiento de documentos\n",
    "\n",
    "### ¿Por qué el Grounding Visual lo Cambia Todo?\n",
    "\n",
    "Los sistemas RAG tradicionales son \"cajas negras\": los usuarios deben confiar ciegamente en la IA. Tu sistema:\n",
    "\n",
    "- **Muestra las fuentes**: Cada afirmación puede ser verificada visualmente\n",
    "- **Construye confianza**: Los usuarios ven exactamente de dónde proviene la información\n",
    "- **Habilita auditorías**: Perfecto para industrias reguladas\n",
    "- **Reduce alucinaciones**: La verificación visual detecta errores\n",
    "\n",
    "### El Poder de la Comprensión Multimodal\n",
    "\n",
    "Al procesar texto, tablas e imágenes, tu sistema:\n",
    "\n",
    "- Captura información completa del documento\n",
    "- Habilita consultas y respuestas más ricas\n",
    "- Maneja la complejidad de documentos del mundo real\n",
    "- Proporciona respuestas completas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3zahmhOg0Jd"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phDxA2ivg0vn"
   },
   "source": [
    "## Proximos Pasos:\n",
    "\n",
    "\n",
    "1. **Experimenta con otros documentos**\n",
    "   - Prueba con documentos en español, francés o alemán\n",
    "   - Prueba documentos con diagramas técnicos y gráficos\n",
    "   - Procesa informes con contenido mixto\n",
    "\n",
    "2. **Personaliza la IA para ti**\n",
    "   - Utiliza los modelos de embeddings, visión y lenguaje que se adapten a tu flujo de trabajo común.\n",
    "   - Ajusta los prompts para mejorar la calidad de las descripciones de imágenes y respuestas del modelo de lenguaje.\n",
    "   \n",
    "   ```python\n",
    "   # Ejemplo: Prompts de imagen específicos a un dominio\n",
    "   medical_prompt = \"Describe esta imagen médica, señalando cualquier anomalía o característica clave\"\n",
    "   financial_prompt = \"Analiza este gráfico financiero, identificando tendencias y puntos de datos clave\"\n",
    "   ```\n",
    "\n",
    "3. **Optimiza el Rendimiento**\n",
    "   - Procesa documentos en batch: [Docling Batch Conversion](https://docling-project.github.io/docling/examples/batch_convert/)\n",
    "   - Usa aceleración por GPU con modelos de visión locales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVPQsnmFg-HB"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3UNKhQhI7Z"
   },
   "source": [
    "## Recursos Adicionales\n",
    "\n",
    "### Documentación Oficial\n",
    "- **[Documentación de Docling](https://github.com/docling-project/docling)**: Últimas características y actualizaciones\n",
    "- **[Modelos IBM Granite](https://www.ibm.com/granite/)**: Tarjetas de modelo y capacidades\n",
    "- **[Documentación de LangChain](https://python.langchain.com/)**: Patrones y mejores prácticas de RAG\n",
    "- **[Documentación de Milvus](https://milvus.io/docs)**: Optimización de bases de datos vectoriales\n",
    "\n",
    "### Recursos Comunitarios\n",
    "- Únete a la [comunidad de Docling en GitHub](https://github.com/docling-project/docling/discussions)\n",
    "- Comparte tus implementaciones\n",
    "- Contribuye con mejoras Docling! ❤️\n",
    "  \n",
    "### Temas Relacionados para Explorar\n",
    "- Análisis de Diseño de Documentos\n",
    "- Embeddings Multimodales\n",
    "- Respuesta a Preguntas Visuales\n",
    "- Sistemas de IA Explicables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytIk0-fghmMy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wADjxUDfhm5L"
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "Has completado un increíble viaje desde la conversión básica de documentos hasta la construcción de un sistema de IA sofisticado y transparente. La combinación de la comprensión de documentos de Docling, las capacidades de IA de Granite y el grounding visual crea una aplicación poderosa.\n",
    "\n",
    "Tu sistema RAG multimodal representa la vanguardia de la IA para documentos. Ya sea que estés construyendo para el sector de la salud, legal, financiero o cualquier otro dominio, ahora tienes las herramientas para crear sistemas de IA que no solo son poderosos, sino también confiables y transparentes."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
