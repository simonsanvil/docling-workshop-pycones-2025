{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U48hO1_V_JRG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Construyendo un sistema RAG multimodal con Docling\n",
    "\n",
    "*Usando IBM Granite vision, embeddings de texto y modelos de IA generativa*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYjGpqcgXR9W"
   },
   "source": [
    "## Lab 3: Del Papel al Conocimiento para IA Transparente - El Viaje Completo\n",
    "Bienvenido al laboratorio final de nuestra workshop de Docling! Has recorrido un largo camino:\n",
    "\n",
    "- **Lab 1**: Aprendiste a convertir documentos preservando la estructura\n",
    "- **Lab 2**: Dominaste estrategias inteligentes de chunking\n",
    "- **Lab 3**: Ahora, construiremos un sistema RAG completo y listo para producci칩n con una caracter칤stica revolucionaria: el *visual grounding*\n",
    "\n",
    "\n",
    "Este laboratorio representa la culminaci칩n de todo lo que has aprendido, mostrando c칩mo Docling permite no solo el procesamiento de documentos, sino sistemas de IA verdaderamente transparentes.\n",
    "\n",
    "## 쯇or qu칠 este laboratorio es importante?\n",
    "\n",
    "Los sistemas RAG tradicionales tienen un problema de confianza. Cuando una IA proporciona informaci칩n, los usuarios a menudo se preguntan:\n",
    "\n",
    "- \"쮻e d칩nde proviene esta informaci칩n?\" 游댌\n",
    "- \"쯇uedo verificar que esto es preciso?\" 九\n",
    "- \"쮼st치 la IA alucinando o utilizando datos reales?\" 游뱂\n",
    "\n",
    "**Visual Grounding** resuelve este problema mostrando a los usuarios exactamente de d칩nde se recuper칩 la informaci칩n en los documentos originales. Esto no es solo una caracter칤stica *agradable* de un sistema de IA, es esencial para casos de uso donde la precisi칩n y la verificabilidad son cruciales:\n",
    "\n",
    "- **Salud**: 游낀 Verificar fuentes de informaci칩n m칠dica\n",
    "- **Legal**: 丘뒲잺 Rastrear citas a ubicaciones exactas en documentos\n",
    "- **Financiero**: 游눯 Auditar ideas financieras generadas por IA\n",
    "- **Investigaci칩n**: 游댧 Validar afirmaciones cient칤ficas\n",
    "- **Empresarial**: 游끽 Construir sistemas de IA internos confiables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HIV_JkOXw9q"
   },
   "source": [
    "## 쯈u칠 hace especial a este lab?\n",
    "\n",
    "Estamos construyendo un sistema RAG multimodal con *visual grounding* que:\n",
    "\n",
    "1. **Procesa m칰ltiples tipos de datos**: Texto, tablas e im치genes de tus documentos\n",
    "2. **Muestra fuentes exactas**: Resalta la ubicaci칩n precisa de la informaci칩n recuperada\n",
    "3. **Comprende im치genes**: Utiliza modelos de visi칩n por IA para comprender el contenido visual\n",
    "4. **Mantiene la transparencia**: Cada respuesta puede ser verificada visualmente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAmR3BxvZ2Y0"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJl_fmofYBmH"
   },
   "source": [
    "## Entendiendo RAG Multimodal con Visual Grounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghh3uCNMCFwU"
   },
   "source": [
    "### 쯈u칠 es RAG Multimodal?\n",
    "\n",
    "[Retrieval Augmented Generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) es una t칠cnica utilizada con LLMs para conectar el modelo con una base de conocimiento externa sin necesidad de realizar [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning).\n",
    "\n",
    "Los sistemas RAG tradicionales est치n limitados a casos de uso basados en texto. Sin embargo, los documentos reales contienen:\n",
    "- **Texto**: P치rrafos, listas, encabezados\n",
    "- **Tablas**: Datos estructurados, informaci칩n financiera\n",
    "- **Im치genes**: Gr치ficos, diagramas, fotos, ilustraciones\n",
    "\n",
    "El RAG Multimodal puede utilizar [LLMs multimodales](https://www.ibm.com/think/topics/multimodal-ai) (MLLM) para procesar informaci칩n de m칰ltiples tipos de datos que se incluyen como parte de la base de conocimiento externa utilizada en RAG. Los datos multimodales pueden incluir texto, im치genes, audio, video u otras formas.\n",
    "\n",
    "\n",
    "Puedes leer m치s sobre RAG Multimodal en este [art칤culo de IBM](https://www.ibm.com/think/topics/multimodal-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpxgNPDCZEDW"
   },
   "source": [
    "### Visual Grounding: La Capa de Transparencia\n",
    "\n",
    "El grounding agrega una capa crucial de transparencia a los sistemas RAG. Cuando el sistema recupera informaci칩n para responder a una consulta, no solo devuelve texto, sino que tambi칠n muestra exactamente de d칩nde proviene esa informaci칩n en el documento original mediante:\n",
    "\n",
    "- Dibujar cuadros delimitadores en las p치ginas del documento\n",
    "- Resaltar regiones espec칤ficas\n",
    "- Etiquetar tipos de contenido (TEXTO, TABLA, IMAGEN)\n",
    "- Usar diferentes colores para m칰ltiples fuentes\n",
    "\n",
    "En este notebook, utilizar치s los modelos IBM Granite, capaces de procesar diferentes modalidades, mejorados con las capacidades de grounding visual de Docling para crear un sistema de IA transparente y verificable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_S93MsAZz1S"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eDALN1A9LF8"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "En este laboratorio, aprender치s a:\n",
    "\n",
    "1. **Configurar Docling para el Grounding Visual**: Configurar el procesamiento de documentos para mantener referencias visuales\n",
    "2. **Procesar Contenido Multimodal**: Manejar texto, tablas e im치genes con los metadatos adecuados\n",
    "3. **Aprovechar los Modelos de Visi칩n AI**: Utilizar los modelos de visi칩n IBM Granite para entendimiento de im치genes\n",
    "4. **Construir una Base de Datos Vectorial**: Almacenar embeddings con metadatos para visual grounding\n",
    "5. **Implementar Atribuci칩n Visual**: Mostrar a los usuarios exactamente de d칩nde proviene la informaci칩n\n",
    "6. **Crear un Pipeline RAG Completo**: Combinar todos los componentes en un sistema listo para producci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSIhob4jZcAN"
   },
   "source": [
    "### Tecnolog칤as que usaremos\n",
    "\n",
    "Componentes clave:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** Un kit de herramientas de c칩digo abierto utilizado para analizar y convertir documentos.\n",
    "2. **[LangChain](https://langchain.com)**: Para orquestar el pipeline RAG\n",
    "3. **[IBM Granite Vision Models](https://www.ibm.com/granite/)**: Para entendimiento de contenido de im치genes\n",
    "4. **Visual Grounding**: La capacidad 칰nica de Docling para atribuci칩n de fuentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQlsFgvGZ9hH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vooxv7ltEZBf"
   },
   "source": [
    "## Prerequisitos\n",
    "\n",
    "Antes de comenzar, aseg칰rate de tener:\n",
    "- Completados los Laboratorios 1 y 2 (o conocimiento equivalente de Docling)\n",
    "- Python 3.10, 3.11 o 3.12 instalado\n",
    "- Comprensi칩n b치sica de embeddings y bases de datos vectoriales\n",
    "- Familiaridad con los conceptos de los laboratorios anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoM938B_JRH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci칩n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BfMWUUSs_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ae0cf79a-d586-4836-dbdc-97d18b217a16",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    'langchain_huggingface[full]' \\\n",
    "    langchain_milvus 'pymilvus[milvus_lite]'\\\n",
    "    docling \\\n",
    "    replicate \\\n",
    "    matplotlib\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gu-Oeay_JRJ"
   },
   "source": [
    "## Importar las librer칤as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G-ke7lF4CFwW"
   },
   "outputs": [],
   "source": [
    "# To see detailed information about the document processing and visual grounding operations, we'll configure INFO log level.\n",
    "# NOTE: It is okay to skip running this cell if you prefer less verbose output.\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rJwoqaBPHySg"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocling_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlabels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocItemLabel\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# LangChain imports for RAG pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Replicate\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_huggingface'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "import io\n",
    "import itertools\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "from PIL import ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "# Docling imports for document processing and visual grounding\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "from docling.chunking import DocMeta\n",
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "from docling_core.types.doc.document import TableItem, RefItem\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "\n",
    "# LangChain imports for RAG pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Model imports\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from ibm_granite_community.notebook_utils import get_env_var, escape_f_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yodQeEWVa3Xa"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyixsnLzCFwW"
   },
   "source": [
    "### Selecci칩n de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB_Hry0ebAtE"
   },
   "source": [
    "### Los Tres Pilares de RAG Multimodal\n",
    "\n",
    "Para un sistema completo de RAG multimodal con visual grounding, necesitamos tres tipos de modelos, cada uno cumpliendo un prop칩sito crucial:\n",
    "\n",
    "1. **Modelo de Embeddings**: Convierte texto en representaciones vectoriales\n",
    "   - Permite la b칰squeda sem치ntica (\"encontrar contenido similar en significado\")\n",
    "   - Debe manejar texto de fragmentos, tablas y descripciones de im치genes\n",
    "\n",
    "2. **Modelo de Visi칩n**: Entiende y describe contenido visual\n",
    "   - Procesa im치genes encontradas en documentos\n",
    "   - Genera descripciones textuales para su recuperaci칩n\n",
    "\n",
    "3. **Modelo de Lenguaje**: Genera respuestas finales\n",
    "   - Sintetiza la informaci칩n recuperada\n",
    "   - Produce respuestas coherentes y precisas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEIpVe6yIAN6"
   },
   "source": [
    "### Modelo de Embeddings\n",
    "\n",
    "Usaremos un [modelo de embeddings Granite](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb) para generar vectores de embeddings de texto.\n",
    "\n",
    "\n",
    "- Optimizado para texto en m칰ltiples idiomas\n",
    "- Compacto (107 millones de par치metros) para un procesamiento r치pido\n",
    "- Excelente comprensi칩n sem치ntica\n",
    "- Ventana de contexto de 512 tokens\n",
    "\n",
    "Si deseas usar un modelo diferente, puedes consultar [esta receta de modelos de embeddings de la comunidad de IBM Granite](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "30aed391581c47598facbe74763a71f3",
      "bb9f7ac4cfce4dc1aa2cd4f9456c4a05",
      "a8214a9c57ca467faa6b32127332d1b7",
      "7ebbf8143a604b409e558131b83706a9",
      "bd7b3990a2e5414a8bfdd5d58aa4c6c4",
      "1d17691f611147058a744cf1b6d92ec5",
      "005e089874c14d16a72d263559c6f11e",
      "f519c63bf62b471eb4138a54774b25b2",
      "46444911d17e43e28758b79d6fd1698c",
      "105cf6ef3ba8456c8666a158d0f3075a",
      "ded96aceadd4480b89374a99d67f56e2",
      "ffa777f2454743a6ab66c35bbd1ac9dd",
      "146abad9d66d4c2ca8f2f31ffe4bee64",
      "1b9419b296104289a2f68d3bc2615ea2",
      "f87a0f86f8ed41579edb87cb2936b980",
      "ff1e65972bee45eeafb4070ca8064517",
      "b9491b7c2a83410097591f434df30df1",
      "268a66e6bdd94deaad362f7e6e860f87",
      "0511ef3a989a4cd49fa4909918e53cf9",
      "7ad3eeeb3980431a91f55b3d7bd97529",
      "c768a35348e14b2cad0d9bc6ed047f41",
      "3109510d96ca4385aca0a3d6b0b99e65",
      "8d94a499aa02419da19ff30e9a4ccc8b",
      "ff2b9ac983e54316858ea7f870d041a9",
      "809e4f104bd04244a5bc2c1c65790497",
      "de429ecb3f7c44978cf6bd9085259933",
      "ea4efa50585b49e5a3242c64fb83a241",
      "dc81b5fd026544ba975f4baa890c9ed2",
      "1078c105925a437b93f26a8f04662b88",
      "c82003732c1146d0b5b59c757f1859b8",
      "78e5987e84d14efab9619dd6de9da651",
      "45bdc919748f41ec946a78453533b9f1",
      "63db65a7e1ba44a49953e564475774e1",
      "0d13347b9e4848d5998b2419ed5765b4",
      "83cb2acd36634bd8ab2e323b54cc8f43",
      "5530b12b7d7146b7be524bf542509765",
      "bef293aaada14720b396c2b2b464e5fe",
      "be3cd11187f04f2ba79a29d6a93ad80b",
      "701317fcc9e047678eca988cda1f0b19",
      "67e6bc45803c4b069c1a054fe1ee28f0",
      "f28fc775dcd34c66bf48c41f829dde38",
      "c1bfcd0e354c4805a13d0b1fc111ee45",
      "6399b41f5b9a4be8813893cd3224b1ce",
      "69570c2887f04f3f8d67bbd975eaa79e",
      "73c46fdee29e46068167061884060eb5",
      "8e523d385eb743899c1086245f25f72a",
      "c699c8d54bf845ddb0c4519a135dd862",
      "fb534df5f7b34766b8a8e783c2008466",
      "09563f36f1c8401c98acdb6176deb521",
      "18bdb8d9c93a4251842c161ce7c15926",
      "e22cbf4f9bcb4370abfebeb52272d15c",
      "42057f222a1e462fb146991dae15b5fd",
      "7e8a2b4d2d914a118ccfd66c460ce89c",
      "bb496aa21cb841e5a368f12a7ef51209",
      "f0b3502132724ec8a6c6345855b6a74d",
      "c62bedacd48d4c70a1db6896bd3ead1b",
      "6108fd6deb8c448fa37918088c9ac44a",
      "d388ea4db4d14dbaad7fb7f2af893fdf",
      "e5c720ed8ff44bf995dd22cc346364ca",
      "49689529e77541fea13c6ae5b9bfb63b",
      "ad8827ca671d4732a55a62bedd6533de",
      "312ad4ea71454b55baab0c6552a4dac8",
      "77ae0c3def7348d987d91a3aef9f8d12",
      "2d856987a7994377b4482d4229681a5c",
      "35f8a006f3c84b168282a927f065afc8",
      "c66a1538bb0b44c6a617ba57189ef7b9",
      "99f8b9c321f940d885df7cfa57d104ae",
      "65278a69c9a24cddbbd1b20ab08abaeb",
      "fe3acc6c2c19435891201245f1bf078c",
      "65595dd5223c457e863e9ff049f6f90b",
      "f34b67eba2ca41e4a3a8c08e6649256c",
      "17361220c0594b9ca30f6500543db6e8",
      "36a09ad6dd31454d8b8185a7a67e36fc",
      "c5e095ef30f7444dbf98a3356c676234",
      "a9116fd44acf4d3290d43f7643d372d1",
      "72814321ea6440e1a6fe2a5f83236376",
      "e3e72eb246594d3497e175ad017e6267",
      "17b72d7de14b48c296350ed182da1abd",
      "aabfc86e48924653b7b816f3653d2ca6",
      "60ce3f06c5144ba3b8f13cb9813ad8c6",
      "91e10ba5e5dd45c1a6e869829f657264",
      "ea0ca547ee1b4d7895a16f525eae20d5",
      "c029c57f8d2a4f89bbe759281c3ad16f",
      "2d870e4a1d3146e49c71c07306b6a10d",
      "0256b42e2d9e43658a52d9bb767fec3c",
      "e05013c54d594ce28aa98b2ac2d72dd2",
      "1ba73dfd141f43aa9ba337192ad39667",
      "7049612009674999be46373e34af2245",
      "927b891f7b7f4061b0ba2ba71de64dc7",
      "0f963da83c9b4469afe820915871fc62",
      "71e6963c6e6d4753bef3e6f036061558",
      "50bd57c1bac040cb860f2aea40e2f64e",
      "e0b9bfb59c8044f19087cef6b08f3d15",
      "3381fa31190349e6bb996e1e333704f0",
      "5ff928c8b9694b2997eb492519cb73ef",
      "488135d46a0c4ac2af0feba79f1e2b12",
      "9d5ca3199c2940e486c8e691abcfe370",
      "50da0c70814f4623a599891fb022caa5",
      "95c84535d1fc4223ae24694e2d8f932d",
      "873b3db7c970448b887deab25c48d37f",
      "08e964cf2358451e8b07678709e98ef2",
      "35fca115556c425aa1f205294cd302b4",
      "16c4dbd4c35a4c78b5254a245028c710",
      "0cda12f4cb304c68ab52be19030ae4bd",
      "81619090425b434bbee76003feec3baa",
      "bd21dad115cf4e77937a7f2701cbcae5",
      "72b986829a23454f9d0381e23e731e6d",
      "9d36c4e7a66447c1bd93028168494b21",
      "197ce4c0d60f43d786064011d0655d63",
      "01eb40035b6544ddb790da91c4c49f5b",
      "a4654dce525f428ea1826f551f6ecf96",
      "1e544112cf7d44db8c3c0538be77bf1b",
      "38764c10d2af43198d15d50241405bf6",
      "20f39553605149899141309285c4d3ea",
      "0116ae1b4a724e4eb3a2d890f6e2d916",
      "dd1ec695d2be45ce957691f086b90c10",
      "fe183c499c1e414ca6c1c41347a74594",
      "19ca17543ff443e19f043478cd82657a",
      "d8723555c1ce41a28a2dcfc24b8ed26d",
      "02d6cc2e89d54f699475cb9f070900a7",
      "12caabb1fa3349afa1b5ba246484534c"
     ]
    },
    "id": "mvztNZly_JRJ",
    "outputId": "b65d35cf-4549-4e77-87f9-46ba5c3f6138"
   },
   "outputs": [],
   "source": [
    "embeddings_model_path = \"ibm-granite/granite-embedding-107m-multilingual\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)\n",
    "print(f\"Embeddings model loaded: {embeddings_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9nTm3qICFwW"
   },
   "source": [
    "### Cargar el Modelo de Visi칩n Granite\n",
    "\n",
    "El modelo de visi칩n nos ayudar치 a comprender las im치genes dentro de los documentos. Esto es crucial para un RAG verdaderamente multimodal, ya que muchos documentos contienen informaci칩n visual importante.\n",
    "\n",
    "**쯇or qu칠 usar un modelo alojado en la nube en lugar de uno local?**\n",
    "- Procesamiento m치s r치pido sin necesidad de GPU\n",
    "- Rendimiento consistente\n",
    "- F치cil de escalar\n",
    "\n",
    "**Nota**: Para configurar Replicate, consulta [Introducci칩n a Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "Para conectarte a un modelo en un proveedor diferente a Replicate, consulta la [receta de LLMs de Langchain de la comunidad de IBM Granite](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377,
     "referenced_widgets": [
      "a534e43df09f4bd184b5ec03735ba106",
      "35df11e94a084fca84a79ff238dd41dc",
      "fcc03f7726b0475b8c692f8102f300db",
      "e38eb7bd68c443b49597a42bf99fddb6",
      "9f0f75412d184759a4c866ce0c57ebca",
      "8fe1a9a63b1f48878b3c46ee8aadcc79",
      "fc671127aeab4397b217363b696a8d32",
      "1ac06c72a57d450f959e4f676f3eb953",
      "eda16fcae5774b6ab97db390f2cf6212",
      "0358db24b603458c94bfbaa265bcd699",
      "8ea2420779ae49938724db859bdfc218",
      "f15ae056261348698338ef17784b267c",
      "c89e1c71be9a4f9d9c520a9e655de69d",
      "021d9eca558d4c52a7d5719fa595019a",
      "b44231c04c5a42808cc49bfef5cb5761",
      "afc6940a831a4712a1c6ec3bdcdeed7c",
      "6a2ef5dbc966469c9e68bde49fa7b912",
      "03d324aaa6734a058bd2ae8ce3fcc198",
      "85874d0df7ba4223b2fc49a150ccf235",
      "018db3282d104bfdbad8e79d568e6452",
      "1656c7ac85ab48949681a70f34706ec4",
      "18e60b6f5e1c415dbf1726d5678ff69a",
      "d7776f5095b1447a87af1a5ff743ffea",
      "f58d57126aca42d7ac30eb5f0d6c5862",
      "c6f57eb2d7144b9382cbe71e65bd78c9",
      "f2c5e47cd6194c7bb48228ea6abdece2",
      "033c76f5356f4e7290d0f7c9f5c7e976",
      "ba4b9c4a029c445a9c2d7bc2f4b94c2f",
      "279c4bf80ed34918a39c9eed77e20448",
      "e848ffc816fd4ac0ae220a575d816257",
      "5535857e2f964b80947edb78e2af619c",
      "7784639f83124f9e8891390744b5f7e8",
      "ccc8df11a0a748d6ae1098f718bf909a",
      "936fbdcc7f8f4d7e9f4c60a91f1bce82",
      "c0863ca49d4343dda2f1e522305ae51c",
      "22793dec401e416ab7e4a1fff8731353",
      "14005f1853094e009bc0792807b56132",
      "4618c66a4e964ea2b6ebbb0b2f620abf",
      "91e6cb46a47f429fa124ee4830905f20",
      "eba20cb1f3094c56a743398a488117a9",
      "3d97db037b7e445d854bcceac1478fea",
      "df5f7eadbaa544a9a90a8325440559ba",
      "5378f121bb09445d83fdaf58d7260b28",
      "7affa0c2c9694c5f8e117ce1820ff07d",
      "8b051518b298442ea0e50ae4de00b607",
      "99776d2b3526490da7dd7f91aace918d",
      "3f97c371ec024e308e4e4bf9deac89a1",
      "c11039a4c7644d51bf815a8c6a147b62",
      "011f08213068401dabee1bef4f21cd30",
      "23589f5c9174456e90a91ad04ab3190b",
      "9980cf7d59434126b5aa29c724b924cc",
      "616ddd6f314d47c3941762d2423e53a9",
      "d6c46e36441a4b8c8cedae9379528e49",
      "033085b9dc7b4a229f10560ec13ed5e4",
      "197584bb2ee748ffae5a12d773c74c59",
      "4158830094b849bda48c0f3383762906",
      "5ce4d6659b09413c9d1c023c6c02d0a0",
      "c5e2a132b2c54a1aaef37d05b6c02d24",
      "a512326a7ce64f51b7acccde449cf211",
      "e5d42ce5909f4bad8988074b3f7e095b",
      "df483317ae1a491d8a6823fd0c542371",
      "d86b3c886e094fecbebac6d06a60c83e",
      "689f0d3ec2fe4a2cb4a906c4a8727bb3",
      "cb9eb27155bc45b3a7581d41a8fbc924",
      "06610e0f196146a9bf3f6d5bd542b3c4",
      "c704fb332dcd4f558684ff8dbbcaab5d",
      "9f299a085bdc49cf9928a8b727bf745f",
      "bfe7ee9615354df2b55844ac87e53347",
      "d7b4dfe4d3f34302986549791633a5a4",
      "9fc1888a5f054a7ea1773f343519234b",
      "328a38ed6f774e229cf94220b5e87d26",
      "581df90d93a24b3c85f73147e84a04db",
      "7c0dc1f65d8f477ebc9984e9014bbc7f",
      "00068a3f007e48bcabe426b077055012",
      "b6bb194f067d4e8bb3eb74ee34c617e5",
      "f36158e5ffe8438a98207c333476e885",
      "c89195399f53453caa59964dfc8fb311",
      "20723c36ae014a79aae19124387ce7f4",
      "208333b9ad43414db26be168e88a9764",
      "067dfbebbd6b493ebe8e56cc68efe016",
      "db953222e9514659bc18e4e9b15d756b",
      "7fb731dd508d47f5a7abb5eb16e790d7",
      "58ee2cc2bd084374a294e8d3edc0833e",
      "df2261a91c354e4aa3133b1eb19e48d9",
      "80b2b32f78314bc89d549242a1f24a26",
      "aa8b3d7be6604fc58fbb55e2016a4f24",
      "896a033ae3894e8990cb626883ad7459",
      "3033414c0baf4d03857520b79de534fc",
      "221c2936753f4954b81fa29d167303c9",
      "ea3a35bc32524806957cc6ef391f035e",
      "b6cde6b309bf4c79afa1e7250e737ed7",
      "79705b356a834632aa5324b66daeef1d",
      "e50b987dc41c4231aa8a5c9219fc49d5",
      "6cacca41b9024c329d2bcba9e821d012",
      "b14dd3351e7f4465a057e2038ff1108d",
      "ea809b34f1d545fea87b12d16a991991",
      "c21796d6756c4247bac05db4453d80eb",
      "6a477ba04063486d87f937d97dac615e",
      "80ec01e074b74fc5829a4be11cc64887"
     ]
    },
    "id": "-5272bCOCFwW",
    "outputId": "d6ad7a1f-2988-4acc-8622-710a694d5702"
   },
   "outputs": [],
   "source": [
    "vision_model_path = \"ibm-granite/granite-vision-3.3-2b\"\n",
    "vision_model = Replicate(\n",
    "    model=vision_model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence,\n",
    "        \"min_tokens\": 100,\n",
    "        \"temperature\": 0.01 # low temperature for reproduceability\n",
    "    },\n",
    ")\n",
    "vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n",
    "print(f\"Vision model loaded: {vision_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma8eWR10_JRJ"
   },
   "source": [
    "### Cargar el Modelo de Lenguaje Granite\n",
    "\n",
    "Finalmente, nuestro modelo de lenguaje generar치 respuestas basadas en el contexto recuperado.\n",
    "\n",
    "**쯇or qu칠 usar este modelo?**:\n",
    "- 8B par치metros: Buen equilibrio entre calidad y velocidad\n",
    "- Ajustado a instrucciones: Sigue nuestros prompts con precisi칩n\n",
    "- Familia Granite: C칩digo abierto y con licencia Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "44c6e3c381944af9ba6850e66e066f1f",
      "10f070266fff46509ca258b61dae89e5",
      "f0e36bc66e794361aee399c9c4f2d683",
      "d03e6698b9d74f00af51217c993fa765",
      "a5bbf421325644fdbc6edf12f8a034d9",
      "64f672136d8c4d4abc184e6fa973b61e",
      "0c838d83f14b4f37bbcf83e7f5401049",
      "8f32c79df5d942129f3bdbc3342d8a05",
      "1eaa73c964af482b8a6852ddcdbc4830",
      "f67521ed26cd46e982675813d0fa5167",
      "b12c8c3c552e451693c1063a298e7a6d",
      "99b1488e5d094dba9e72c7bc2ca21427",
      "cbb843c832fd4527a107089ca5f2fad3",
      "07ead5501f5b4c358a2b0baa34b08490",
      "2c5b43a5f60544fc9b6f5b5e7114b1b8",
      "40a81bb7bd1e475885afbd2e09ef1fc6",
      "85a41be27c604244b35c582e32be1c30",
      "0749c65dab7e4a46b2f3dce6ccde0987",
      "06eca4f2a8cc4bc6a22f190a68ca53e5",
      "d8b0ed5af0a24cb1ae9d530de10b91f5",
      "e31a3cf13e1b46d9a304cfff8ba8cc4d",
      "9b6f8669ac5a459f84ec622b831e6b94",
      "3fe5bb2331e34a62a54d07dc6ccb181b",
      "7ee6bf328f9c41779e0ec04a11128d23",
      "f52d124f4eaf47209d9736876b079301",
      "f8a832d2353844efb5605d9c833a38b5",
      "bf4655af6bc54811b9c190a796088651",
      "6560e20b62374bddac5c2ba17c463b34",
      "7fa8f0281e57466691155c9372286419",
      "6d1220067e184e0f9637f10962ea8aff",
      "46fa9c2dbdc44d8ba27e551ff6d84b65",
      "df8b0f131e9a44e2a925d34ca2ccecb6",
      "e7c88f3fb527430caaf311309de069a5",
      "76c3c4d8293d4953876edff0f1a75898",
      "657e53b228354a5695f73b5a7f6e2335",
      "e00786348c644dcdaf5026296145e703",
      "2d2f6e60a1c8457593943784a91c20fa",
      "dc6d9d6bb5d14358b3c1d15f93d6d398",
      "7fc30294363d4e9d8f1d38297996f17d",
      "ba50a4b2fefd4b069a8f02f169d4b753",
      "515b69ef7ef640c1b2d0c078c4075c0d",
      "543b1fa684d24d57b5756816a1948dd6",
      "6c3f9316cc424841a7bf2d8a9e70df70",
      "40062877163d4ec38610bc4cbf0f6fcb",
      "8efb1ca61b53466abedc92f95ae16633",
      "d87cb3db4f1749bc8f749d2bf7fc7d45",
      "2a233805f4f2437595b06c70a4d702b1",
      "c3f51829de3b448291e0cc66cff2312b",
      "218a3779ad18493eb1d23a06c6774388",
      "b69c6b35e51d496891dc2af739ee6526",
      "198a035bd5e94ab7a7b8768ce5c85e0a",
      "5bccd67539f94be19746bc2777eac659",
      "82796002bad64fada9314d34bcf4fecd",
      "c0c081a48abe4410b16e60173eaa4326",
      "51cb65a268a74e2e9e8b5e357c3b022f",
      "65d629651352431384420d3606289847",
      "cce0890083934592b0f60e04fb9622ac",
      "70690a4626714ab397e249ad2cfe286e",
      "3351e52693674feaac570cf92c83bf55",
      "a7c612ec254d4de59ea7a85022610b9a",
      "3e1b1638c3f64d76a2b6990a7474385c",
      "ddb41d7e7adf4d97906ec238bd63edc6",
      "69257ae2351c4565a97320b554fc3c16",
      "4939785c8a514734ba57f363860fd587",
      "b2be447bf8844e4eaba87d100f1af07b",
      "ac210c1417ac4f16bf4ec22a111bd6cc"
     ]
    },
    "id": "Ckyj7Zrh_JRK",
    "outputId": "41c24db5-5e33-4afb-e358-fdb2efe3ff38"
   },
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"min_tokens\": 100,\n",
    "        \"temperature\": 0.01 # low temperature for reproduceability\n",
    "\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(f\"Language model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVfNx7tCcB8l"
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviHG3n7_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Procesamiento de documentos con soporte para visual grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0TSvvRFJTDJ"
   },
   "source": [
    "El visual grounding requiere una configuraci칩n especial durante la conversi칩n de documentos. A diferencia del procesamiento est치ndar, necesitamos:\n",
    "\n",
    "1. **Generar im치genes de p치gina de alta calidad**: Para resaltar elementos de la p치gina visualmente.\n",
    "2. **Preservar la informaci칩n de coordenadas**: Para saber d칩nde se encuentra el contenido.\n",
    "3. **Mantener la estructura del documento**: Para una atribuci칩n de fuentes precisa.\n",
    "4. **Almacenar los documentos adecuadamente**: Para su posterior recuperaci칩n y visualizaci칩n.\n",
    "\n",
    "Vamos a configurar Docling con estos requisitos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3jGNMkKJZ-R",
    "outputId": "b85c1c47-5206-4701-fd54-20212154f568"
   },
   "outputs": [],
   "source": [
    "# Configure the document converter to support visual grounding\n",
    "pdf_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=False,  # Set to True if your PDFs contain scanned images\n",
    "    generate_picture_images=True,  # Extract images from documents\n",
    "    generate_page_images=True,  # CRITICAL: Generate page images for visual grounding\n",
    ")\n",
    "\n",
    "format_options = {\n",
    "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
    "}\n",
    "\n",
    "converter = DocumentConverter(format_options=format_options)\n",
    "print(\"Document converter configured with visual grounding support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ7Guu7A_JRK"
   },
   "source": [
    "### Creamos un almac칠n local de documentos para el visual grounding\n",
    "\n",
    "Almacenaremos documentos que mantendr치n la estructura completa del documento necesaria para el visual grounding. Esto es esencial para resaltar las coordenadas de origen m치s adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNGz_0gZ_JRK",
    "outputId": "f3585ca6-1668-49ae-c7cf-252301e98d94"
   },
   "outputs": [],
   "source": [
    "# Create document store for visual grounding\n",
    "doc_store = {}\n",
    "doc_store_root = Path(mkdtemp()) # Temporary directory for document store\n",
    "print(f\"Document store created at: {doc_store_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSLYgc7JCFwX"
   },
   "source": [
    "### Convertir documentos con seguimiento visual\n",
    "\n",
    "Ahora procesaremos documentos mientras preservamos toda la informaci칩n necesaria para el visual grounding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLDMCxFbCFwX",
    "outputId": "d2a325a8-04ef-4cf2-94ac-c8c6a9f372e0"
   },
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"https://arxiv.org/pdf/2501.17887\" # Docling paper\n",
    "    # \"https://arxiv.org/pdf/2206.01062\", # DocLayNet paper\n",
    "    # \"https://arxiv.org/pdf/2311.18481\", # DocQA\n",
    "    # A침ade m치s documentos seg칰n sea necesario\n",
    "]\n",
    "\n",
    "conversions = {}\n",
    "\n",
    "print(\"Iniciando la conversi칩n de documentos con visual grounding...\")\n",
    "for source in sources:\n",
    "    # Por cada fuente, convertimos el documento preservando las im치genes y guard치ndolas en nuestro almac칠n local de documentos\n",
    "    print(f\"\\n Procesando: {source}\")\n",
    "\n",
    "    # Convert document\n",
    "    result = converter.convert(source=source)\n",
    "    docling_document = result.document\n",
    "    conversions[source] = docling_document\n",
    "\n",
    "    # Save document to store for visual grounding\n",
    "    # The binary hash ensures unique identification\n",
    "    file_path = doc_store_root / f\"{docling_document.origin.binary_hash}.json\"\n",
    "    docling_document.save_as_json(file_path)\n",
    "    doc_store[docling_document.origin.binary_hash] = file_path\n",
    "\n",
    "    print(\"Documento convertido y guardado en el almac칠n local.\")\n",
    "    print(f\"  - Document ID: {docling_document.origin.binary_hash}\")\n",
    "    print(f\"  - Paginas: {len(docling_document.pages)}\")\n",
    "    print(f\"  - Tablas: {len(docling_document.tables)}\")\n",
    "    print(f\"  - Im치genes: {len(docling_document.pictures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vX3XqSOKBos"
   },
   "source": [
    "## Procesado del Contenido del Documento con Metadatos de Atribuci칩n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dvccMBeKC7c"
   },
   "source": [
    "### La Importancia de los Metadatos para el Visual Grounding\n",
    "\n",
    "Para que el visual grounding funcione, cada fragmento de contenido debe mantener metadatos sobre su ubicaci칩n de origen. Esto incluye:\n",
    "- **N칰meros de p치gina**: Qu칠 p치gina(s) contienen este contenido\n",
    "- **Cajas delimitadoras**: Coordenadas exactas en la p치gina\n",
    "- **Referencias de documentos**: Enlaces de regreso al documento fuente\n",
    "- **Tipo de contenido**: Si es texto, tabla o imagen\n",
    "\n",
    "Este metadato es lo que nos permite resaltar regiones de inter칠s espec칤ficas en las p치ginas del documento m치s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdT7gzIeKFyd"
   },
   "source": [
    "## Procesamos los Chunks de Texto con metadatos de ubicaci칩n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zgnHW4UCFwX"
   },
   "source": [
    "Ahora procesamos cualquier tabla en los documentos. Convertimos los datos de la tabla al formato markdown para pasarlos al modelo de lenguaje. Se crea una lista de documentos LangChain a partir de las representaciones en markdown de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDa6yik4CFwX",
    "outputId": "a8ff50a6-20d6-4a0f-c198-a7a6e4dc9f46"
   },
   "outputs": [],
   "source": [
    "from docling.chunking import DocMeta\n",
    "\n",
    "doc_id = 0\n",
    "texts: list[Document] = []\n",
    "\n",
    "print(\"\\nProcessing text chunks with visual grounding metadata...\")\n",
    "for source, docling_document in conversions.items():\n",
    "    chunker = HybridChunker(tokenizer=embeddings_tokenizer)\n",
    "\n",
    "    for chunk in chunker.chunk(docling_document):\n",
    "        items = chunk.meta.doc_items\n",
    "\n",
    "        # Skip single-item chunks that are tables (we'll process them separately)\n",
    "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
    "            continue\n",
    "\n",
    "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
    "        text = chunk.text\n",
    "\n",
    "        # Create document with enhanced metadata for visual grounding\n",
    "        document = Document( # langchain_core.documents.Document\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"doc_id\": (doc_id:=doc_id+1),\n",
    "                \"source\": source,\n",
    "                \"ref\": refs,  # References for tracking specific document items\n",
    "                \"dl_meta\": chunk.meta.model_dump(),  # CRITICAL: Store chunk metadata for visual grounding\n",
    "                \"origin_hash\": docling_document.origin.binary_hash  # Link to stored document\n",
    "            },\n",
    "        )\n",
    "        texts.append(document)\n",
    "\n",
    "print(f\"Created {len(texts)} text chunks with visual grounding metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzoFPRBjCFwX"
   },
   "source": [
    "### Procesamos las tablas con informaci칩n espacial\n",
    "\n",
    "Las tablas requieren un manejo especial para preservar su estructura y ubicaci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaiUN8nVCFwX",
    "outputId": "bb25ef96-c755-4884-89bb-093e47524dd1"
   },
   "outputs": [],
   "source": [
    "doc_id = len(texts)\n",
    "tables: list[Document] = []\n",
    "\n",
    "print(\"\\nProcessing tables...\")\n",
    "for source, docling_document in conversions.items():\n",
    "    for table in docling_document.tables:\n",
    "        if table.label in [DocItemLabel.TABLE]:\n",
    "            ref = table.get_ref().cref\n",
    "            text = table.export_to_markdown(docling_document)\n",
    "\n",
    "            # Extract provenance information for visual grounding\n",
    "            prov_data = []\n",
    "            if hasattr(table, 'prov') and table.prov:\n",
    "                for prov in table.prov:\n",
    "                    # Get the page to access its height for coordinate conversion\n",
    "                    if prov.page_no < len(docling_document.pages):\n",
    "                        page = docling_document.pages[prov.page_no]\n",
    "                        # Convert to top-left origin and normalize\n",
    "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                        bbox_norm = bbox.normalized(page.size)\n",
    "\n",
    "                        prov_data.append({\n",
    "                           \"page_no\": prov.page_no,\n",
    "                           \"bbox\": {\n",
    "                              \"l\": bbox_norm.l,  # Use normalized coordinates\n",
    "                              \"t\": bbox_norm.t,\n",
    "                              \"r\": bbox_norm.r,\n",
    "                              \"b\": bbox_norm.b\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
    "                    \"item_type\": \"table\",  # Mark as table\n",
    "                    \"prov_data\": prov_data  # Store provenance as simple data\n",
    "                },\n",
    "            )\n",
    "            tables.append(document)\n",
    "\n",
    "print(f\"Created {len(tables)} table documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWHbpeEuCFwX"
   },
   "source": [
    "### Procesamos las im치genes con entendimiento visual\n",
    "\n",
    "Para un verdadero RAG multimodal, necesitamos entender el contenido de las im치genes. Usaremos el modelo de visi칩n Granite para generar descripciones.\n",
    "\n",
    "**NOTA**: El procesamiento de im치genes puede llevar tiempo dependiendo de la cantidad de im치genes y del servicio del modelo de visi칩n. Cada imagen ser치 analizada individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htIYVVjHPKSX",
    "outputId": "251f0d3e-132c-43b6-eeae-e87c6bd87a03"
   },
   "outputs": [],
   "source": [
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    \"\"\"Encode image to base64 for vision model processing\"\"\"\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri\n",
    "\n",
    "# Configuraci칩n del prompt de visi칩n - si칠ntete libre de experimentar con esto!\n",
    "image_prompt = \"Give a detailed description of what is depicted in the image\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": image_prompt},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "vision_prompt = vision_processor.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "pictures: list[Document] = []\n",
    "doc_id = len(texts) + len(tables)\n",
    "\n",
    "for source, docling_document in conversions.items():\n",
    "    num_pictures = len(docling_document.pictures)\n",
    "    for i, picture in enumerate(docling_document.pictures):\n",
    "        ref = picture.get_ref().cref\n",
    "        print(f\"  Processing image: {ref} ({i+1}/{num_pictures})\")\n",
    "\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            # Generate image description using vision model\n",
    "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
    "\n",
    "            # Extract provenance information for visual grounding\n",
    "            prov_data = []\n",
    "            if hasattr(picture, 'prov') and picture.prov:\n",
    "                for prov in picture.prov:\n",
    "                    # Get the page to access its height for coordinate conversion\n",
    "                    if prov.page_no < len(docling_document.pages):\n",
    "                        page = docling_document.pages[prov.page_no]\n",
    "                        # Convert to top-left origin and normalize\n",
    "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                        bbox_norm = bbox.normalized(page.size)\n",
    "\n",
    "                        prov_data.append({\n",
    "                            \"page_no\": prov.page_no,\n",
    "                            \"bbox\": {\n",
    "                                \"l\": bbox_norm.l,\n",
    "                                \"t\": bbox_norm.t,\n",
    "                                \"r\": bbox_norm.r,\n",
    "                                \"b\": bbox_norm.b\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
    "                    \"item_type\": \"picture\",  # Mark as picture for special handling\n",
    "                    \"prov_data\": prov_data  # Store normalized provenance data\n",
    "                },\n",
    "            )\n",
    "            pictures.append(document)\n",
    "\n",
    "print(f\"Created {len(pictures)} image descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    \"\"\"Encode image to base64 for vision model processing\"\"\"\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV3N4DIVL1a_"
   },
   "source": [
    "### Mostramos una muestra de los documentos procesados\n",
    "\n",
    "Examinemos lo que hemos creado para entender la naturaleza multimodal de nuestro sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "C_F_3_ZUL1yS",
    "outputId": "5a54c832-6f9d-432f-8df1-1205791b0b8d"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"\\nSample processed documents:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample text chunks\n",
    "print(\"\\nTEXT CHUNK EXAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "for i, text_doc in enumerate(texts[:3]):  # Show first 3 text chunks\n",
    "    print(f\"\\nText Chunk {i+1}:\")\n",
    "    print(f\"  Document ID: {text_doc.metadata['doc_id']}\")\n",
    "    print(f\"  Source: {text_doc.metadata['source'].split('/')[-1]}\")  # Just filename\n",
    "    print(f\"  Reference: {text_doc.metadata['ref']}\")\n",
    "    print(f\"  Has visual grounding: {'dl_meta' in text_doc.metadata}\")\n",
    "    print(\"  Content preview:\")\n",
    "    print(f\"    {text_doc.page_content[:250]}...\")\n",
    "    if i < 2:  # Add separator between examples except after the last one\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Show sample tables\n",
    "print(\"\\n\\nTABLE EXAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "if tables:\n",
    "    for i, table_doc in enumerate(tables[:3]):  # Show first 3 tables\n",
    "        print(f\"\\nTable {i+1}:\")\n",
    "        print(f\"  Document ID: {table_doc.metadata['doc_id']}\")\n",
    "        print(f\"  Reference: {table_doc.metadata['ref']}\")\n",
    "        print(\"  Content preview (Markdown format):\")\n",
    "        # Show first few lines of the table\n",
    "        table_lines = table_doc.page_content.split('\\n')[:8]\n",
    "        for line in table_lines:\n",
    "            print(f\"    {line}\")\n",
    "else:\n",
    "    print(\"  No tables found in the document.\")\n",
    "\n",
    "# Show sample images with descriptions\n",
    "print(\"\\n\\nIMAGE EXAMPLES WITH AI-GENERATED DESCRIPTIONS:\")\n",
    "print(\"-\" * 80)\n",
    "if pictures:\n",
    "    for i, pic_doc in enumerate(pictures[:3]):  # Show first 3 images\n",
    "        print(f\"\\nImage {i+1}:\")\n",
    "        print(f\"  Document ID: {pic_doc.metadata['doc_id']}\")\n",
    "        print(f\"  Reference: {pic_doc.metadata['ref']}\")\n",
    "        print(\"  AI-Generated Description:\")\n",
    "        # Wrap the description for better readability\n",
    "        wrapped_text = textwrap.fill(pic_doc.page_content, width=70, initial_indent=\"    \", subsequent_indent=\"    \")\n",
    "        print(wrapped_text)\n",
    "\n",
    "        # Display the actual image\n",
    "        source = pic_doc.metadata['source']\n",
    "        ref = pic_doc.metadata['ref']\n",
    "        docling_document = conversions[source]\n",
    "        picture = RefItem(cref=ref).resolve(docling_document)\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            print(\"\\n  Original Image:\")\n",
    "            # Resize image for display if too large\n",
    "            display_image = image.copy()\n",
    "            max_width = 600\n",
    "            if display_image.width > max_width:\n",
    "                ratio = max_width / display_image.width\n",
    "                new_height = int(display_image.height * ratio)\n",
    "                display_image = display_image.resize((max_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
    "            display(display_image)\n",
    "\n",
    "        if i < min(2, len(pictures)-1):\n",
    "            print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"  No images found in the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W292HEjOOkC3"
   },
   "source": [
    "### Comprendiendo la Atribuci칩n Visual de Fuentes\n",
    "\n",
    "El grounding visual es lo que diferencia a este sistema. Las funciones que definiremos en las siguientes celdas nos permiten:\n",
    "1. **Localizar**: Encontrar la fuente exacta de cualquier informaci칩n recuperada\n",
    "2. **Resaltar**: Dibujar indicadores visuales en las p치ginas del documento\n",
    "3. **Diferenciar**: Usar estilos distintos para texto, tablas e im치genes\n",
    "4. **Verificar**: Permitir a los usuarios confirmar las respuestas de la IA contra los documentos fuente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0SFvP-tOtkE"
   },
   "outputs": [],
   "source": [
    "# Esta funci칩n visualiza de d칩nde proviene un fragmento de texto en el documento original.\n",
    "def visualize_chunk_grounding(chunk, doc_store, highlight_color=\"blue\"):\n",
    "    \"\"\"\n",
    "    Visualize where a text chunk comes from in the original document.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the original document from the store\n",
    "    2. Finds the pages containing the chunk content\n",
    "    3. Draws bounding boxes around the source regions\n",
    "    4. Displays the highlighted pages\n",
    "\n",
    "    Args:\n",
    "        chunk: LangChain Document with visual grounding metadata\n",
    "        doc_store: Dictionary mapping document hashes to file paths\n",
    "        highlight_color: Color for highlighting (blue, green, red, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of page images with highlights\n",
    "    \"\"\"\n",
    "    # Get the origin hash\n",
    "    origin_hash = chunk.metadata.get(\"origin_hash\")\n",
    "    if not origin_hash:\n",
    "        print(\"No origin hash found in metadata\")\n",
    "        return None\n",
    "\n",
    "    # Load the full document from store\n",
    "    dl_doc = DoclingDocument.load_from_json(doc_store.get(origin_hash))\n",
    "\n",
    "    print(f\"Visualizing source location for chunk {chunk.metadata.get('doc_id', 'Unknown')}\")\n",
    "\n",
    "    # Handle different types of content\n",
    "    page_images = {}\n",
    "    item_type = chunk.metadata.get(\"item_type\", \"text\")\n",
    "\n",
    "    if item_type in [\"picture\", \"table\"] and \"prov_data\" in chunk.metadata:\n",
    "        # Handle tables and pictures with simple provenance data\n",
    "        prov_data = chunk.metadata[\"prov_data\"]\n",
    "\n",
    "        if not prov_data:\n",
    "            print(f\"No provenance data available for this {item_type}\")\n",
    "            return None\n",
    "\n",
    "        for prov in prov_data:\n",
    "            page_no = prov[\"page_no\"]\n",
    "\n",
    "            # Get page image\n",
    "            if page_no < len(dl_doc.pages):\n",
    "                page = dl_doc.pages[page_no]\n",
    "                if hasattr(page, 'image') and page.image:\n",
    "                    if page_no not in page_images:\n",
    "                        img = page.image.pil_image.copy()\n",
    "                        page_images[page_no] = {\n",
    "                            'image': img,\n",
    "                            'page': page,\n",
    "                            'draw': ImageDraw.Draw(img)\n",
    "                        }\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    draw = page_images[page_no]['draw']\n",
    "                    bbox = prov[\"bbox\"]\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    draw = page_images[page_no]['draw']\n",
    "                    bbox = prov[\"bbox\"]\n",
    "\n",
    "                    # The coordinates are already normalized and in top-left origin\n",
    "                    # Just scale to image dimensions\n",
    "                    img_width = page_images[page_no]['image'].width\n",
    "                    img_height = page_images[page_no]['image'].height\n",
    "\n",
    "                    l = int(bbox[\"l\"] * img_width)\n",
    "                    r = int(bbox[\"r\"] * img_width)\n",
    "                    t = int(bbox[\"t\"] * img_height)\n",
    "                    b = int(bbox[\"b\"] * img_height)\n",
    "\n",
    "                    # Ensure coordinates are valid (min/max) just in case\n",
    "                    l, r = min(l, r), max(l, r)\n",
    "                    t, b = min(t, b), max(t, b)\n",
    "\n",
    "                    # Clamp to image bounds\n",
    "                    l = max(0, min(l, img_width - 1))\n",
    "                    r = max(0, min(r, img_width - 1))\n",
    "                    t = max(0, min(t, img_height - 1))\n",
    "                    b = max(0, min(b, img_height - 1))\n",
    "\n",
    "                    # Draw highlight with different styles for different types\n",
    "                    if item_type == \"picture\":\n",
    "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=4)\n",
    "                        draw.text((l, t-20), \"IMAGE\", fill=highlight_color)\n",
    "                    elif item_type == \"table\":\n",
    "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=3)\n",
    "                        draw.text((l, t-20), \"TABLE\", fill=highlight_color)\n",
    "\n",
    "    elif \"dl_meta\" in chunk.metadata:\n",
    "        # Handle text chunks with DocMeta\n",
    "        try:\n",
    "            meta = DocMeta.model_validate(chunk.metadata[\"dl_meta\"])\n",
    "\n",
    "            # Process each item in the chunk to find source locations\n",
    "            for doc_item in meta.doc_items:\n",
    "                if hasattr(doc_item, 'prov') and doc_item.prov:\n",
    "                    for prov in doc_item.prov:\n",
    "                        page_no = prov.page_no\n",
    "\n",
    "                        # Get or create page image\n",
    "                        if page_no not in page_images:\n",
    "                            if page_no < len(dl_doc.pages):\n",
    "                                page = dl_doc.pages[page_no]\n",
    "                                if hasattr(page, 'image') and page.image:\n",
    "                                    img = page.image.pil_image.copy()\n",
    "                                    page_images[page_no] = {\n",
    "                                        'image': img,\n",
    "                                        'page': page,\n",
    "                                        'draw': ImageDraw.Draw(img)\n",
    "                                    }\n",
    "\n",
    "                        # Draw bounding box on the page\n",
    "                        if page_no in page_images:\n",
    "                            page_data = page_images[page_no]\n",
    "                            page = page_data['page']\n",
    "                            draw = page_data['draw']\n",
    "\n",
    "                            # Convert coordinates to image space\n",
    "                            bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
    "                            bbox = bbox.normalized(page.size)\n",
    "\n",
    "                            # Scale to actual image dimensions\n",
    "                            l = int(bbox.l * page_data['image'].width)\n",
    "                            r = int(bbox.r * page_data['image'].width)\n",
    "                            t = int(bbox.t * page_data['image'].height)\n",
    "                            b = int(bbox.b * page_data['image'].height)\n",
    "\n",
    "                            # Draw highlight rectangle\n",
    "                            draw.rectangle([l, t, r, b], outline=highlight_color, width=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text chunk metadata: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No visual grounding metadata available for this chunk\")\n",
    "        return None\n",
    "\n",
    "    # Display highlighted pages\n",
    "    for page_no, page_data in sorted(page_images.items()):\n",
    "        plt.figure(figsize=(12, 16))\n",
    "        plt.imshow(page_data['image'])\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Add title indicating content type\n",
    "        if item_type == \"picture\":\n",
    "            title = \"Image Location\"\n",
    "        elif item_type == \"table\":\n",
    "            title = \"Table Location\"\n",
    "        else:\n",
    "            title = \"Text Location\"\n",
    "        plt.title(f'{title} - Page {page_no + 1}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return page_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuVVgC_YQaln"
   },
   "source": [
    "## Popular la base de datos vectorial con embeddings y metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wqnAw0Te1zi"
   },
   "source": [
    "### Comprendiendo las Bases de Datos Vectoriales en RAG Multimodal\n",
    "\n",
    "Las bases de datos vectoriales son el motor de b칰squeda de nuestro sistema RAG. Estas:\n",
    "- Almacenan representaciones num칠ricas (embeddings) de nuestro contenido\n",
    "- Permiten b칰squedas por similitud sem치ntica\n",
    "- Mantienen todos los metadatos necesarios para el grounding visual\n",
    "- Soportan una recuperaci칩n r치pida a escala\n",
    "\n",
    "Para contenido multimodal, esto significa:\n",
    "- Los fragmentos de texto se incrustan directamente\n",
    "- El markdown de las tablas se incrusta para b칰squedas estructurales\n",
    "- Las descripciones de im치genes generadas por IA se incrustan para b칰squedas visuales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_0oC5arQbHi"
   },
   "source": [
    "### Configuraci칩n de la Base de Datos Vectorial\n",
    "\n",
    "Usaremos Milvus, una base de datos vectorial de alto rendimiento. Milvus es una base de datos de c칩digo abierto dise침ada para manejar grandes vol칰menes de datos vectoriales, lo que la hace ideal para aplicaciones de IA y aprendizaje autom치tico. Para m치s opciones de bases de datos vectoriales, consulta [esta receta de Almacenamiento Vectorial con Langchain](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQIOysr3Qgg5",
    "outputId": "0feefa9e-27df-446f-ae90-3e9f156eebd5"
   },
   "outputs": [],
   "source": [
    "# Create a temporary database file\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
    "print(f\"Vector database will be saved to: {db_file}\")\n",
    "\n",
    "# Initialize Milvus vector store\n",
    "vector_db: VectorStore = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,  # Allows flexible metadata storage\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},  # Automatic index optimization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy-12B1eQq_P"
   },
   "source": [
    "### A침adimos Documentos a la Base de Datos Vectorial\n",
    "\n",
    "Ahora a침adiremos todos nuestros documentos procesados (fragmentos de texto, tablas y descripciones de im치genes) a la base de datos vectorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlQMqXOEQt2o",
    "outputId": "0302a03a-c95a-4e46-f1e5-9c9320a7abda"
   },
   "outputs": [],
   "source": [
    "print(\"\\nA침adiendo documentos a la base de datos vectorial...\")\n",
    "documents = list(itertools.chain(texts, tables, pictures))\n",
    "ids = vector_db.add_documents(documents)\n",
    "print(f\"A침adidos {len(ids)} documentos a la base de datos vectorial.\")\n",
    "print(f\"  - Fragmentos de texto: {len(texts)}\")\n",
    "print(f\"  - Tablas: {len(tables)}\")\n",
    "print(f\"  - Descripciones de im치genes: {len(pictures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xavz2IieQyqI"
   },
   "source": [
    "## Test de Recuperaci칩n con Atribuci칩n Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjuJtoCKfPAo"
   },
   "source": [
    "### Probando la Recuperaci칩n con Atribuci칩n Visual\n",
    "\n",
    "Antes de construir todo el pipeline de RAG, probemos que nuestra recuperaci칩n y atribuci칩n visual funcionen correctamente. Esto ayuda a verificar:\n",
    "- Se encuentra contenido basado en similitud sem치ntica\n",
    "- Se preserva la metadata de atribuci칩n visual\n",
    "- Se manejan correctamente diferentes tipos de contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gLXwOObfSCR"
   },
   "source": [
    "### Basic Retrieval Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuc09LOtQ5IP",
    "outputId": "c8d08282-c57a-442c-c154-1469271947d3"
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"How much was spent on food distribution relative to the amount of food distributed?\"\n",
    "\n",
    "print(f\"\\nTesting retrieval for query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs = vector_db.as_retriever().invoke(test_query)\n",
    "\n",
    "# Display retrieved documents\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nRetrieved Document {i+1}:\")\n",
    "\n",
    "    # Determine content type\n",
    "    item_type = doc.metadata.get('item_type', 'text')\n",
    "    if item_type == 'picture':\n",
    "        content_type = \"AI-Generated Image Description\"\n",
    "    elif item_type == 'table':\n",
    "        content_type = \"Table (Markdown)\"\n",
    "    else:\n",
    "        content_type = \"Text Chunk\"\n",
    "\n",
    "    print(f\"Type: {content_type}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source'].split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iBcEjhVRBcG"
   },
   "source": [
    "## Construcci칩n del Pipeline RAG Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma2N5kKTRHWO"
   },
   "source": [
    "\n",
    "Ahora implementaremos el sistema RAG multimodal completo que:\n",
    "1. Recupera contenido multimodal relevante\n",
    "2. Muestra exactamente de d칩nde proviene cada pieza\n",
    "3. Genera respuestas precisas y fundamentadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_60Mv9wgfmkm"
   },
   "source": [
    "### Diferenciando Tipos de Contenido\n",
    "\n",
    "Nuestro sistema maneja tres tipos de contenido de manera distinta:\n",
    "\n",
    "1. **Fragmentos de Texto**: El resaltado est치ndar muestra pasajes de texto\n",
    "2. **Tablas**: Bordes gruesos con etiquetas \"TABLE\" marcan datos estructurados\n",
    "3. **Im치genes**: Bordes distintivos con etiquetas \"IMAGE\" muestran ubicaciones de im치genes\n",
    "\n",
    "Esta diferenciaci칩n visual ayuda a los usuarios a comprender r치pidamente qu칠 tipo de contenido contribuy칩 a la respuesta de la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PRpHSBcJehQ"
   },
   "source": [
    "### Probando el Sistema RAG Multimodal con Visual Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmQdDvcXRD39"
   },
   "outputs": [],
   "source": [
    "# Bringing It All Together\n",
    "def rag_with_visual_grounding(question, vector_db, doc_store, model, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform RAG with visual grounding of results.\n",
    "\n",
    "    This function:\n",
    "    1. Retrieves relevant chunks from the vector database\n",
    "    2. Visualizes where each chunk comes from in the original document\n",
    "    3. Generates a response using the retrieved context\n",
    "\n",
    "    Args:\n",
    "        question: User's query\n",
    "        vector_db: Vector database with embedded documents\n",
    "        doc_store: Document store for visual grounding\n",
    "        model: Language model for response generation\n",
    "        tokenizer: Tokenizer for the language model\n",
    "        top_k: Number of chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (outputs, relevant_chunks)\n",
    "    \"\"\"\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(f\"\\nRecuperando los {top_k} fragmentos relevantes...\")\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    relevant_chunks = retriever.invoke(question)\n",
    "\n",
    "    print(f\"Se encontraron {len(relevant_chunks)} fragmentos relevantes\")\n",
    "\n",
    "    # Step 2: Visualize each chunk's source location\n",
    "    print(\"\\nVisualizando la ubicaci칩n de los fragmentos recuperados...\")\n",
    "\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"\\n--- Resultado {i+1} ---\")\n",
    "\n",
    "        # Determine content type\n",
    "        item_type = chunk.metadata.get('item_type', 'text')\n",
    "        if item_type == 'picture':\n",
    "            content_type = \"Descripci칩n de Imagen Generada por IA\"\n",
    "            color = 'red'\n",
    "        elif item_type == 'table':\n",
    "            content_type = \"Tabla (Markdown)\"\n",
    "            color = 'green'\n",
    "        else:\n",
    "            content_type = \"Fragmento de Texto\"\n",
    "            color = 'blue'\n",
    "\n",
    "        print(f\"Content type: {content_type}\")\n",
    "        print(f\"Text preview: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Source: {chunk.metadata.get('source', 'Unknown').split('/')[-1]}\")\n",
    "\n",
    "        # Show visual grounding if available\n",
    "        if \"dl_meta\" in chunk.metadata or \"prov_data\" in chunk.metadata:\n",
    "            visualize_chunk_grounding(\n",
    "                chunk,\n",
    "                doc_store,\n",
    "                highlight_color=color\n",
    "            )\n",
    "        else:\n",
    "            print(\"  (No visual grounding available for this chunk)\")\n",
    "\n",
    "    # Step 3: Create RAG pipeline for response generation\n",
    "    print(\"\\nGenerando respuesta con LLM...\")\n",
    "\n",
    "    # Create Granite prompt template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer the question based on the provided context in a concise manner. Always answer in the same language as the question.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"{input}\",\n",
    "        }],\n",
    "        documents=[{\n",
    "            \"doc_id\": \"0\",\n",
    "            \"text\": \"{context}\",\n",
    "        }],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=escape_f_string(prompt, \"input\", \"context\")\n",
    "    )\n",
    "\n",
    "    # Document prompt template\n",
    "    document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
    "{page_content}\"\"\")\n",
    "\n",
    "    # Create chains\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt_template,\n",
    "        document_prompt=document_prompt_template,\n",
    "        document_separator=\"\",\n",
    "    )\n",
    "\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=combine_docs_chain,\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    outputs = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "    print(\"\\nRespuesta generada:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(outputs['answer'])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return outputs, relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQitCyY0RVPn"
   },
   "source": [
    "## Demostraci칩n Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d__u-q6RVnO"
   },
   "source": [
    "\n",
    "Vamos a ejecutar una consulta y ver el sistema completo en acci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ud0Oy_K4Ra7u",
    "outputId": "f7f459a6-e44b-4878-c119-bc4f28d69d08"
   },
   "outputs": [],
   "source": [
    "main_query = \"Como funciona la pipeline de conversi칩n de PDFs de Docling?\"\n",
    "outputs, chunks = rag_with_visual_grounding(\n",
    "    main_query,\n",
    "    vector_db,\n",
    "    doc_store,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dABGUdQ5RaX4"
   },
   "source": [
    "# Resumen y Pr칩ximos Pasos\n",
    "\n",
    "### Lo que Has Logrado\n",
    "\n",
    "춰Felicidades! Has construido con 칠xito un sistema avanzado de RAG multimodal. Esto es lo que has aprendido:\n",
    "\n",
    "1. **Implementaci칩n de un RAG con *Visual Grounding***\n",
    "   - Configuraste Docling para preservar referencias visuales\n",
    "   - Mantuviste metadatos de coordenadas a lo largo del pipeline de procesamiento\n",
    "   - Creaste atribuci칩n visual para todos los tipos de contenido\n",
    "\n",
    "2. **Procesamiento de Documentos Multimodal**\n",
    "   - Manejo fluido de texto, tablas e im치genes\n",
    "   - Uso de chunking inteligente para optimizar la recuperaci칩n\n",
    "   - Uso de modelos de visi칩n IA para comprensi칩n de im치genes\n",
    "\n",
    "3. **Arquitectura Transparente de RAG**\n",
    "   - Como establecer confianza en tu sistema RAG mediante verificaci칩n visual\n",
    "   - Habilitaci칩n de atribuci칩n de fuentes para cada respuesta\n",
    "   - Creaci칩n de un resaltado diferenciado por cada tipo de contenido\n",
    "\n",
    "4. **Integraci칩n Lista para Producci칩n**\n",
    "   - Combinaci칩n efectiva de m칰ltiples modelos de IA\n",
    "   - Creaci칩n de un pipeline escalable de procesamiento de documentos\n",
    "\n",
    "### 쯇or qu칠 el Grounding Visual lo Cambia Todo?\n",
    "\n",
    "Los sistemas RAG tradicionales son \"cajas negras\": los usuarios deben confiar ciegamente en la IA. Tu sistema:\n",
    "\n",
    "- **Muestra las fuentes**: Cada afirmaci칩n puede ser verificada visualmente\n",
    "- **Construye confianza**: Los usuarios ven exactamente de d칩nde proviene la informaci칩n\n",
    "- **Habilita auditor칤as**: Perfecto para industrias reguladas\n",
    "- **Reduce alucinaciones**: La verificaci칩n visual detecta errores\n",
    "\n",
    "### El Poder de la Comprensi칩n Multimodal\n",
    "\n",
    "Al procesar texto, tablas e im치genes, tu sistema:\n",
    "\n",
    "- Captura informaci칩n completa del documento\n",
    "- Habilita consultas y respuestas m치s ricas\n",
    "- Maneja la complejidad de documentos del mundo real\n",
    "- Proporciona respuestas completas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3zahmhOg0Jd"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phDxA2ivg0vn"
   },
   "source": [
    "## Proximos Pasos:\n",
    "\n",
    "\n",
    "1. **Experimenta con otros documentos**\n",
    "   - Prueba con documentos en espa침ol, franc칠s o alem치n\n",
    "   - Prueba documentos con diagramas t칠cnicos y gr치ficos\n",
    "   - Procesa informes con contenido mixto\n",
    "\n",
    "2. **Personaliza la IA para ti**\n",
    "   - Utiliza los modelos de embeddings, visi칩n y lenguaje que se adapten a tu flujo de trabajo com칰n.\n",
    "   - Ajusta los prompts para mejorar la calidad de las descripciones de im치genes y respuestas del modelo de lenguaje.\n",
    "   \n",
    "   ```python\n",
    "   # Ejemplo: Prompts de imagen espec칤ficos a un dominio\n",
    "   medical_prompt = \"Describe esta imagen m칠dica, se침alando cualquier anomal칤a o caracter칤stica clave\"\n",
    "   financial_prompt = \"Analiza este gr치fico financiero, identificando tendencias y puntos de datos clave\"\n",
    "   ```\n",
    "\n",
    "3. **Optimiza el Rendimiento**\n",
    "   - Procesa documentos en batch: [Docling Batch Conversion](https://docling-project.github.io/docling/examples/batch_convert/)\n",
    "   - Usa aceleraci칩n por GPU con modelos de visi칩n locales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVPQsnmFg-HB"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3UNKhQhI7Z"
   },
   "source": [
    "## Recursos Adicionales\n",
    "\n",
    "### Documentaci칩n Oficial\n",
    "- **[Documentaci칩n de Docling](https://github.com/docling-project/docling)**: 칔ltimas caracter칤sticas y actualizaciones\n",
    "- **[Modelos IBM Granite](https://www.ibm.com/granite/)**: Tarjetas de modelo y capacidades\n",
    "- **[Documentaci칩n de LangChain](https://python.langchain.com/)**: Patrones y mejores pr치cticas de RAG\n",
    "- **[Documentaci칩n de Milvus](https://milvus.io/docs)**: Optimizaci칩n de bases de datos vectoriales\n",
    "\n",
    "### Recursos Comunitarios\n",
    "- 칔nete a la [comunidad de Docling en GitHub](https://github.com/docling-project/docling/discussions)\n",
    "- Comparte tus implementaciones\n",
    "- Contribuye con mejoras Docling! 仇벒잺\n",
    "  \n",
    "### Temas Relacionados para Explorar\n",
    "- An치lisis de Dise침o de Documentos\n",
    "- Embeddings Multimodales\n",
    "- Respuesta a Preguntas Visuales\n",
    "- Sistemas de IA Explicables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytIk0-fghmMy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wADjxUDfhm5L"
   },
   "source": [
    "## Conclusi칩n\n",
    "\n",
    "Has completado un incre칤ble viaje desde la conversi칩n b치sica de documentos hasta la construcci칩n de un sistema de IA sofisticado y transparente. La combinaci칩n de la comprensi칩n de documentos de Docling, las capacidades de IA de Granite y el grounding visual crea una aplicaci칩n poderosa.\n",
    "\n",
    "Tu sistema RAG multimodal representa la vanguardia de la IA para documentos. Ya sea que est칠s construyendo para el sector de la salud, legal, financiero o cualquier otro dominio, ahora tienes las herramientas para crear sistemas de IA que no solo son poderosos, sino tambi칠n confiables y transparentes."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
